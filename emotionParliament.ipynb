{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) C. Cochrane, L. Rheault, T. Whyte, M.-C. Wong, J.-F. Godbout, T. Whyte\n",
    "\n",
    "(2) Script v.2019-09-24\n",
    "\n",
    "(3) Title: Comparing Human and Machine Classification of Written and Video Records of Political Speech\n",
    "\n",
    "(4) Abstract: The volume of machine-readable transcripts of legislative and other speeches has increased exponentially over the past three decades, leading to the widespread application of existing tools for the automated analyses of emotion in text.  Unlike in writing, however, expressing emotion in speech is not confined to word-choice and syntax, and instead relies heavily on intonation, facial expressions, and body language, which go undetected in analyses of political text.  This raises the question of whether tools developed for analyses of writing can detect emotion in transcripts of political speeches. Drawing on a new corpus of text and video data from the Canadian House of Commons, this paper does three things.  First, we examine whether transcripts capture the emotional content of speeches by comparing human judgments of video clips to human judgments of the corresponding transcripts. We find that transcripts capture the sentiment, but not the emotional intensity, of political speeches. Second, we compare strategies for the automated analysis of sentiment in text, and test their outputs against human-coded sentiment analysis of speech transcripts. We find that leading dictionary and supervised approaches to sentiment detection performed reasonably well, but lexicons generated using word embeddings far surpassed these other approaches. Finally, we test the robustness of word embeddings to domain specificity, choice of seed words, chance, and corpus size.   We find that word embeddings can be transferred across domains and are reasonably robust to alternative specifications and conditions. We conclude by discussing the implications of these findings for the analysis of emotion in speech. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Randomization Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first stage of the project involved selecting speech fragments from video record of question period maintained by the Canadian Parliamentary Access Channel (CPAC).  We recorded the full video of every third question period during the four years beginning November 26, 2013 (last session of 41st Parliament and Harper Government) to December 13, 2017 (first session of 42nd Parliament and Trudeau Government). The videos of the question periods were trimmed to run from the beginning of the first speech to the end of the last speech.  \n",
    "\n",
    "The script below identifies 10 random time points within each video.  We manually created video clips of the sentences being spoken at each time point. If nobody was speaking at an indicated time, or if two selected time points captured the same sentence, we selected the sentence spoken immediately prior to the sentence being spoken at a given time point. There were three cases with an error in an identified clip (email dings). We chose the sentence prior to these clips as well. To identify the beginning and end of a sentence, we used the official record of debates in the language of the speaker. C. Cochrane trimmed the clips manually. The clips are available on YouTube (see hansardExtractedVideoTranscripts.csv, below, for links) or by request (16GB), and the full videos are available upon request (870GB).\n",
    "\n",
    "The script in this section was originally run in R, but can be run in this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -w 5 -h 5 --units in -r 200\n",
    "## FILE PATHS\n",
    "install.packages('lubridate')\n",
    "library(lubridate)\n",
    "\n",
    "## FILE PATHS\n",
    "outpath <- \"/Users/chriscochrane/Dropbox/Sentiment/emotionInParliament/clipSelections/\" # (add your desination fold\n",
    "\n",
    "## META DATA\n",
    "year <- 2017 #Enter Year (YYYY)\n",
    "month <- 12 #Enter Month (1-12)\n",
    "day <- 13 #Enter Day (1-31)\n",
    "videoLength = \"47:54\" #Enter Length of Video (MM:SS)\n",
    "\n",
    "##EXTRACTION POINTS\n",
    "\n",
    "nOfExtractionPoints = 10 #Enter number of sentences to extract\n",
    "\n",
    "## DEFINE RANDOMIZER\n",
    "\n",
    "runRandomizer <- function(nOfExtractionPoints, videoLength){\n",
    "  #generates a set of n random numbers in the domain of 0 to k,\n",
    "  #where n is the number of extraction points requested (line 9)\n",
    "  #and k is the length of the video (line 7):\n",
    "\n",
    "  timeStamps <- runif(nOfExtractionPoints, 0,as.period(ms(videoLength), unit=\"sec\") )\n",
    "  timeStamps <- round(seconds_to_period(timeStamps), digits=0) #rounding to nearest second\n",
    "  timeStamps <- sort(timeStamps) #ascending order\n",
    "\n",
    "  return(timeStamps)\n",
    "}\n",
    "\n",
    "##RUN RANDOMIZER\n",
    "#Function accepts arguments for number of extraction points and\n",
    "#lenth of the video. These are defined above.\n",
    "\n",
    "extractionPoints <- runRandomizer(nOfExtractionPoints, videoLength)\n",
    "\n",
    "## OUTPUT\n",
    "subpath <- sprintf(\"%s-%s-%s%s\", year, month, day,\"/\")\n",
    "\n",
    "dir.create(file.path(subpath))\n",
    "extractionPoints <- data.frame(extractionPoints) #creating dataframe\n",
    "colnames(extractionPoints) <- c(\"timeStamp\") #naming column\n",
    "extractionPoints[c(\"Speaker\", \"French\", \"Party\", \"Length\", \n",
    "                   \"English Hansard\", \"Hansard Floor\", \n",
    "                   \"Google Translate\")] <- NA #creating columns\n",
    "\n",
    "write.csv(extractionPoints, file=sprintf(\"%s%s-%s-%s-%s%s\", subpath,  year, month, day, \"selections\", \".csv\"))\n",
    "write.csv(extractionPoints, file=sprintf(\"%s%s-%s-%s-%s%s\", subpath,  year, month, day, \"final\", \".csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Video Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the video clips were created, they were uploaded to youTube and piped into a Qualtrics survey instrument.  The transcripts, metadata, and YouTube links for these videos is in hansardExtractedVideoTranscripts.csv.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansardExtractedVideos = pd.read_csv('hansardExtractedVideoTranscripts.csv')\n",
    "hansardExtractedVideos['Video'] = hansardExtractedVideos['ID_main']\n",
    "hansardExtractedVideos.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clips were shown at random---via a Qualtrics Survey Instrument---to three independent, bilingual coders.  The coders were asked to seperately adjudge the valence (sentiment) and activation (intensity/arousal) of each speech fragment.  The raw Qualtrics output is in the file: 'emotionInHansard_December+20%2C+2018_14.52.csv'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawQualtricsOutput = pd.read_csv('emotionInHansard_December+20%2C+2018_14.52.csv')\n",
    "rawQualtricsOutput.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.A Extracting Data from Qualtrics Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each coder saw each video more than once.  Qualtrics output is a dog's breakfast. The following script extracts from the Qualtrics output, and structures, the first two sets of scores assigned by each coder to each video.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.A.1 Empty Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoList = []\n",
    "coderList = []\n",
    "sentimentList = []\n",
    "activationList = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.A.2 A function for splitting strings based on a property.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The qualtrics output puts the values of different variables into the same cell, separating the variables by some symbol or another, but typically by a ' | '. The qualtrics data is also broken into blocs.  This is because the survey creation tool loads a still image for every video linked anywhere in the instrument, thus crashing the browser.  Blocks provide a workaround.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findStringParts(string, symbol, n):\n",
    "    string = str(string)\n",
    "    parts = string.split(symbol, n+1)\n",
    "    if len(parts)<=n+1:\n",
    "        return parts\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.A.3 Parse by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(rawQualtricsOutput)):\n",
    "\n",
    "    if rawQualtricsOutput['distributionChannel'][i] != \"preview\": #drop previews\n",
    "        \n",
    "        coder = rawQualtricsOutput['QID78_TEXT'][i]\n",
    "        if coder == \"js\":\n",
    "            coder = \"JS\"\n",
    "        \n",
    "        #-----------------\n",
    "        # Bloc A\n",
    "        #-----------------\n",
    "        '''count n of bars, which separate variables'''\n",
    "        numBars = rawQualtricsOutput['BL_9z3f3SVI5D1eOk5_DO'][i].count('|') #count seperators (|)       \n",
    "        \n",
    "        '''break strings into components, which are separated by bars'''\n",
    "        blocA_DO = findStringParts(rawQualtricsOutput['BL_9z3f3SVI5D1eOk5_DO'][i], '|', numBars) \n",
    "        blocA_title_vid1 = blocA_DO[0] # The first video title\n",
    "        blocA_title_vid2 = blocA_DO[1] # The second video title\n",
    "        \n",
    "        '''activation score for first video'''\n",
    "        blocA_act_vid1 = rawQualtricsOutput['QID23_1'][i] \n",
    "\n",
    "        '''activation score for second video'''\n",
    "        blocA_act_vid2 = rawQualtricsOutput['QID23_2'][i] \n",
    "\n",
    "        '''sentiment score for first video'''\n",
    "        blocA_sent_vid1 = rawQualtricsOutput['QID26_1'][i] \n",
    "\n",
    "        '''sentiment score for second video'''\n",
    "        blocA_sent_vid2 = rawQualtricsOutput['QID26_2'][i] \n",
    "\n",
    "        '''Add Bloc A data to running Tally'''\n",
    "\n",
    "        '''For Video 1'''\n",
    "        coderList.append(coder)\n",
    "        videoList.append(blocA_title_vid1)\n",
    "        sentimentList.append(blocA_sent_vid1)\n",
    "        activationList.append(blocA_act_vid1)\n",
    "\n",
    "        '''For Video 2'''\n",
    "        coderList.append(coder)\n",
    "        videoList.append(blocA_title_vid2)\n",
    "        sentimentList.append(blocA_sent_vid2)\n",
    "        activationList.append(blocA_act_vid2)\n",
    "\n",
    "        #-----------------\n",
    "        # Bloc B\n",
    "        #-----------------\n",
    "        '''count n of bars, which separate variables'''\n",
    "\n",
    "        numBars = rawQualtricsOutput['BL_6Rb4ytpLTIRc2ot_DO'][i].count('|') #count seperators (|)       \n",
    "\n",
    "        '''break strings into components, which are separated by bars'''\n",
    "\n",
    "        blocB_DO = findStringParts(rawQualtricsOutput['BL_6Rb4ytpLTIRc2ot_DO'][i], '|', numBars) \n",
    "        blocB_title_vid1 = blocB_DO[0] # The first video title\n",
    "        blocB_title_vid2 = blocB_DO[1] # The second video title\n",
    "\n",
    "        # *** CC FIXED. For BLOC B, Sentiment and Activation Labels Reversed in Qualtrics Data***\n",
    "\n",
    "        '''activation score for first video'''\n",
    "        blocB_act_vid1 = rawQualtricsOutput['QID3450_1'][i] \n",
    "\n",
    "        '''activation score for second video'''\n",
    "        blocB_act_vid2 = rawQualtricsOutput['QID3450_2'][i] \n",
    "\n",
    "        '''sentiment score for first video'''\n",
    "        blocB_sent_vid1 = rawQualtricsOutput['QID3451_1'][i]\n",
    "\n",
    "        '''sentiment score for second video'''\n",
    "        blocB_sent_vid2 = rawQualtricsOutput['QID3451_2'][i]\n",
    "        \n",
    "        '''Add Bloc B data to running Tally'''\n",
    "\n",
    "        '''For Video 1'''\n",
    "        coderList.append(coder)\n",
    "        videoList.append(blocB_title_vid1)\n",
    "        sentimentList.append(blocB_sent_vid1)\n",
    "        activationList.append(blocB_act_vid1)\n",
    "\n",
    "        '''For Video 2'''\n",
    "        coderList.append(coder)\n",
    "        videoList.append(blocB_title_vid2)\n",
    "        sentimentList.append(blocB_sent_vid2)\n",
    "        activationList.append(blocB_act_vid2)\n",
    "   \n",
    "\n",
    "        #-----------------\n",
    "        # Bloc C\n",
    "        #-----------------\n",
    "\n",
    "        '''count n of bars, which separate variables'''\n",
    "        numBars = rawQualtricsOutput['BL_9YmtLZfF0ecSpuJ_DO'][i].count('|') #count seperators (|)       \n",
    "\n",
    "        '''break strings into components, which are separated by bars'''\n",
    "        blocC_DO = findStringParts(rawQualtricsOutput['BL_9YmtLZfF0ecSpuJ_DO'][i], '|', numBars) \n",
    "        blocC_title_vid1 = blocC_DO[0] # The first video title\n",
    "        blocC_title_vid2 = blocC_DO[1] # The second video title\n",
    "\n",
    "        '''arousal code for first video'''\n",
    "        blocC_act_vid1 = rawQualtricsOutput['QID1055_1'][i]  \n",
    "\n",
    "        '''arousal code for second video'''\n",
    "        blocC_act_vid2 = rawQualtricsOutput['QID1055_2'][i]\n",
    "\n",
    "        '''sentiment code for first video'''\n",
    "        blocC_sent_vid1 = rawQualtricsOutput['QID1056_1'][i]\n",
    "\n",
    "        '''sentiment code for second video'''\n",
    "        blocC_sent_vid2 = rawQualtricsOutput['QID1056_2'][i]\n",
    "\n",
    "        '''Add Bloc C data to running Tally'''\n",
    "\n",
    "        '''For Video 1'''\n",
    "        coderList.append(coder)\n",
    "        videoList.append(blocC_title_vid1)\n",
    "        sentimentList.append(blocC_sent_vid1)\n",
    "        activationList.append(blocC_act_vid1)\n",
    "\n",
    "        '''For Video 2'''\n",
    "        coderList.append(coder)\n",
    "        videoList.append(blocC_title_vid2)\n",
    "        sentimentList.append(blocC_sent_vid2)\n",
    "        activationList.append(blocC_act_vid2)\n",
    "        \n",
    "        #-----------------\n",
    "        # BlocD\n",
    "        #-----------------\n",
    "\n",
    "        '''count n of bars, which separate variables'''\n",
    "        numBars = rawQualtricsOutput['BL_6rskZu2BNcPiZfL_DO'][i].count('|') #count seperators (|)       \n",
    "\n",
    "        '''break strings into components, which are separated by bars'''\n",
    "        blocD_DO = findStringParts(rawQualtricsOutput['BL_6rskZu2BNcPiZfL_DO'][i], '|', numBars) \n",
    "        blocD_title_vid1 = blocD_DO[0] # The first video title\n",
    "        blocD_title_vid2 = blocD_DO[1] # The second video title\n",
    "\n",
    "        # *** CC FIXED! Sentiment and Activation Labels Reversed in Qualtrics ***\n",
    "\n",
    "        '''arousal code for first video'''  \n",
    "        blocD_act_vid1 = rawQualtricsOutput['QID3454_1'][i] #FIXED!\n",
    "        \n",
    "        '''arousal code for second video'''\n",
    "        blocD_act_vid2 = rawQualtricsOutput['QID3454_2'][i] #FIXED!\n",
    "        \n",
    "        '''sentiment code for first video'''\n",
    "        blocD_sent_vid1 = rawQualtricsOutput['QID3455_1'][i] #FIXED!\n",
    "        \n",
    "        '''sentiment code for second video'''\n",
    "        blocD_sent_vid2 = rawQualtricsOutput['QID3455_2'][i] #FIXED!\n",
    "        \n",
    "        '''Add Bloc D data to running Tally'''\n",
    "        '''For Video 1'''\n",
    "        coderList.append(coder)\n",
    "        videoList.append(blocD_title_vid1)\n",
    "        sentimentList.append(blocD_sent_vid1)\n",
    "        activationList.append(blocD_act_vid1)\n",
    "\n",
    "        '''For Video 2'''\n",
    "        coderList.append(coder)\n",
    "        videoList.append(blocD_title_vid2)\n",
    "        sentimentList.append(blocD_sent_vid2)\n",
    "        activationList.append(blocD_act_vid2)\n",
    "\n",
    "        #-----------------\n",
    "        # BlocE\n",
    "        #-----------------\n",
    "\n",
    "        '''count n of bars, which separate variables'''\n",
    "        numBars = rawQualtricsOutput['BL_3aS8nqvLjwudQl7_DO'][i].count('|') #count seperators (|)       \n",
    "\n",
    "        '''break strings into components, which are separated by bars'''\n",
    "        blocE_DO = findStringParts(rawQualtricsOutput['BL_3aS8nqvLjwudQl7_DO'][i], '|', numBars) \n",
    "        blocE_title_vid1 = blocE_DO[0] # The first video title\n",
    "        blocE_title_vid2 = blocE_DO[1] # The second video title\n",
    "\n",
    "        '''arousal code for first video'''\n",
    "        blocE_act_vid1 = rawQualtricsOutput['QID1060_1'][i] \n",
    "\n",
    "        '''arousal code for second video'''\n",
    "        blocE_act_vid2 = rawQualtricsOutput['QID1060_2'][i] \n",
    "\n",
    "        '''sentiment code for first video'''\n",
    "        blocE_sent_vid1 = rawQualtricsOutput['QID1061_1'][i] \n",
    "\n",
    "        '''sentiment code for second video'''\n",
    "        blocE_sent_vid2 = rawQualtricsOutput['QID1061_2'][i]    \n",
    "                \n",
    "        '''Add Bloc E data to running Tally'''\n",
    "        '''For Video 1'''\n",
    "        coderList.append(coder)\n",
    "        videoList.append(blocE_title_vid1)\n",
    "        sentimentList.append(blocE_sent_vid1)\n",
    "        activationList.append(blocE_act_vid1)\n",
    "\n",
    "        '''For Video 2'''\n",
    "\n",
    "        coderList.append(coder)\n",
    "        videoList.append(blocE_title_vid2)\n",
    "        sentimentList.append(blocE_sent_vid2)\n",
    "        activationList.append(blocE_act_vid2)\n",
    "        \n",
    "        #-----------------\n",
    "        # BlocF\n",
    "        #-----------------\n",
    "\n",
    "        '''count n of bars, which separate variables'''\n",
    "        numBars = rawQualtricsOutput['BL_cNiU4FegXk1GkCx_DO'][i].count('|') #count seperators (|)       \n",
    "\n",
    "        '''break strings into components, which are separated by bars'''\n",
    "        blocF_DO = findStringParts(rawQualtricsOutput['BL_cNiU4FegXk1GkCx_DO'][i], '|', numBars) \n",
    " \n",
    "        blocF_title_vid1 = blocF_DO[0] # The first video title\n",
    "        blocF_title_vid2 = blocF_DO[1] # The second video title\n",
    "\n",
    "        '''arousal score for first video'''\n",
    "        blocF_act_vid1 = rawQualtricsOutput['QID1063_1'][i] \n",
    "\n",
    "        '''arousal code for second video'''\n",
    "        blocF_act_vid2 = rawQualtricsOutput['QID1063_2'][i] \n",
    "\n",
    "        '''sentiment code for first video'''\n",
    "        blocF_sent_vid1 = rawQualtricsOutput['QID1064_1'][i] \n",
    "\n",
    "        '''sentiment code for second video'''\n",
    "        blocF_sent_vid2 = rawQualtricsOutput['QID1064_2'][i] \n",
    "\n",
    "        '''Add Bloc F data to running Tally'''\n",
    "\n",
    "        '''For Video 1'''\n",
    "        coderList.append(coder)\n",
    "        videoList.append(blocF_title_vid1)\n",
    "        sentimentList.append(blocF_sent_vid1)\n",
    "        activationList.append(blocF_act_vid1)\n",
    "\n",
    "        '''For Video 2'''\n",
    "        coderList.append(coder)\n",
    "        videoList.append(blocF_title_vid2)\n",
    "        sentimentList.append(blocF_sent_vid2)\n",
    "        activationList.append(blocF_act_vid2)\n",
    "\n",
    "\n",
    "        #-----------------\n",
    "        # BlocA1B1\n",
    "        #-----------------\n",
    "\n",
    "        '''count n of bars, which separate variables'''\n",
    "        if pd.notnull(rawQualtricsOutput['BL_0uhk6r0UWlNJ3tb_DO'][i]):\n",
    "\n",
    "            numBars = rawQualtricsOutput['BL_0uhk6r0UWlNJ3tb_DO'][i].count('|') #count seperators (|)       \n",
    "            \n",
    "            '''break strings into components, which are separated by bars'''\n",
    "            blocA1B1_DO = findStringParts(rawQualtricsOutput['BL_0uhk6r0UWlNJ3tb_DO'][i], '|', numBars) \n",
    "            \n",
    "            blocA1B1_title_vid1 = blocA1B1_DO[0] # The first video title\n",
    "            \n",
    "            blocA1B1_title_vid2 = blocA1B1_DO[1] # The second video title\n",
    "            \n",
    "            '''arousal code for first video'''\n",
    "            blocA1B1_act_vid1 = rawQualtricsOutput['QID4911_1'][i] \n",
    "\n",
    "            '''arousal code for second video'''\n",
    "            blocA1B1_act_vid2 = rawQualtricsOutput['QID4911_2'][i] \n",
    "\n",
    "            '''sentiment code for first video'''\n",
    "            blocA1B1_sent_vid1 = rawQualtricsOutput['QID4910_1'][i] \n",
    "\n",
    "            '''sentiment code for second video'''\n",
    "            blocA1B1_sent_vid2 = rawQualtricsOutput['QID4910_2'][i] \n",
    "\n",
    "            \n",
    "            '''Add Bloc A1B1 data to running Tally'''\n",
    "\n",
    "            '''For Video 1'''\n",
    "\n",
    "            coderList.append(coder)\n",
    "            videoList.append(blocA1B1_title_vid1)\n",
    "            sentimentList.append(blocA1B1_sent_vid1)\n",
    "            activationList.append(blocA1B1_act_vid1)\n",
    "\n",
    "            '''For Video 2'''\n",
    "            coderList.append(coder)\n",
    "            videoList.append(blocA1B1_title_vid2)\n",
    "            sentimentList.append(blocA1B1_sent_vid2)\n",
    "            activationList.append(blocA1B1_act_vid2)\n",
    "        \n",
    "        \n",
    "        #-----------------\n",
    "        # BlocA1B2\n",
    "        #-----------------\n",
    "\n",
    "        '''count n of bars, which separate variables'''\n",
    "        if pd.notnull(rawQualtricsOutput['BL_eCHbbimkb0dc5i5_DO'][i]):\n",
    "            numBars = rawQualtricsOutput['BL_eCHbbimkb0dc5i5_DO'][i].count('|') #count seperators (|)\n",
    "\n",
    "            '''count n of bars, which separate variables'''\n",
    "            numBars = rawQualtricsOutput['BL_eCHbbimkb0dc5i5_DO'][i].count('|') #count seperators (|)       \n",
    "\n",
    "            '''break strings into components, which are separated by bars'''\n",
    "            blocA1B2_DO = findStringParts(rawQualtricsOutput['BL_eCHbbimkb0dc5i5_DO'][i], '|', numBars) \n",
    "\n",
    "            blocA1B2_title_vid1 = blocA1B2_DO[0] # The first video title\n",
    "\n",
    "            blocA1B2_title_vid2 = blocA1B2_DO[1] # The second video title\n",
    "\n",
    "            '''arousal code for first video'''\n",
    "            blocA1B2_act_vid1 = rawQualtricsOutput['QID5054_1'][i] \n",
    "\n",
    "            '''arousal code for second video'''\n",
    "            blocA1B2_act_vid2 = rawQualtricsOutput['QID5054_2'][i] \n",
    "\n",
    "            '''sentiment code for first video'''\n",
    "            blocA1B2_sent_vid1 = rawQualtricsOutput['QID5052_1'][i] \n",
    "\n",
    "            '''sentiment code for second video'''\n",
    "            blocA1B2_sent_vid2 = rawQualtricsOutput['QID5052_2'][i] \n",
    "\n",
    "\n",
    "            '''Add Bloc A1B2 data to running Tally'''\n",
    "            '''For Video 1'''\n",
    "\n",
    "            coderList.append(coder)\n",
    "            videoList.append(blocA1B2_title_vid1)\n",
    "            sentimentList.append(blocA1B2_sent_vid1)\n",
    "            activationList.append(blocA1B2_act_vid1)\n",
    "\n",
    "            '''For Video 2'''\n",
    "\n",
    "            coderList.append(coder)\n",
    "            videoList.append(blocA1B2_title_vid2)\n",
    "            sentimentList.append(blocA1B2_sent_vid2)\n",
    "            activationList.append(blocA1B2_act_vid2)\n",
    "\n",
    "\n",
    "        #-----------------\n",
    "        # BlocA1F1\n",
    "        #-----------------\n",
    "        \n",
    "        '''count n of bars, which separate variables'''\n",
    "\n",
    "        if pd.notnull(rawQualtricsOutput['BL_0kXaBSNouGm7OXH_DO'][i]):\n",
    "\n",
    "            numBars = rawQualtricsOutput['BL_0kXaBSNouGm7OXH_DO'][i].count('|') #count seperators (|)\n",
    "\n",
    "            '''count n of bars, which separate variables'''\n",
    "            numBars = rawQualtricsOutput['BL_0kXaBSNouGm7OXH_DO'][i].count('|') #count seperators (|)       \n",
    "\n",
    "            '''break strings into components, which are separated by bars'''\n",
    "            blocA1F1_DO = findStringParts(rawQualtricsOutput['BL_0kXaBSNouGm7OXH_DO'][i], '|', numBars) \n",
    "            blocA1F1_title_vid1 = blocA1F1_DO[0] # The first video title\n",
    "            blocA1F1_title_vid2 = blocA1F1_DO[1] # The second video title\n",
    "\n",
    "            '''arousal code for first video'''\n",
    "            blocA1F1_act_vid1 = rawQualtricsOutput['QID5342_1'][i] \n",
    "\n",
    "            '''arousal code for second video'''\n",
    "            blocA1F1_act_vid2 = rawQualtricsOutput['QID5342_2'][i] \n",
    "\n",
    "            '''sentiment code for first video'''\n",
    "            blocA1F1_sent_vid1 = rawQualtricsOutput['QID5341_1'][i] \n",
    "\n",
    "            '''sentiment code for second video'''\n",
    "            blocA1F1_sent_vid2 = rawQualtricsOutput['QID5341_2'][i] \n",
    "        \n",
    "\n",
    "            '''Add Bloc A1F1 data to running Tally'''\n",
    "\n",
    "            '''For Video 1'''\n",
    "\n",
    "            coderList.append(coder)\n",
    "            videoList.append(blocA1F1_title_vid1)\n",
    "            sentimentList.append(blocA1F1_sent_vid1)\n",
    "            activationList.append(blocA1F1_act_vid1)\n",
    "\n",
    "            '''For Video 2'''\n",
    "\n",
    "            coderList.append(coder)\n",
    "            videoList.append(blocA1F1_title_vid2)\n",
    "            sentimentList.append(blocA1F1_sent_vid2)\n",
    "            activationList.append(blocA1F1_act_vid2)\n",
    "\n",
    "\n",
    "        #-----------------\n",
    "        # BlocA1F2\n",
    "        #-----------------\n",
    "\n",
    "        '''count n of bars, which separate variables'''\n",
    "\n",
    "        if pd.notnull(rawQualtricsOutput['BL_0kTwd0XKn0tOQ7z_DO'][i]):\n",
    "            numBars = rawQualtricsOutput['BL_0kTwd0XKn0tOQ7z_DO'][i].count('|') #count seperators (|)     \n",
    "\n",
    "            '''break strings into components, which are separated by bars'''\n",
    "            blocA1F2_DO = findStringParts(rawQualtricsOutput['BL_0kTwd0XKn0tOQ7z_DO'][i], '|', numBars) \n",
    "            blocA1F2_title_vid1 = blocA1F2_DO[0] # The first video title\n",
    "            blocA1F2_title_vid2 = blocA1F2_DO[1] # The second video title\n",
    "\n",
    "            '''arousal code for first video'''\n",
    "            blocA1F2_act_vid1 = rawQualtricsOutput['QID5520_1'][i] \n",
    "\n",
    "            '''arousal code for second video'''\n",
    "            blocA1F2_act_vid2 = rawQualtricsOutput['QID5520_2'][i] \n",
    "\n",
    "            '''sentiment code for first video'''\n",
    "            blocA1F2_sent_vid1 = rawQualtricsOutput['QID5522_1'][i] \n",
    "\n",
    "            '''sentiment code for second video'''\n",
    "            blocA1F2_sent_vid2 = rawQualtricsOutput['QID5522_2'][i] \n",
    "\n",
    "            '''Add Bloc A1F2 data to running Tally'''\n",
    "            '''For Video 1'''\n",
    "            coderList.append(coder)\n",
    "            videoList.append(blocA1F2_title_vid1)\n",
    "            sentimentList.append(blocA1F2_sent_vid1)\n",
    "            activationList.append(blocA1F2_act_vid1)\n",
    "\n",
    "            '''For Video 2'''\n",
    "            coderList.append(coder)\n",
    "            videoList.append(blocA1F2_title_vid2)\n",
    "            sentimentList.append(blocA1F2_sent_vid2)\n",
    "            activationList.append(blocA1F2_act_vid2)\n",
    "\n",
    "\n",
    "        #-----------------\n",
    "        # BlocA2F1\n",
    "        #-----------------\n",
    "\n",
    "        '''count n of bars, which separate variables'''\n",
    "\n",
    "        if pd.notnull(rawQualtricsOutput['BL_8expPTVNXuTVOUB_DO'][i]):\n",
    "            numBars = rawQualtricsOutput['BL_8expPTVNXuTVOUB_DO'][i].count('|') #count seperators (|)       \n",
    "\n",
    "            '''break strings into components, which are separated by bars'''\n",
    "            blocA2F1_DO = findStringParts(rawQualtricsOutput['BL_8expPTVNXuTVOUB_DO'][i], '|', numBars) \n",
    "            blocA2F1_title_vid1 = blocA2F1_DO[0] # The first video title\n",
    "            blocA2F1_title_vid2 = blocA2F1_DO[1] # The second video title\n",
    "\n",
    "            '''arousal code for first video'''\n",
    "            blocA2F1_act_vid1 = rawQualtricsOutput['QID5693_1'][i] \n",
    "\n",
    "            '''arousal code for second video'''\n",
    "            blocA2F1_act_vid2 = rawQualtricsOutput['QID5693_2'][i] \n",
    "\n",
    "            '''sentiment code for first video'''\n",
    "            blocA2F1_sent_vid1 = rawQualtricsOutput['QID5695_1'][i] \n",
    "\n",
    "            '''sentiment code for second video'''\n",
    "            blocA2F1_sent_vid2 = rawQualtricsOutput['QID5695_2'][i] \n",
    "            \n",
    "            \n",
    "            '''Add Bloc A2F1 data to running Tally'''\n",
    "            '''For Video 1'''\n",
    "            coderList.append(coder)\n",
    "            videoList.append(blocA2F1_title_vid1)\n",
    "            sentimentList.append(blocA2F1_sent_vid1)\n",
    "            activationList.append(blocA2F1_act_vid1)\n",
    "\n",
    "            '''For Video 2'''\n",
    "            coderList.append(coder)\n",
    "            videoList.append(blocA2F1_title_vid2)\n",
    "            sentimentList.append(blocA2F1_sent_vid2)\n",
    "            activationList.append(blocA2F1_act_vid2)\n",
    "            \n",
    "        #-----------------\n",
    "        # BlocA2B1\n",
    "        #-----------------\n",
    "\n",
    "        '''count n of bars, which separate variables'''\n",
    "        if pd.notnull(rawQualtricsOutput['BL_6tBHFflywlCq6nb_DO'][i]):\n",
    "            \n",
    "            '''count n of bars, which separate variables'''\n",
    "            numBars = rawQualtricsOutput['BL_6tBHFflywlCq6nb_DO'][i].count('|') #count seperators (|)       \n",
    "\n",
    "            '''break strings into components, which are separated by bars'''\n",
    "            blocA2B1_DO = findStringParts(rawQualtricsOutput['BL_6tBHFflywlCq6nb_DO'][i], '|', numBars) \n",
    "            blocA2B1_title_vid1 = blocA2B1_DO[0] # The first video title\n",
    "            blocA2B1_title_vid2 = blocA2B1_DO[1] # The second video title\n",
    "\n",
    "            '''arousal code for first video'''\n",
    "            blocA2B1_act_vid1 = rawQualtricsOutput['QID5781_1'][i] \n",
    "\n",
    "            '''arousal code for second video'''\n",
    "            blocA2B1_act_vid2 = rawQualtricsOutput['QID5781_2'][i] \n",
    "\n",
    "            '''sentiment code for first video'''\n",
    "            blocA2B1_sent_vid1 = rawQualtricsOutput['QID5782_1'][i] \n",
    "\n",
    "            '''sentiment code for second video'''\n",
    "            blocA2B1_sent_vid2 = rawQualtricsOutput['QID5782_2'][i] \n",
    "\n",
    "\n",
    "            '''Add Bloc A2B1 data to running Tally'''\n",
    "            '''For Video 1'''\n",
    "            coderList.append(coder)\n",
    "            videoList.append(blocA2B1_title_vid1)\n",
    "            sentimentList.append(blocA2B1_sent_vid1)\n",
    "            activationList.append(blocA2B1_act_vid1)\n",
    "\n",
    "            '''For Video 2'''\n",
    "            coderList.append(coder)\n",
    "            videoList.append(blocA2B1_title_vid2)\n",
    "            sentimentList.append(blocA2B1_sent_vid2)\n",
    "            activationList.append(blocA2B1_act_vid2)\n",
    "            \n",
    "        #-----------------\n",
    "        # BlocA2B2\n",
    "        #-----------------\n",
    "\n",
    "        '''count n of bars, which separate variables'''\n",
    "\n",
    "        if pd.notnull(rawQualtricsOutput['BL_42F6yra3ogqCtpP_DO'][i]):\n",
    "\n",
    "            '''count n of bars, which separate variables'''\n",
    "            numBars = rawQualtricsOutput['BL_42F6yra3ogqCtpP_DO'][i].count('|') #count seperators (|)\n",
    "            \n",
    "            '''break strings into components, which are separated by bars'''\n",
    "            blocA2B2_DO = findStringParts(rawQualtricsOutput['BL_42F6yra3ogqCtpP_DO'][i], '|', numBars) \n",
    "            blocA2B2_title_vid1 = blocA2B2_DO[0] # The first video title\n",
    "            blocA2B2_title_vid2 = blocA2B2_DO[1] # The second video title\n",
    "\n",
    "            '''arousal code for first video'''\n",
    "            blocA2B2_act_vid1 = rawQualtricsOutput['QID5954_1'][i] \n",
    "\n",
    "            '''arousal code for second video'''\n",
    "            blocA2B2_act_vid2 = rawQualtricsOutput['QID5954_2'][i] \n",
    "\n",
    "            '''sentiment code for first video'''\n",
    "            blocA2B2_sent_vid1 = rawQualtricsOutput['QID5955_1'][i] \n",
    "\n",
    "            '''sentiment code for second video'''\n",
    "            blocA2B2_sent_vid2 = rawQualtricsOutput['QID5955_2'][i] \n",
    "\n",
    "\n",
    "            '''Add Bloc A2B2 data to running Tally'''\n",
    "            '''For Video 1'''\n",
    "            coderList.append(coder)\n",
    "            videoList.append(blocA2B2_title_vid1)\n",
    "            sentimentList.append(blocA2B2_sent_vid1)\n",
    "            activationList.append(blocA2B2_act_vid1)\n",
    "\n",
    "            '''For Video 2'''\n",
    "            coderList.append(coder)\n",
    "            videoList.append(blocA2B2_title_vid2)\n",
    "            sentimentList.append(blocA2B2_sent_vid2)\n",
    "            activationList.append(blocA2B2_act_vid2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.A.3 Save Structured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Video Labelling Error\n",
    "videoList = [video.replace('2017 12 1 ','2017 12 01 ' ) for video in videoList]\n",
    "\n",
    "'''Combine Running Tally Lists in Dictionary'''\n",
    "qualtricsStructured = {'Coder': coderList,\n",
    "                       'Video': videoList,\n",
    "                       'Sentiment': sentimentList,\n",
    "                       'Activation': activationList\n",
    "                      }\n",
    "\n",
    "'''Convert to Pandas DF'''\n",
    "qualtricsStructured = pd.DataFrame(qualtricsStructured)\n",
    "\n",
    "'''Assign Missing Values'''\n",
    "qualtricsStructured = qualtricsStructured.replace('-99',pd.NaT)\n",
    "\n",
    "'''Drop Missing Values, as there was no reason for coders to produce them\n",
    "and only one coder did, as if he was interrupted in the middle\n",
    "of coding a single video.  This could have been a connection issue.'''\n",
    "\n",
    "qualtricsStructured = qualtricsStructured.dropna()\n",
    "\n",
    "qualtricsStructured = qualtricsStructured[['Video', 'Coder', 'Sentiment', \n",
    "                                           'Activation']]\n",
    "\n",
    "qualtricsStructured.index.name = 'Order'\n",
    "\n",
    "'''Export to CSV'''\n",
    "qualtricsStructured.to_csv('qualtricsStructured.csv', encoding='utf-8')\n",
    "\n",
    "qualtricsStructured[qualtricsStructured['Video'] == \"2017 10 05 1\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.A.3 Extract First Two Codes of Each Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the first two codes because we later compare the consistency of video coders to the consistency of text coders, and text coders coded each sentence twice.  If we keep all of the video codes, then video codes may become more reliable simply by virtue of averaging a greater number of observations for each coder.  We avoid the problem of comparing the reliability of video and text coders by contraining the analysis to the first two coding decisions of each coder for each video.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Choose first two coding decisions\n",
    "qualtricsStructured['Sequence'] = qualtricsStructured.groupby(['Coder', 'Video']).cumcount()\n",
    "qualtricsStructured.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualtricsStructured = qualtricsStructured[qualtricsStructured['Sequence'] < 2]\n",
    "qualtricsStructured.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualtricsStructured['Mode'] = \"Video\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linking the qualtrics output to metadata about the extracted segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualtricsStructured = qualtricsStructured.merge(hansardExtractedVideos, how='left', on='Video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualtricsStructured.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "firstTwo = qualtricsStructured[qualtricsStructured['Sequence'] <= 1]\n",
    "firstTwo['Sentiment'] = pd.to_numeric(firstTwo['Sentiment'])\n",
    "firstTwo['Activation'] = pd.to_numeric(firstTwo['Activation'])\n",
    "\n",
    "firstTwo = firstTwo.assign(sentimentFirst = pd.to_numeric(firstTwo['Sentiment'])) #create sentimentFirst assign Sentiment\n",
    "firstTwo.loc[firstTwo['Sequence'] == 1, 'sentimentFirst'] = np.NaN #replace sentimentFirst with NaN if Sequence with vals (0,1) = 1\n",
    "firstTwo = firstTwo.assign(sentimentSecond = pd.to_numeric(firstTwo['Sentiment'])) #create sentimentSecond assign Sentiment\n",
    "firstTwo.loc[firstTwo['Sequence'] == 0, 'sentimentSecond'] = np.NaN #replace sentimentSecond with NaN if Sequence with vals (0,1) = 0\n",
    "\n",
    "firstTwo = firstTwo.assign(activationFirst = pd.to_numeric(firstTwo['Activation'])) #create sentimentFirst assign Sentiment\n",
    "firstTwo.loc[firstTwo['Sequence'] == 1, 'activationFirst'] = np.NaN #replace sentimentFirst with NaN if Sequence with vals (0,1) = 1\n",
    "firstTwo = firstTwo.assign(activationSecond = pd.to_numeric(firstTwo['Activation'])) #create sentimentSecond assign Sentiment\n",
    "firstTwo.loc[firstTwo['Sequence'] == 0, 'activationSecond'] = np.NaN #replace sentimentSecond with NaN if Sequence with vals (0,1) = 0\n",
    "\n",
    "videoAverages = firstTwo.groupby(['Coder', 'Video']).agg({\n",
    "    'Video': 'first',\n",
    "    'Coder': 'first',\n",
    "    'sentimentFirst': 'mean',\n",
    "    'sentimentSecond': 'mean',\n",
    "    'activationFirst': 'mean',\n",
    "    'activationSecond': 'mean',\n",
    "    'Sentiment': 'mean',\n",
    "    'Activation': 'mean'})\n",
    "\n",
    "videoAverages.head()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Text Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We asked three independent coders to code the sentiment and activation of the transcripts of each video clip.  The order of the transcripts were randomized.  We then had the same coders return, three months later, to redo their coding. The following script extracts these codes and integrates them with the results of the video coding.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. A. Append together the text coding files for each coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jv1 = pd.read_csv('jv_coding1.csv')\n",
    "jv2 = pd.read_csv('jv_coding2.csv')\n",
    "cm1 = pd.read_csv('cm_coding1.csv')\n",
    "cm2 = pd.read_csv('cm_coding2.csv')\n",
    "sf1 = pd.read_csv('sf_coding1.csv')\n",
    "sf2 = pd.read_csv('sf_coding2.csv')\n",
    "\n",
    "jv1['Sequence'] = 0\n",
    "cm1['Sequence'] = 0\n",
    "sf1['Sequence'] = 0\n",
    "jv2['Sequence'] = 1\n",
    "cm2['Sequence'] = 1\n",
    "sf2['Sequence'] = 1\n",
    "\n",
    "jv1['Coder'] = 'JV'\n",
    "jv2['Coder'] = 'JV'\n",
    "sf1['Coder'] = 'SF'\n",
    "sf2['Coder'] = 'SF'\n",
    "cm1['Coder'] = 'CM'\n",
    "cm2['Coder'] = 'CM'\n",
    "\n",
    "textScores = pd.DataFrame(columns=['Label', 'Coder', 'Sentiment', 'Activation', 'Sequence'])\n",
    "textScores = textScores.append([jv1, jv2, sf1, sf2, cm1, cm2])\n",
    "textScores['Mode'] = \"Text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textScores.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. B. Linking speech data to extracted data about the speech segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textScores = textScores.merge(hansardExtractedVideos, how=\"left\", on = \"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. C. Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(textScores.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textScores = textScores.drop(columns=['Video_x', 'Speaker_x', 'Party', 'Shuffle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textScores.rename(columns = {'Video_y': 'Video', 'Video_x': 'Video', 'Speaker_y': 'Speaker'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textScores = textScores.sort_values(by=['Video', 'Coder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textScores.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Appending Text and Video Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingData = pd.DataFrame(columns = ['Activation', 'Coder', 'Label', 'Sentiment', 'Sequence', 'englishHansard', 'Mode', 'Date', 'ID_main', 'youTube', 'timeStamp', 'Speaker', 'French', 'party', 'seconds', 'english', 'floor', 'Video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingData = fullCodingData.append([textScores, qualtricsStructured])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingData.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Aggregating Data to Video Level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo = fullCodingData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates for each video its average sentiment and activation scores from each coder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo[['Sentiment', 'Activation']] = fullCodingDataVideo[['Sentiment', 'Activation']].apply(pd.to_numeric)\n",
    "fullCodingDataVideo['coderMeanSent'] = fullCodingDataVideo.groupby(['Video', 'Coder'])['Sentiment'].transform('mean')\n",
    "fullCodingDataVideo['coderMeanAct'] = fullCodingDataVideo.groupby(['Video', 'Coder'])['Activation'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Captures the first and second scores from each coder for each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo['sent1'] = np.where(fullCodingDataVideo['Sequence']==0, fullCodingDataVideo['Sentiment'], np.NaN)\n",
    "fullCodingDataVideo['sent2'] = np.where(fullCodingDataVideo['Sequence']==1, fullCodingDataVideo['Sentiment'], np.NaN)\n",
    "fullCodingDataVideo['act1'] = np.where(fullCodingDataVideo['Sequence']==0, fullCodingDataVideo['Activation'], np.NaN)\n",
    "fullCodingDataVideo['act2'] = np.where(fullCodingDataVideo['Sequence']==1, fullCodingDataVideo['Activation'], np.NaN)\n",
    "fullCodingDataVideo['sent1'] = fullCodingDataVideo.groupby(['Video', 'Coder'])['sent1'].transform('mean')\n",
    "fullCodingDataVideo['sent2'] = fullCodingDataVideo.groupby(['Video', 'Coder'])['sent2'].transform('mean')\n",
    "fullCodingDataVideo['act1'] = fullCodingDataVideo.groupby(['Video', 'Coder'])['act1'].transform('mean')\n",
    "fullCodingDataVideo['act2'] = fullCodingDataVideo.groupby(['Video', 'Coder'])['act2'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the coder scores into columns for arrangement at the video level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both activation and sentiment, the following section will create seperate columns in the dataframe for the first, second, and average coding scores assigned to each video/snippet by each coder. E.g., each video will have columns for t1Sent1, t1Sent2, t1SentAvg, t1Act1, ...., v3ActAvg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo['Coder'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Coder SF, extract first and second code for each video, as well as the average of the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo['t1Sent1'] = np.where(fullCodingDataVideo['Coder']==\"SF\", fullCodingDataVideo['sent1'], np.NaN) #the first sentiment score if coder == SF\n",
    "fullCodingDataVideo['t1Act1'] = np.where(fullCodingDataVideo['Coder']==\"SF\", fullCodingDataVideo['act1'], np.NaN)\n",
    "fullCodingDataVideo['t1Sent2'] = np.where(fullCodingDataVideo['Coder']==\"SF\", fullCodingDataVideo['sent2'], np.NaN) #the second sentiment score if coder == SF\n",
    "fullCodingDataVideo['t1Act2'] = np.where(fullCodingDataVideo['Coder']==\"SF\", fullCodingDataVideo['act2'], np.NaN)\n",
    "fullCodingDataVideo['t1SentAvg'] = np.where(fullCodingDataVideo['Coder']==\"SF\", fullCodingDataVideo['coderMeanSent'], np.NaN) #the average sentiment score if coder == SF\n",
    "fullCodingDataVideo['t1ActAvg'] = np.where(fullCodingDataVideo['Coder']==\"SF\", fullCodingDataVideo['coderMeanAct'], np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the columns extracted above to all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo['t1Sent1'] = fullCodingDataVideo.groupby(['Video'])['t1Sent1'].transform('mean')\n",
    "fullCodingDataVideo['t1Sent2'] = fullCodingDataVideo.groupby(['Video'])['t1Sent2'].transform('mean') \n",
    "fullCodingDataVideo['t1SentAvg'] = fullCodingDataVideo.groupby(['Video'])['t1SentAvg'].transform('mean') \n",
    "fullCodingDataVideo['t1Act1'] = fullCodingDataVideo.groupby(['Video'])['t1Act1'].transform('mean')\n",
    "fullCodingDataVideo['t1Act2'] = fullCodingDataVideo.groupby(['Video'])['t1Act2'].transform('mean')\n",
    "fullCodingDataVideo['t1ActAvg'] = fullCodingDataVideo.groupby(['Video'])['t1ActAvg'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Coder CM, extract first and second code for each video, as well as the average of the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo['t2Sent1'] = np.where(fullCodingDataVideo['Coder']==\"CM\", fullCodingDataVideo['sent1'], np.NaN) #the first sentiment score if coder == SF\n",
    "fullCodingDataVideo['t2Act1'] = np.where(fullCodingDataVideo['Coder']==\"CM\", fullCodingDataVideo['act1'], np.NaN)\n",
    "fullCodingDataVideo['t2Sent2'] = np.where(fullCodingDataVideo['Coder']==\"CM\", fullCodingDataVideo['sent2'], np.NaN) #the second sentiment score if coder == SF\n",
    "fullCodingDataVideo['t2Act2'] = np.where(fullCodingDataVideo['Coder']==\"CM\", fullCodingDataVideo['act2'], np.NaN)\n",
    "fullCodingDataVideo['t2SentAvg'] = np.where(fullCodingDataVideo['Coder']==\"CM\", fullCodingDataVideo['coderMeanSent'], np.NaN) #the average sentiment score if coder == SF\n",
    "fullCodingDataVideo['t2ActAvg'] = np.where(fullCodingDataVideo['Coder']==\"CM\", fullCodingDataVideo['coderMeanAct'], np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the columns extracted above to all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo['t2Sent1'] = fullCodingDataVideo.groupby(['Video'])['t2Sent1'].transform('mean')\n",
    "fullCodingDataVideo['t2Sent2'] = fullCodingDataVideo.groupby(['Video'])['t2Sent2'].transform('mean') \n",
    "fullCodingDataVideo['t2SentAvg'] = fullCodingDataVideo.groupby(['Video'])['t2SentAvg'].transform('mean') \n",
    "fullCodingDataVideo['t2Act1'] = fullCodingDataVideo.groupby(['Video'])['t2Act1'].transform('mean')\n",
    "fullCodingDataVideo['t2Act2'] = fullCodingDataVideo.groupby(['Video'])['t2Act2'].transform('mean')\n",
    "fullCodingDataVideo['t2ActAvg'] = fullCodingDataVideo.groupby(['Video'])['t2ActAvg'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Coder JV, extract first and second code for each video, as well as the average of the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo['t3Sent1'] = np.where(fullCodingDataVideo['Coder']==\"JV\", fullCodingDataVideo['sent1'], np.NaN) #the first sentiment score if coder == SF\n",
    "fullCodingDataVideo['t3Act1'] = np.where(fullCodingDataVideo['Coder']==\"JV\", fullCodingDataVideo['act1'], np.NaN)\n",
    "fullCodingDataVideo['t3Sent2'] = np.where(fullCodingDataVideo['Coder']==\"JV\", fullCodingDataVideo['sent2'], np.NaN) #the second sentiment score if coder == SF\n",
    "fullCodingDataVideo['t3Act2'] = np.where(fullCodingDataVideo['Coder']==\"JV\", fullCodingDataVideo['act2'], np.NaN)\n",
    "fullCodingDataVideo['t3SentAvg'] = np.where(fullCodingDataVideo['Coder']==\"JV\", fullCodingDataVideo['coderMeanSent'], np.NaN) #the average sentiment score if coder == SF\n",
    "fullCodingDataVideo['t3ActAvg'] = np.where(fullCodingDataVideo['Coder']==\"JV\", fullCodingDataVideo['coderMeanAct'], np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the columns extracted above to all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo['t3Sent1'] = fullCodingDataVideo.groupby(['Video'])['t3Sent1'].transform('mean')\n",
    "fullCodingDataVideo['t3Sent2'] = fullCodingDataVideo.groupby(['Video'])['t3Sent2'].transform('mean') \n",
    "fullCodingDataVideo['t3SentAvg'] = fullCodingDataVideo.groupby(['Video'])['t3SentAvg'].transform('mean') \n",
    "fullCodingDataVideo['t3Act1'] = fullCodingDataVideo.groupby(['Video'])['t3Act1'].transform('mean')\n",
    "fullCodingDataVideo['t3Act2'] = fullCodingDataVideo.groupby(['Video'])['t3Act2'].transform('mean')\n",
    "fullCodingDataVideo['t3ActAvg'] = fullCodingDataVideo.groupby(['Video'])['t3ActAvg'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Coder PO, extract first and second code for each video, as well as the average of the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo['v1Sent1'] = np.where(fullCodingDataVideo['Coder']==\"P-O R. B.\", fullCodingDataVideo['sent1'], np.NaN) #the first sentiment score if coder == SF\n",
    "fullCodingDataVideo['v1Act1'] = np.where(fullCodingDataVideo['Coder']==\"P-O R. B.\", fullCodingDataVideo['act1'], np.NaN)\n",
    "fullCodingDataVideo['v1Sent2'] = np.where(fullCodingDataVideo['Coder']==\"P-O R. B.\", fullCodingDataVideo['sent2'], np.NaN) #the second sentiment score if coder == SF\n",
    "fullCodingDataVideo['v1Act2'] = np.where(fullCodingDataVideo['Coder']==\"P-O R. B.\", fullCodingDataVideo['act2'], np.NaN)\n",
    "fullCodingDataVideo['v1SentAvg'] = np.where(fullCodingDataVideo['Coder']==\"P-O R. B.\", fullCodingDataVideo['coderMeanSent'], np.NaN) #the average sentiment score if coder == SF\n",
    "fullCodingDataVideo['v1ActAvg'] = np.where(fullCodingDataVideo['Coder']==\"P-O R. B.\", fullCodingDataVideo['coderMeanAct'], np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the columns extracted above to all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo['v1Sent1'] = fullCodingDataVideo.groupby(['Video'])['v1Sent1'].transform('mean')\n",
    "fullCodingDataVideo['v1Sent2'] = fullCodingDataVideo.groupby(['Video'])['v1Sent2'].transform('mean') \n",
    "fullCodingDataVideo['v1SentAvg'] = fullCodingDataVideo.groupby(['Video'])['v1SentAvg'].transform('mean') \n",
    "fullCodingDataVideo['v1Act1'] = fullCodingDataVideo.groupby(['Video'])['v1Act1'].transform('mean')\n",
    "fullCodingDataVideo['v1Act2'] = fullCodingDataVideo.groupby(['Video'])['v1Act2'].transform('mean')\n",
    "fullCodingDataVideo['v1ActAvg'] = fullCodingDataVideo.groupby(['Video'])['v1ActAvg'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Coder MS, extract first and second code for each video, as well as the average of the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo['v2Sent1'] = np.where(fullCodingDataVideo['Coder']==\"MS\", fullCodingDataVideo['sent1'], np.NaN) #the first sentiment score if coder == SF\n",
    "fullCodingDataVideo['v2Act1'] = np.where(fullCodingDataVideo['Coder']==\"MS\", fullCodingDataVideo['act1'], np.NaN)\n",
    "fullCodingDataVideo['v2Sent2'] = np.where(fullCodingDataVideo['Coder']==\"MS\", fullCodingDataVideo['sent2'], np.NaN) #the second sentiment score if coder == SF\n",
    "fullCodingDataVideo['v2Act2'] = np.where(fullCodingDataVideo['Coder']==\"MS\", fullCodingDataVideo['act2'], np.NaN)\n",
    "fullCodingDataVideo['v2SentAvg'] = np.where(fullCodingDataVideo['Coder']==\"MS\", fullCodingDataVideo['coderMeanSent'], np.NaN) #the average sentiment score if coder == SF\n",
    "fullCodingDataVideo['v2ActAvg'] = np.where(fullCodingDataVideo['Coder']==\"MS\", fullCodingDataVideo['coderMeanAct'], np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the columns extracted above to all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo['v2Sent1'] = fullCodingDataVideo.groupby(['Video'])['v2Sent1'].transform('mean')\n",
    "fullCodingDataVideo['v2Sent2'] = fullCodingDataVideo.groupby(['Video'])['v2Sent2'].transform('mean') \n",
    "fullCodingDataVideo['v2SentAvg'] = fullCodingDataVideo.groupby(['Video'])['v2SentAvg'].transform('mean') \n",
    "fullCodingDataVideo['v2Act1'] = fullCodingDataVideo.groupby(['Video'])['v2Act1'].transform('mean')\n",
    "fullCodingDataVideo['v2Act2'] = fullCodingDataVideo.groupby(['Video'])['v2Act2'].transform('mean')\n",
    "fullCodingDataVideo['v2ActAvg'] = fullCodingDataVideo.groupby(['Video'])['v2ActAvg'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Coder JS, extract first and second code for each video, as well as the average of the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo['v3Sent1'] = np.where(fullCodingDataVideo['Coder']==\"JS\", fullCodingDataVideo['sent1'], np.NaN) #the first sentiment score if coder == SF\n",
    "fullCodingDataVideo['v3Act1'] = np.where(fullCodingDataVideo['Coder']==\"JS\", fullCodingDataVideo['act1'], np.NaN)\n",
    "fullCodingDataVideo['v3Sent2'] = np.where(fullCodingDataVideo['Coder']==\"JS\", fullCodingDataVideo['sent2'], np.NaN) #the second sentiment score if coder == SF\n",
    "fullCodingDataVideo['v3Act2'] = np.where(fullCodingDataVideo['Coder']==\"JS\", fullCodingDataVideo['act2'], np.NaN)\n",
    "fullCodingDataVideo['v3SentAvg'] = np.where(fullCodingDataVideo['Coder']==\"JS\", fullCodingDataVideo['coderMeanSent'], np.NaN) #the average sentiment score if coder == SF\n",
    "fullCodingDataVideo['v3ActAvg'] = np.where(fullCodingDataVideo['Coder']==\"JS\", fullCodingDataVideo['coderMeanAct'], np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the columns extracted above to all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideo['v3Sent1'] = fullCodingDataVideo.groupby(['Video'])['v3Sent1'].transform('mean')\n",
    "fullCodingDataVideo['v3Sent2'] = fullCodingDataVideo.groupby(['Video'])['v3Sent2'].transform('mean') \n",
    "fullCodingDataVideo['v3SentAvg'] = fullCodingDataVideo.groupby(['Video'])['v3SentAvg'].transform('mean') \n",
    "fullCodingDataVideo['v3Act1'] = fullCodingDataVideo.groupby(['Video'])['v3Act1'].transform('mean')\n",
    "fullCodingDataVideo['v3Act2'] = fullCodingDataVideo.groupby(['Video'])['v3Act2'].transform('mean')\n",
    "fullCodingDataVideo['v3ActAvg'] = fullCodingDataVideo.groupby(['Video'])['v3ActAvg'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 1 for each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideoOnePer = fullCodingDataVideo.groupby(['Label']).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(fullCodingDataVideoOnePer.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns not measured at video level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideoOnePer = fullCodingDataVideoOnePer.drop(columns=['Activation', \n",
    "                                                                    'Coder', \n",
    "                                                                    'ID_main', \n",
    "                                                                    'Mode', \n",
    "                                                                    'Sentiment',\n",
    "                                                                    'Sequence',\n",
    "                                                                    'englishHansard',\n",
    "                                                                    'coderMeanSent',\n",
    "                                                                    'coderMeanAct',\n",
    "                                                                    'sent1',\n",
    "                                                                    'sent2',\n",
    "                                                                    'act1',\n",
    "                                                                    'act2'\n",
    "                                                                   ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideoOnePer.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCodingDataVideoOnePer.to_csv('fullCodingDataVideoOnePer.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Coder Reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use coderReliability.R in R-Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII. Hansard Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script accesses the directory /Hansard/forParser, extracts all xml files, and then parses them into a dataframe format.  It requires the file authorityFile.p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XML parser for Canadian Record of Parliamentary Debates\n",
    "#version: C.Cochrane, 2018-01-03\n",
    "\n",
    "######################################################################################################################\n",
    "# OVERVIEW\n",
    "######################################################################################################################\n",
    "#Notes:     (1) Python 3.6 Anaconda\n",
    "#           (2) Encoding UTF-8\n",
    "#           (3) Runs from parent directory of Hansard folder.\n",
    "\n",
    "#\n",
    "#Modules:   (1) xml.etree.ElementTree\n",
    "#               see https://docs.python.org/3.3/library/xml.etree.elementtree.html\n",
    "#           (2) os\n",
    "#               see https://docs.python.org/2/library/os.html\n",
    "#           (3) re\n",
    "#               see\n",
    "#           (4) Pandas\n",
    "#               see\n",
    "#\n",
    "# Data:     (1) xml from www.ourcommons.ca\n",
    "#               (e.g., http://www.ourcommons.ca/DocumentViewer/en/42-1/house/sitting-200/hansard)\n",
    "######################################################################################################################\n",
    "# INITIALIZE\n",
    "######################################################################################################################\n",
    "#Load Modules\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "#Load Authority File\n",
    "\n",
    "authorityFile = pickle.load( open(\"authorityFile.p\", \"rb\"))\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################\n",
    "# DEFINING FUNCTIONS FOR NLP TOOLS\n",
    "##########################################################################################################\n",
    "#Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "#Spacy Parser\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "# Hansard XML Function for extracting Speeches and Metadata about Speeches\n",
    "######################################################################################################################\n",
    "\n",
    "def hansardXmlParserSpeeches(directory=\"Hansard/forParser/\"): #accepts path of folder containing xml files\n",
    "\n",
    "\n",
    "\n",
    "    # Notes re Structure of XML Schema from ourcommons.ca.\n",
    "    #            (1) \"Children\" are categories that begin within some\n",
    "    #                 other category. (denoted by a \"tag\")\n",
    "    #            (2) \"Parent\" is the category within which some other\n",
    "    #                 category begins.\n",
    "    #            (3) \"Geneology\" traces parent-child connections back\n",
    "    #                 through earlier generations. Geneological terms\n",
    "    #                 are used here as they are in everyday speech (.e.g,\n",
    "    #                 grandparents, great-grandparents, uncles, siblings...)\n",
    "    #            (4)  \"Attributes\" are traits passed on to later\n",
    "    #                 generations (children, etc). They are always shared by siblings.\n",
    "\n",
    "    # Rules:\n",
    "    #           (1) The Parliament's xml schema is not obviously hierarchical/linear.\n",
    "    #               The following rules are necessary to make sense\n",
    "    #               of the schema:\n",
    "    #                   a) All parents pass down their attributes to their\n",
    "    #                      children (forward inheritance).\n",
    "    #                   b) All parents possess the attributes\n",
    "    #                      of their children, even when the attribute first\n",
    "    #                      appears at the level of the child (backward\n",
    "    #                      inheritance for one level).\n",
    "    #                   c) All siblings possess the same\n",
    "    #                      attributes, even when the attribute is only listed for\n",
    "    #                      one sibling and not the other (horizontal\n",
    "    #                      inheritance).\n",
    "    #                   d) Taking a), b) and c) together, attributes that first\n",
    "    #                      appear in cousins are passed up to the cousin's\n",
    "    #                      parent (i.e., aunt) via backward inheritance,\n",
    "    #                      across to the the child's parent via horizontal\n",
    "    #                      inheritance, and down to the child via forward\n",
    "    #                      inheritance.  In short, children inherit the\n",
    "    #                      attributes of their cousins.\n",
    "    #\n",
    "    # ================================================================\n",
    "\n",
    "    dfList = []\n",
    "\n",
    "    fileList = glob.glob(str(directory)+'*.XML')  #extract list of xml files from the indicated directory\n",
    "    #print(\"FileList\", fileList)\n",
    "    print(\"LengthOfFileList\", len(fileList))\n",
    "    #for file in fileList:\n",
    "    #    print(str(file))\n",
    "    #Declaring variables and default values\n",
    "\n",
    "    topLine=None\n",
    "    secondLine=None\n",
    "    orderOfBusinessRubric=None\n",
    "    subjectOfBusinessTitle=None\n",
    "    subjectOfBusinessID=None\n",
    "    subjectOfBusinessQualifier = None\n",
    "    speechId = None\n",
    "    interventionId = None\n",
    "    date = None\n",
    "    year = None\n",
    "    month = None\n",
    "    day = None\n",
    "    weekday = None\n",
    "    speakerName = None\n",
    "    affiliationType = None\n",
    "    affiliationDbId = None\n",
    "    floorLanguage = None\n",
    "    speech = None\n",
    "    mentionedEntityName = None\n",
    "    mentionedEntityType = None\n",
    "\n",
    "\n",
    "    for file in fileList:\n",
    "\n",
    "        print(\"Currently Processing:\", file)\n",
    "\n",
    "\n",
    "        ##################\n",
    "        ##################\n",
    "        ##################\n",
    "        #Part 1\n",
    "        ##################\n",
    "        ##################\n",
    "        ##################\n",
    "\n",
    "        file = str(file)\n",
    "\n",
    "\n",
    "        with open(file, 'r', encoding='utf-8', errors='ignore') as xml_file:\n",
    "            tree = ET.parse(xml_file)\n",
    "        xml_file.close()\n",
    "\n",
    "\n",
    "\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Declaring to capture speakerIDs associated with each party\n",
    "\n",
    "        capturedConId = []\n",
    "        capturedLibId = []\n",
    "        capturedNDPId = []\n",
    "        capturedGrnId = []\n",
    "        capturedFDId = []\n",
    "        capturedIndId = []\n",
    "        capturedBQId = []\n",
    "\n",
    "        for elementHansard in root:\n",
    "            #=================================================================\n",
    "            # elementHansard is a part of the second generation (level).\n",
    "            # ElementHansard has five children (tags): (1) StartPageNumber\n",
    "            #                                          (2) DocumentTitle (ignored)\n",
    "            #                                          (3) ExtractedInformation\n",
    "            #                                          (4) HansardBody\n",
    "            # Geneology: elementHansard is the child of root.\n",
    "            # ================================================================\n",
    "\n",
    "            if elementHansard.tag ==  \"StartPageNumber\":\n",
    "                # =================================================================\n",
    "                # StartPageNumber is a part of the third generation (level), has\n",
    "                # one new attribute, and no children of its own.\n",
    "                # genes:        (1) sourceStartPageNumber\n",
    "                #\n",
    "                # Geneology: StartPageNumber is the child of elementHansard, and\n",
    "                # the grandchild of root.\n",
    "                # ================================================================\n",
    "                sourceStartPageNumber = int(elementHansard.text) #The page number of the first page for\n",
    "                                                                 #this day in the Official Record of Debates.\n",
    "            if elementHansard.tag == \"ExtractedInformation\":\n",
    "\n",
    "                for elementExtractedInformation in elementHansard:\n",
    "                    #==========================================================\n",
    "                    # elementExtractedInformation is also part of the third\n",
    "                    # generation (level). It is a sibling of StartPageNumber\n",
    "                    # and therefore carries the attribute sourceStartPageNumber.\n",
    "                    # \"ExtractedInformation\" has no children and 28 new\n",
    "                    # attributes.\n",
    "                    #               (1)  {'Name': 'InstitutionDebate'}\n",
    "                    #               (2)  {'Name': 'Volume'}\n",
    "                    #               (3)  {'Name': 'Number'}\n",
    "                    #               (4)  {'Name': 'Session'}\n",
    "                    #               (5)  {'Name': 'Parliament'}\n",
    "                    #               (6)  {'Name': 'Date'}\n",
    "                    #               (7)  {'Name': 'SpeakerName'}\n",
    "                    #               (8)  {'Name': 'Institution'}\n",
    "                    #               (9)  {'Name': 'Country'}\n",
    "                    #               (10)  {'Name': 'TOCNote'} (ignored)\n",
    "                    #               (11) {'Name': 'HeaderTitle'}\n",
    "                    #               (12) {'Name': 'HeaderDate'}\n",
    "                    #               (13) {'Name': 'MetaDocumentCategory'} (ignored)\n",
    "                    #               (14) {'Name': 'MetaTitle'}\n",
    "                    #               (15) {'Name': 'MetaTitleEn'}\n",
    "                    #               (16) {'Name': 'MetaTitleFr'}\n",
    "                    #               (17) {'Name': 'MetaVolumeNumber'}\n",
    "                    #               (18) {'Name': 'MetaNumberNumber'}\n",
    "                    #               (19) {'Name': 'MetaDateNumDay'}\n",
    "                    #               (20) {'Name': 'MetaDateNumMonth'}\n",
    "                    #               (21) {'Name': 'MetaDateNumYear'}\n",
    "                    #               (22) {'Name': 'MetaCreationTime'}\n",
    "                    #               (23) {'Name': 'MetaInstitution'}\n",
    "                    #               (24) {'Name': 'InstitutionDebateFr'}\n",
    "                    #               (25) {'Name': 'InstitutionDebateEn'}\n",
    "                    #               (26) {'Name': 'ParliamentNumber'}\n",
    "                    #               (27) {'Name': 'SessionNumber'}\n",
    "                    #               (28) {'Name': 'InCameraNote'}\n",
    "                    # Geneology: ExtractedInformation is the child of elementHansard,\n",
    "                    #            the grandchild of root, and has no children of its\n",
    "                    #            own. StartPageNumber, DocumentTitle, and\n",
    "                    #            ExtractedInformation have properties shared by\n",
    "                    #            all descendants of HansardBody, but they have\n",
    "                    #            no children of their own.\n",
    "                    #=========================================================\n",
    "\n",
    "                    if elementExtractedInformation.attrib=={'Name': 'MetaInstitution'}: #e.g., House of Commons\n",
    "                        chamber = elementExtractedInformation.text\n",
    "\n",
    "                    if elementExtractedInformation.attrib=={'Name': 'ParliamentNumber'}: #Which Parliament Number\n",
    "                        parliamentNumber = int(elementExtractedInformation.text)\n",
    "\n",
    "                    if elementExtractedInformation.attrib=={'Name': 'SessionNumber'}: #Which Session Number\n",
    "                        parliamentSession = int(elementExtractedInformation.text)\n",
    "\n",
    "                    if elementExtractedInformation.attrib=={'Name': 'HeaderTitle'}: #E.g., \"Commons Debates\"\n",
    "                        sourceTitle = elementExtractedInformation.text\n",
    "\n",
    "                    if elementExtractedInformation.attrib=={'Name': 'MetaVolumeNumber'}: #Hansard Volume\n",
    "                        sourceVolume = elementExtractedInformation.text\n",
    "\n",
    "                    if elementExtractedInformation.attrib=={'Name': 'MetaNumberNumber'}: #Hansard Number\n",
    "                        sourceNumber = int(elementExtractedInformation.text)\n",
    "\n",
    "                    if elementExtractedInformation.attrib=={'Name': 'HeaderDate'}: #Date in Format (January 1, 2001)\n",
    "                        date = elementExtractedInformation.text\n",
    "\n",
    "                    if elementExtractedInformation.attrib=={'Name': 'MetaDateNumYear'}: #Year\n",
    "                        year = int(elementExtractedInformation.text)\n",
    "\n",
    "                    if elementExtractedInformation.attrib=={'Name': 'MetaDateNumMonth'}: #Month Number\n",
    "                        month = int(elementExtractedInformation.text)\n",
    "\n",
    "                    if elementExtractedInformation.attrib=={'Name': 'MetaDateNumDay'}: #Day Number\n",
    "                        day = int(elementExtractedInformation.text)\n",
    "\n",
    "                    if elementExtractedInformation.attrib=={'Name': 'Date'}:\n",
    "                        weekday = elementExtractedInformation.text.split(',',1)[0] #Day of Week, e.g., \"Monday\"\n",
    "\n",
    "            if elementHansard.tag == \"HansardBody\":\n",
    "                for elementHansardBody in elementHansard:\n",
    "                # ======================================================================\n",
    "                # elementHansard Body is also part of the third generation.  It is a\n",
    "                # sibling of StartPageNumber and elementExtractedInformation, and thus\n",
    "                # carries the attributes sourceStartPageNumber from startPageNumber, and the\n",
    "                # 28 attributes from elementExtractedInformation.  elementHansardBody has 10\n",
    "                # new genes (attributes) and two children (tags).\n",
    "                # attrib:       (1)  {} #Empty\n",
    "                #               (2)  {'id': '9796727', 'Rubric': 'Other'}\n",
    "                #               (3)  {'id': '9794282', 'Rubric': 'StatementsByMembers'}\n",
    "                #               (4)  {'id': '9794342', 'Rubric': 'NewMP'}\n",
    "                #               (5)  {'id': '9794349', 'Rubric': 'OralQuestionPeriod'}\n",
    "                #               (6)  {'id': '9794620', 'Rubric': 'RoutineProceedings'}\n",
    "                #               (7)  {'id': '9794803', 'Rubric': 'Other'}\n",
    "                #               (8)  {'id': '9796436', 'Rubric': 'Other'}\n",
    "                #               (9)  {'id': '9796555', 'Rubric': 'RoutineProceedings'}\n",
    "                #               (10)  {'id': '9796639', 'Rubric': 'LateShow'}\n",
    "                # children:     (1)   Intro\n",
    "                #               (2)   OrderOfBusiness\n",
    "                # Geneology: elementHansardBody is the child of elementHansard, and the\n",
    "                #            grandchild of root.  It has two children.  It shares all of\n",
    "                #            the genes of its siblings, elementExtractedInformation and\n",
    "                #            StartPageNumber, and it passes down all of these attribs to\n",
    "                #            its two children.  It also has 10 new attribs of its own, for a\n",
    "                #            total of 39 attribs that it passes down.\n",
    "                # ======================================================================\n",
    "                    if elementHansardBody.tag==\"Intro\":\n",
    "                        for elementIntro in elementHansardBody:\n",
    "                            #====================================================================\n",
    "                            # elementIntro is a part of the fourth generation.  It has two new\n",
    "                            # attribs and no children (tags).\n",
    "                            # attribs:       (1) Paratext\n",
    "                            #                (2) Prayer\n",
    "                            # Geneology: elementIntro is the child of elementHansardBody, the\n",
    "                            # grandchild of elementHansard, and the great-grandchild of root.\n",
    "                            # elementIntro has two new genes and two children.  Although\n",
    "                            # elementIntro's line dies off in this generation, its two genes are\n",
    "                            # recorded here because they are shared by its sibling,\n",
    "                            # OrderOfBusiness, and passed down through that lineage.\n",
    "                            #====================================================================\n",
    "\n",
    "                            if elementIntro.tag==\"ParaText\": #Attribute of elementIntro.  Procedural text in this case.\n",
    "                                topLine = elementIntro.text #top line for the day.\n",
    "\n",
    "                            if elementIntro.tag==\"Prayer\": #Attribute of elementIntro\n",
    "                                secondLine = elementIntro.text\n",
    "\n",
    "                    if elementHansardBody.tag==\"OrderOfBusiness\":\n",
    "                        for elementOrderOfBusiness in elementHansardBody:\n",
    "                            # ======================================================================\n",
    "                            # elementOrderOfBusiness is also a part of the fourth generation, and therefore\n",
    "                            # shares the two attributes of its sibling, elementIntro.  elementOrderOfBusiness\n",
    "                            # has two additional new attributes. It has one child.\n",
    "                            # genes:        (1)   OrderOfBusinessTitle\n",
    "                            #               (2)   Catchline\n",
    "                            # children:     (1)   SubjectOfBusiness\n",
    "                            #\n",
    "                            # Note: OrderOfBusinessTitle repeats Catchline.  It is therefore ignored.\n",
    "                            #\n",
    "                            # Geneology: elementOrderOfBusiness is the child of elementHansardBody,\n",
    "                            # the grandchild of elementHansard, and the great-grandchild of root. It is\n",
    "                            # the sibling of elementIntro.\n",
    "                            # ======================================================================\n",
    "                                if elementOrderOfBusiness.tag== \"SubjectOfBusiness\":\n",
    "                                    for elementSubjectOfBusiness in elementOrderOfBusiness:\n",
    "                                        subjectOfBusinessID = elementOrderOfBusiness.attrib.get(\"id\") #stores official ID\n",
    "                                        #========================================================================\n",
    "                                        # elementSubjectOfBusiness is the fifth generation, and the only child\n",
    "                                        # of elementOrderOfBusiness.  elementSubjectOfBusiness has 4 genes and\n",
    "                                        # 1 child.\n",
    "                                        # genes:        (1) Timestamp\n",
    "                                        #               (2) FloorLanguage\n",
    "                                        #               (3) SubjectOfBusinessTitle\n",
    "                                        #               (4) SubjectOfBusinessQualifier\n",
    "                                        #\n",
    "                                        # children:     (1) SubjectOfBusinessContent\n",
    "                                        #\n",
    "                                        # Geneology: elementSubjectOfBusiness is the only child of\n",
    "                                        # elementOrderofBusiness, the grandchild of elementHansardBody,\n",
    "                                        # the great-grandchild of elementHansard, and the great-great-\n",
    "                                        # grandchild of root.\n",
    "                                        #========================================================================\n",
    "                                        if elementSubjectOfBusiness.tag==\"Timestamp\":\n",
    "                                            timeStampHr = elementSubjectOfBusiness.attrib['Hr']\n",
    "                                            timeStampMin = elementSubjectOfBusiness.attrib['Mn']\n",
    "\n",
    "\n",
    "                                        orderOfBusinessRubric = elementHansardBody.attrib['Rubric']  # A secondary description,\n",
    "                                        #                                                              an attribute inherited from\n",
    "                                        #                                                              elementHansardBody.\n",
    "                                        #                                                              Needs to go with\n",
    "                                        #                                                              each speech.\n",
    "                                        if elementSubjectOfBusiness.tag==\"FloorLanguage\":\n",
    "                                            floorLanguage = elementSubjectOfBusiness.attrib['language']\n",
    "\n",
    "                                        if elementSubjectOfBusiness.tag=='SubjectOfBusinessTitle':     #A general title, procedurally oriented\n",
    "\n",
    "                                            subjectOfBusinessTitle = elementSubjectOfBusiness.text\n",
    "                                            subjectOfBusinessQualifier = \"\" #Not always available. resets so it doesn't get the previous qualifier\n",
    "\n",
    "                                            if subjectOfBusinessTitle == \"\":\n",
    "                                                subjectOfBusinessTitle = \"NA\"\n",
    "                                                subjectOfBusinessQualifier = \"\"  # Not always available. resets so it doesn't get the previous qualifier\n",
    "\n",
    "\n",
    "                                        if elementSubjectOfBusiness.tag==\"SubjectOfBusinessQualifier\":   #A more specific (and substantive title). Not always available.\n",
    "                                            subjectOfBusinessQualifier = elementSubjectOfBusiness.text\n",
    "\n",
    "\n",
    "                                        for elementSubjectOfBusinessContent in elementSubjectOfBusiness:\n",
    "                                            # ========================================================================\n",
    "                                            # elementSubjectOfBusinessContent is the sixth generation, and the only child\n",
    "                                            # of elementSubjectOfBusiness.  elementSubjectOfBusinessContent has 2 attribs and\n",
    "                                            # 1 child.\n",
    "                                            # attribs:      (1) Timestamp\n",
    "                                            #               (2) FloorLanguage\n",
    "                                            #\n",
    "                                            # children:     (1) Intervention\n",
    "                                            #\n",
    "                                            #\n",
    "                                            # Geneology: elementSubjectOfBusinessContent is the only child of\n",
    "                                            # elementSubjectofBusiness, the grandchild of elementOrderOfBusiness,\n",
    "                                            # the great-great child of elementHansardBody, the great-great-grandchild of\n",
    "                                            # elementHansard and the great-great-great grandchild of root.\n",
    "                                            if elementSubjectOfBusinessContent.tag == \"Timestamp\": #Time Stamped every five mintues, at different levels.\n",
    "                                                timeStampHr = elementSubjectOfBusinessContent.attrib['Hr']\n",
    "                                                timeStampMin = elementSubjectOfBusinessContent.attrib['Mn']\n",
    "\n",
    "                                            if elementSubjectOfBusinessContent.tag == \"FloorLanguage\":\n",
    "                                                floorLanguage = elementSubjectOfBusinessContent.attrib['language']\n",
    "\n",
    "                                            if elementSubjectOfBusinessContent.tag == \"ParaText\":  #This would be procedural text (e.g., It being 5:30 p.m., the House will now proceed to the taking of the....)\n",
    "                                                pass\n",
    "\n",
    "                                            if elementSubjectOfBusinessContent.tag == \"Intervention\": #This is an intervention\n",
    "\n",
    "                                                for elementSubjectOfBusinessContentIntervention in elementSubjectOfBusinessContent:\n",
    "                                                    # ========================================================================\n",
    "                                                    # elementSubjectOfBusinessContentIntervention is the seventh generation,\n",
    "                                                    # and the only child of elementSubjectOfBusinessContent.\n",
    "                                                    # elementSubjectOfBusinessContentIntervention has two children:\n",
    "                                                    #\n",
    "                                                    # children:     (1) Person Speaking\n",
    "                                                    #               (2) Content\n",
    "\n",
    "                                                    #print(elementSubjectOfBusinessContent.attrib)\n",
    "                                                    interventionId = elementSubjectOfBusinessContent.attrib.get(\"id\")\n",
    "\n",
    "                                                    if elementSubjectOfBusinessContentIntervention.tag==\"PersonSpeaking\":\n",
    "\n",
    "                                                        for elementSubjectOfBusinessContentInterventionPersonSpeaking in elementSubjectOfBusinessContentIntervention:\n",
    "                                                            # ========================================================================\n",
    "                                                            # elementSubjectOfBusinessContentInterventionPersonSpeaking is the eigth generation,\n",
    "                                                            # and child of elementSubjectOfBusinessContentIntervention.\n",
    "                                                            # elementSubjectOfBusinessContentInterventionPersonSpeaking has one attributes:\n",
    "                                                            #               (1) Affiliation\n",
    "                                                            #                   (A) Affiliation Type\n",
    "                                                            #                   (B) Affiliation DataBase ID\n",
    "                                                            #                   (C) Affiliation Name (i.e. Speaker's Name)\n",
    "                                                            if elementSubjectOfBusinessContentInterventionPersonSpeaking.tag==\"Affiliation\":\n",
    "\n",
    "                                                                try:\n",
    "                                                                    affiliationType = elementSubjectOfBusinessContentInterventionPersonSpeaking.attrib['Type']\n",
    "                                                                except:\n",
    "                                                                    affiliationType = \"NA\"\n",
    "\n",
    "                                                                affiliationDbId = elementSubjectOfBusinessContentInterventionPersonSpeaking.attrib['DbId']\n",
    "\n",
    "                                                                if elementSubjectOfBusinessContentInterventionPersonSpeaking.text is not None: #a couple of speaker names are missing\n",
    "                                                                    speakerName = elementSubjectOfBusinessContentInterventionPersonSpeaking.text\n",
    "                                                                else:\n",
    "                                                                    speakerName = \"NA\"\n",
    "\n",
    "                                                                capturedConId.append(214648) #Ambrose has multiple Aliases.  She seems to start with an old one.\n",
    "                                                                capturedConId.append(214568)\n",
    "\n",
    "                                                                capturedLibId.append(213924) #Lamoureux given different IDs for first and subsequent speeches\n",
    "\n",
    "                                                                if \"Justin Trudeau\" in speakerName :\n",
    "                                                                    party = 'Lib'\n",
    "                                                                    capturedLibId.append(affiliationDbId)\n",
    "\n",
    "                                                                if \"Rona Ambrose\" in speakerName:\n",
    "                                                                    party = 'Con'\n",
    "                                                                    capturedLibId.append(affiliationDbId)\n",
    "\n",
    "                                                                elif \"Thomas Mulcair\" in speakerName:\n",
    "                                                                    party == 'NDP'\n",
    "                                                                    capturedNDPId.append(affiliationDbId)\n",
    "\n",
    "                                                                elif \"Stephen Harper\" in speakerName:\n",
    "                                                                    party = 'Con'\n",
    "                                                                    capturedConId.append(affiliationDbId)\n",
    "\n",
    "                                                                elif \"Andrew Scheer\" in speakerName:\n",
    "                                                                    party = 'Con'\n",
    "                                                                    capturedConId.append(affiliationDbId)\n",
    "\n",
    "                                                                #Party\n",
    "                                                                elif speakerName[-3:] == 'PC)':\n",
    "                                                                    party = \"Con\"\n",
    "                                                                    capturedConId.append(affiliationDbId)\n",
    "\n",
    "                                                                elif speakerName[-3:] == 'b.)':\n",
    "                                                                    party = 'Lib'\n",
    "                                                                    capturedLibId.append(affiliationDbId)\n",
    "\n",
    "                                                                elif speakerName[-3:] == 'DP)':\n",
    "                                                                    party = 'NDP'\n",
    "                                                                    capturedNDPId.append(affiliationDbId)\n",
    "\n",
    "                                                                elif speakerName[-3:] == 'FD)':\n",
    "                                                                    party = 'FD'\n",
    "                                                                    capturedFDId.append(affiliationDbId)\n",
    "\n",
    "                                                                elif speakerName[-3:] == 'GP)':\n",
    "                                                                    party = 'Grn'\n",
    "                                                                    capturedGrnId.append(affiliationDbId)\n",
    "\n",
    "                                                                elif speakerName[-3:] == 'BQ)':\n",
    "                                                                    party = 'BQ'\n",
    "                                                                    capturedBQId.append(affiliationDbId)\n",
    "\n",
    "                                                                elif speakerName[-3:] == 'd.)':\n",
    "                                                                    party = 'Ind'\n",
    "                                                                    capturedIndId.append(affiliationDbId)\n",
    "\n",
    "                                                                elif affiliationDbId in capturedConId:\n",
    "                                                                    party = 'Con'\n",
    "\n",
    "                                                                elif affiliationDbId in capturedLibId:\n",
    "                                                                    party = 'Lib'\n",
    "\n",
    "                                                                elif affiliationDbId in capturedNDPId:\n",
    "                                                                    party = \"NDP\"\n",
    "\n",
    "                                                                elif affiliationDbId in capturedBQId:\n",
    "                                                                    party = 'BQ'\n",
    "\n",
    "                                                                elif affiliationDbId in capturedFDId:\n",
    "                                                                    party = 'FD'\n",
    "\n",
    "                                                                elif affiliationDbId in capturedGrnId:\n",
    "                                                                    party = \"Grn\"\n",
    "\n",
    "                                                                else:\n",
    "                                                                    party = None\n",
    "                                     \n",
    "\n",
    "                                                    if elementSubjectOfBusinessContentIntervention.tag==\"Content\":\n",
    "                                                        speech = [] #an empty list, which will collect speech fragements below.\n",
    "                                                        paraNumRange = [] #empty list, for paragraph number range\n",
    "\n",
    "                                                        documentType = []  # empty list, for any documents (Type) mentioned by speaker. Type is unlabelled integer.  (To Look up)\n",
    "                                                        documentID = []  # empty list, for any documents mentioned by speaker.  This is a database ID. (To Look up)\n",
    "                                                        documentTitleList = []  # mpty list, for title of any document mentioned by Speaker.  This is readable text.\n",
    "\n",
    "                                                        mentionedEntityType = []  # empty list, for any Members mentioned in the speech.  This is unlabelled integer.  (To Look up)\n",
    "                                                        mentionedEntityDbId = []  # empty list, for any Members mentioned in the speech.  This is a database ID. (To look up)\n",
    "                                                        mentionedEntityNameList = []  # empty list, for names of any members mentioned in the speech.\n",
    "\n",
    "                                                        for elementSubjectOfBusinessContentInterventionContent in elementSubjectOfBusinessContentIntervention:\n",
    "                                                            # ========================================================================\n",
    "                                                            # elementSubjectOfBusinessContentInterventionContent is the eigth generation,\n",
    "                                                            # and child of elementSubjectOfBusinessContentIntervention.\n",
    "                                                            # elementSubjectOfBusinessContentIntervention has three attributes:\n",
    "                                                            #               (1) FloorLanguage\n",
    "                                                            #               (2) TimeStemp\n",
    "                                                            #               (3) ParaText\n",
    "                                                            #\n",
    "                                                            if elementSubjectOfBusinessContentInterventionContent.tag == \"FloorLanguage\":\n",
    "                                                                floorLanguage = elementSubjectOfBusinessContentInterventionContent.attrib['language']\n",
    "\n",
    "                                                            if elementSubjectOfBusinessContentInterventionContent.tag == \"Timestamp\":\n",
    "                                                                timeStampHr = elementSubjectOfBusinessContentInterventionContent.attrib['Hr']\n",
    "                                                                timeStampMin = elementSubjectOfBusinessContentInterventionContent.attrib['Mn']\n",
    "\n",
    "                                                            if elementSubjectOfBusinessContentInterventionContent.tag == \"ParaText\":\n",
    "                                                                try:\n",
    "                                                                    paraNumRange.append(int(elementSubjectOfBusinessContentInterventionContent.attrib[\"id\"]))\n",
    "                                                                except:\n",
    "                                                                    pass\n",
    "                                                                phrase = ET.tostring(elementSubjectOfBusinessContentInterventionContent, method=\"text\")\n",
    "                                                                phrase = phrase.decode(\"UTF-8\")\n",
    "                                                                phrase = phrase.replace('\\n', '').replace('\\t', '')\n",
    "\n",
    "                                                                speech.append(phrase)\n",
    "                                                                speech.append(\"\\n\\n\")\n",
    "\n",
    "\n",
    "                                                                for elementSubjectOfBusinessContentInterventionContentParaText in elementSubjectOfBusinessContentInterventionContent:\n",
    "                                                                    #print(elementSubjectOfBusinessContentInterventionContentParaText.tag)\n",
    "                                                                    # ========================================================================\n",
    "                                                                    # elementSubjectOfBusinessContentInterventionContentParaText is the ninth generation,\n",
    "                                                                    # and child of elementSubjectOfBusinessContentIntervention.\n",
    "                                                                    # elementSubjectOfBusinessContentIntervention has three attributes:\n",
    "                                                                    #               (1) Document (Any Bills Mentioned)\n",
    "                                                                    #               (2) Affiliation (Any Members Mentioned)\n",
    "                                                                    #               (B) ParaText\n",
    "                                                                    #\n",
    "                                                                    # print(elementSubjectOfBusinessContentInterventionContent.tag)\n",
    "                                                                    if elementSubjectOfBusinessContentInterventionContentParaText.tag==\"Document\":\n",
    "                                                                        try:\n",
    "                                                                            documentType.append(elementSubjectOfBusinessContentInterventionContentParaText.attrib['Type'])\n",
    "                                                                        except:\n",
    "                                                                            documentType.append(\"NA\")\n",
    "                                                                        try:\n",
    "                                                                            documentID.append(elementSubjectOfBusinessContentInterventionContentParaText.attrib['DbId'])\n",
    "                                                                        except:\n",
    "                                                                            documentID.append(\"NA\")\n",
    "                                                                        try:\n",
    "                                                                            documentTitle = elementSubjectOfBusinessContentInterventionContentParaText.text\n",
    "                                                                        except:\n",
    "                                                                            documentTitle = \"NA\"\n",
    "                                                                        documentTitleList.append(documentTitle)\n",
    "\n",
    "                                                                    if elementSubjectOfBusinessContentInterventionContentParaText.tag==\"Affiliation\":\n",
    "                                                                        try:\n",
    "                                                                            mentionedEntityType.append(elementSubjectOfBusinessContentInterventionContentParaText.attrib['Type'])\n",
    "                                                                        except:\n",
    "                                                                            mentionedEntityType.append(\"NA\")\n",
    "                                                                        try:\n",
    "                                                                            mentionedEntityDbId.append(elementSubjectOfBusinessContentInterventionContentParaText.attrib['DbId'])\n",
    "                                                                        except:\n",
    "                                                                            mentionedEntityDbId.append(\"NA\")\n",
    "                                                                        mentionedEntityName = elementSubjectOfBusinessContentInterventionContentParaText.text\n",
    "                                                                        mentionedEntityNameList.append(mentionedEntityName)\n",
    "\n",
    "\n",
    "\n",
    "                                                        #################################################################\n",
    "                                                        #Cleanup\n",
    "                                                        #################################################################\n",
    "\n",
    "                                                        speech = ''.join(speech)\n",
    "                                                        for openquote in ['&#8220;']:\n",
    "                                                            if openquote in speech:\n",
    "                                                                speech=speech.replace(openquote,\"\\\"\")\n",
    "                                                        for endquote in ['&#8221;']:\n",
    "                                                            if endquote in speech:\n",
    "                                                                speech=speech.replace(endquote,\"\\\"\")\n",
    "\n",
    "                                                        speech = re.sub(r' Minister(?! )', 'Minister ', speech)\n",
    "                                                        speech = re.sub(r'PrimeMinister', 'Prime Minister', speech)\n",
    "                                                        speech = re.sub(r'Minister of Finance(?! )', 'Minister of Finance ', speech)\n",
    "                                                        speech = re.sub(r'finance minister(?! )', 'finance minister ',speech)\n",
    "                                                        speech = re.sub(r'Minister of Environment and Climate Change(?! )', 'Minister of Environment and Climate Change ',speech)\n",
    "\n",
    "                                                        speechId = str(year) + \"-\" + str(month) +\"-\" + str(day) +\"-\" + str(interventionId)\n",
    "\n",
    "                                                        \n",
    "                                                        \n",
    "\n",
    "                                                        speech_filtered = \"\"\n",
    "\n",
    "                                                        if speech != None:\n",
    "                                                            \n",
    "                                                            speech_nlp = nlp(speech) # convering speech into Spacy doc\n",
    "                                                            \n",
    "                                                            for token in speech_nlp:\n",
    "                                                                pair = \"_\".join([token.text, token.tag_])\n",
    "                                                                speech_filtered = \" \".join([speech_filtered, pair])\n",
    "\n",
    "\n",
    "                                                            '''\n",
    "                                                            df = df.append({'parliamentNumber': int(parliamentNumber),\n",
    "                                                                            'parliamentSession': int(parliamentSession),\n",
    "                                                                            'orderOfBusinessRubric': orderOfBusinessRubric,\n",
    "                                                                            'subjectOfBusinessTitle': subjectOfBusinessTitle,\n",
    "                                                                            'subjectOfBusinessID': subjectOfBusinessID,\n",
    "                                                                            'subjectOfBusinessQualifier': subjectOfBusinessQualifier,\n",
    "                                                                            'speechId': speechId,\n",
    "                                                                            'party': party,\n",
    "                                                                            'interventionId': interventionId,\n",
    "                                                                            'date': date,\n",
    "                                                                            'year': int(year),\n",
    "                                                                            'month': int(month),\n",
    "                                                                            'day': int(day),\n",
    "                                                                            'weekday': weekday,\n",
    "                                                                            'timeStamp': timeStampHr+\":\"+timeStampMin,\n",
    "                                                                            'speakerName': speakerName,\n",
    "                                                                            'affiliationType': affiliationType,\n",
    "                                                                            'affiliationDbId': affiliationDbId,\n",
    "                                                                            'floorLanguage': floorLanguage,\n",
    "                                                                            'speech': speech,\n",
    "                                                                             'mentionedDocumentsTitle': documentTitleList,\n",
    "                                                                            'mentionedDocumentsId': documentID,\n",
    "                                                                            'mentionedDocumentsType': documentType,\n",
    "                                                                            'mentionedEntityName': mentionedEntityNameList,\n",
    "                                                                            'mentionedEntityId': mentionedEntityDbId,\n",
    "                                                                            'mentionedEntityType': mentionedEntityType,\n",
    "                                                                            }, ignore_index=True)\n",
    "                                                            '''\n",
    "\n",
    "\n",
    "\n",
    "                                                            dateYMD = datetime.strptime(str(year)+\"-\"+str(month)+\"-\"+str(day), '%Y-%m-%d')\n",
    "                                                            \n",
    "                                                            #Calling from dictionary\n",
    "                                                            \n",
    "                                                            id = int(affiliationDbId)\n",
    "                                                            \n",
    "                                                            try:\n",
    "                                                                parlInfoId = authorityFile[id]['parlInfoId']\n",
    "                                                                fullName = authorityFile[id]['fullName']\n",
    "                                                                firstName = authorityFile[id]['firstName']\n",
    "                                                                lastName = authorityFile[id]['lastName']\n",
    "                                                                middleName = authorityFile[id]['middleName']\n",
    "                                                                sex = authorityFile[id]['sex']\n",
    "                                                                visibleMinority = authorityFile[id]['visibleMinority']\n",
    "                                                                indigenous = authorityFile[id]['indigenous']\n",
    "                                                                dateOfBirth = authorityFile[id]['dateOfBirth']\n",
    "                                                                isEstimateDOB = authorityFile[id]['isEstimateDOB']\n",
    "                                                                birthProvince = authorityFile[id]['birthProvince']\n",
    "                                                                birthCountry = authorityFile[id]['birthCountry']\n",
    "                                                                firstDay = authorityFile[id]['firstDay']\n",
    "                                                                provOfRiding = authorityFile[id]['provOfRiding']\n",
    "                                                                parlInfoPage = authorityFile[id]['parlInfoPage']\n",
    "                                                                daysInOffice = (dateYMD-firstDay).days\n",
    "                                                                \n",
    "                                                            except: #if not dictionary entry for this dbID\n",
    "                                                                parlInfoId = \"NA\"\n",
    "                                                                parlInfoId = \"NA\"\n",
    "                                                                fullName = \"NA\"\n",
    "                                                                firstName = \"NA\"\n",
    "                                                                lastName = \"NA\"\n",
    "                                                                middleName = \"NA\"\n",
    "                                                                sex = \"NA\"\n",
    "                                                                visibleMinority = \"NA\"\n",
    "                                                                indigenous = \"NA\"\n",
    "                                                                dateOfBirth = \"NA\"\n",
    "                                                                isEstimateDOB = \"NA\"\n",
    "                                                                birthProvince = \"NA\"\n",
    "                                                                birthCountry = \"NA\"\n",
    "                                                                firstDay = \"NA\"\n",
    "                                                                provOfRiding = \"NA\"\n",
    "                                                                parlInfoPage = \"NA\"\n",
    "                                                                daysInOffice = \"NA\"\n",
    "                                                            \n",
    "                                                                                                               \n",
    "                                                            if dateOfBirth != \"NA\":\n",
    "                                                                if (dateOfBirth.year == 9999):\n",
    "                                                                    age = \"NA\"\n",
    "                                                                else:\n",
    "                                                                    age = (dateYMD-dateOfBirth).days/365.25\n",
    "                                                            else:\n",
    "                                                                age = \"NA\"\n",
    "                                                            \n",
    "                                                            dfList.append([int(parliamentNumber),\n",
    "                                                                      int(parliamentSession),\n",
    "                                                                       orderOfBusinessRubric,\n",
    "                                                                       subjectOfBusinessTitle,\n",
    "                                                                       subjectOfBusinessID,\n",
    "                                                                       subjectOfBusinessQualifier,\n",
    "                                                                       speechId,\n",
    "                                                                       interventionId,\n",
    "                                                                       date,\n",
    "                                                                       dateYMD,\n",
    "                                                                       int(year),\n",
    "                                                                       int(month),\n",
    "                                                                       int(day),\n",
    "                                                                       weekday,\n",
    "                                                                       timeStampHr+\":\"+timeStampMin,\n",
    "                                                                       speakerName,\n",
    "                                                                       party,\n",
    "                                                                       parlInfoId,\n",
    "                                                                       fullName,\n",
    "                                                                       firstName,\n",
    "                                                                       lastName,\n",
    "                                                                       middleName,\n",
    "                                                                       sex,\n",
    "                                                                       age,\n",
    "                                                                       daysInOffice,\n",
    "                                                                       visibleMinority,\n",
    "                                                                       indigenous,\n",
    "                                                                       dateOfBirth,\n",
    "                                                                       isEstimateDOB,\n",
    "                                                                       birthProvince,\n",
    "                                                                       birthCountry,\n",
    "                                                                       firstDay,\n",
    "                                                                       provOfRiding,\n",
    "                                                                       parlInfoPage,\n",
    "                                                                       affiliationType,\n",
    "                                                                       affiliationDbId,\n",
    "                                                                       floorLanguage,\n",
    "                                                                       speech,\n",
    "                                                                       speech_filtered,\n",
    "                                                                       documentTitleList,\n",
    "                                                                       documentID,\n",
    "                                                                       documentType,\n",
    "                                                                       mentionedEntityNameList,\n",
    "                                                                       mentionedEntityDbId,\n",
    "                                                                       mentionedEntityType,\n",
    "                                                                       file])\n",
    "\n",
    "\n",
    "\n",
    "    labels = ['parliamentNumber', \n",
    "              'parliamentSession', \n",
    "              'orderOfBusinessRubric',\n",
    "              'subjectOfBusinessTitle',\n",
    "              'subjectOfBusinessID', \n",
    "              'subjectOfBusinessQualifier', \n",
    "              'speechId', \n",
    "              'interventionId',\n",
    "              'date', \n",
    "              'dateYMD', \n",
    "              'year', \n",
    "              'month', \n",
    "              'day', \n",
    "              'weekday', \n",
    "              'timeStamp',\n",
    "              'speakerName', \n",
    "              'party', \n",
    "              'parlInfoId', \n",
    "              'fullName', \n",
    "              'firstName', \n",
    "              'lastName', \n",
    "              'middleName',\n",
    "              'sex', \n",
    "              'age', \n",
    "              'daysInOffice', \n",
    "              'visibleMinority', \n",
    "              'indigenous', \n",
    "              'dateOfBirth', \n",
    "              'isEstimateDOB', \n",
    "              'birthProvince', \n",
    "              'birthCountry', \n",
    "              'firstDay', \n",
    "              'provOfRiding', \n",
    "              'parlInfoPage',\n",
    "              'affiliationType', \n",
    "              'affiliationDbId', \n",
    "              'floorLanguage',\n",
    "              'speech', \n",
    "              'speechFiltered',\n",
    "              'mentionedDocumentsTitle', \n",
    "              'mentionedDocumentsId', \n",
    "              'mentionedDocumentsType',\n",
    "              'mentionedEntityName', \n",
    "              'mentionedEntityId',\n",
    "              'mentionedEntityType', \n",
    "              'filename'\n",
    "              ]\n",
    "\n",
    "\n",
    "    df = pd.DataFrame.from_records(dfList, columns=labels)\n",
    "\n",
    "    df.to_csv(\"hansardExtractedSpeechesFull.csv\", sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "hansardXmlParserSpeeches()\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IX. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IX. A. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "no!\n",
      "no!\n",
      "no!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-11 14:47:06,158 : INFO : collecting all words and their counts\n",
      "2020-09-11 14:47:06,158 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-09-11 14:47:06,178 : INFO : PROGRESS: at sentence #10000, processed 122535 words, keeping 10496 word types\n",
      "2020-09-11 14:47:06,199 : INFO : PROGRESS: at sentence #20000, processed 250225 words, keeping 14636 word types\n",
      "2020-09-11 14:47:06,219 : INFO : PROGRESS: at sentence #30000, processed 372363 words, keeping 17917 word types\n",
      "2020-09-11 14:47:06,245 : INFO : PROGRESS: at sentence #40000, processed 498552 words, keeping 20694 word types\n",
      "2020-09-11 14:47:06,269 : INFO : PROGRESS: at sentence #50000, processed 618329 words, keeping 22787 word types\n",
      "2020-09-11 14:47:06,293 : INFO : PROGRESS: at sentence #60000, processed 748477 words, keeping 24625 word types\n",
      "2020-09-11 14:47:06,315 : INFO : PROGRESS: at sentence #70000, processed 878598 words, keeping 26542 word types\n",
      "2020-09-11 14:47:06,335 : INFO : PROGRESS: at sentence #80000, processed 999488 words, keeping 28025 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4006269 sentences in our corpus of questions.\n",
      "currently processing: training model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-11 14:47:06,360 : INFO : PROGRESS: at sentence #90000, processed 1127079 words, keeping 29797 word types\n",
      "2020-09-11 14:47:06,383 : INFO : PROGRESS: at sentence #100000, processed 1251812 words, keeping 31259 word types\n",
      "2020-09-11 14:47:06,406 : INFO : PROGRESS: at sentence #110000, processed 1380137 words, keeping 32674 word types\n",
      "2020-09-11 14:47:06,427 : INFO : PROGRESS: at sentence #120000, processed 1504194 words, keeping 33971 word types\n",
      "2020-09-11 14:47:06,448 : INFO : PROGRESS: at sentence #130000, processed 1627319 words, keeping 35224 word types\n",
      "2020-09-11 14:47:06,472 : INFO : PROGRESS: at sentence #140000, processed 1750120 words, keeping 36459 word types\n",
      "2020-09-11 14:47:06,495 : INFO : PROGRESS: at sentence #150000, processed 1880099 words, keeping 37892 word types\n",
      "2020-09-11 14:47:06,517 : INFO : PROGRESS: at sentence #160000, processed 2004067 words, keeping 38902 word types\n",
      "2020-09-11 14:47:06,538 : INFO : PROGRESS: at sentence #170000, processed 2124317 words, keeping 39909 word types\n",
      "2020-09-11 14:47:06,560 : INFO : PROGRESS: at sentence #180000, processed 2248343 words, keeping 40897 word types\n",
      "2020-09-11 14:47:06,581 : INFO : PROGRESS: at sentence #190000, processed 2369506 words, keeping 41711 word types\n",
      "2020-09-11 14:47:06,603 : INFO : PROGRESS: at sentence #200000, processed 2493124 words, keeping 42564 word types\n",
      "2020-09-11 14:47:06,626 : INFO : PROGRESS: at sentence #210000, processed 2620543 words, keeping 43624 word types\n",
      "2020-09-11 14:47:06,649 : INFO : PROGRESS: at sentence #220000, processed 2745161 words, keeping 44566 word types\n",
      "2020-09-11 14:47:06,673 : INFO : PROGRESS: at sentence #230000, processed 2872770 words, keeping 45410 word types\n",
      "2020-09-11 14:47:06,695 : INFO : PROGRESS: at sentence #240000, processed 3000089 words, keeping 46269 word types\n",
      "2020-09-11 14:47:06,718 : INFO : PROGRESS: at sentence #250000, processed 3125689 words, keeping 47062 word types\n",
      "2020-09-11 14:47:06,739 : INFO : PROGRESS: at sentence #260000, processed 3253994 words, keeping 47855 word types\n",
      "2020-09-11 14:47:06,762 : INFO : PROGRESS: at sentence #270000, processed 3377473 words, keeping 48661 word types\n",
      "2020-09-11 14:47:06,784 : INFO : PROGRESS: at sentence #280000, processed 3495047 words, keeping 49503 word types\n",
      "2020-09-11 14:47:06,805 : INFO : PROGRESS: at sentence #290000, processed 3620358 words, keeping 50409 word types\n",
      "2020-09-11 14:47:06,828 : INFO : PROGRESS: at sentence #300000, processed 3746177 words, keeping 51303 word types\n",
      "2020-09-11 14:47:06,850 : INFO : PROGRESS: at sentence #310000, processed 3870243 words, keeping 51955 word types\n",
      "2020-09-11 14:47:06,872 : INFO : PROGRESS: at sentence #320000, processed 3993757 words, keeping 52792 word types\n",
      "2020-09-11 14:47:06,894 : INFO : PROGRESS: at sentence #330000, processed 4119077 words, keeping 53497 word types\n",
      "2020-09-11 14:47:06,917 : INFO : PROGRESS: at sentence #340000, processed 4246959 words, keeping 54307 word types\n",
      "2020-09-11 14:47:06,939 : INFO : PROGRESS: at sentence #350000, processed 4371234 words, keeping 55186 word types\n",
      "2020-09-11 14:47:06,961 : INFO : PROGRESS: at sentence #360000, processed 4493694 words, keeping 55847 word types\n",
      "2020-09-11 14:47:06,983 : INFO : PROGRESS: at sentence #370000, processed 4616259 words, keeping 56547 word types\n",
      "2020-09-11 14:47:07,004 : INFO : PROGRESS: at sentence #380000, processed 4737133 words, keeping 57251 word types\n",
      "2020-09-11 14:47:07,029 : INFO : PROGRESS: at sentence #390000, processed 4863058 words, keeping 57956 word types\n",
      "2020-09-11 14:47:07,050 : INFO : PROGRESS: at sentence #400000, processed 4990103 words, keeping 58585 word types\n",
      "2020-09-11 14:47:07,074 : INFO : PROGRESS: at sentence #410000, processed 5115881 words, keeping 59146 word types\n",
      "2020-09-11 14:47:07,097 : INFO : PROGRESS: at sentence #420000, processed 5243478 words, keeping 59820 word types\n",
      "2020-09-11 14:47:07,119 : INFO : PROGRESS: at sentence #430000, processed 5367706 words, keeping 60609 word types\n",
      "2020-09-11 14:47:07,142 : INFO : PROGRESS: at sentence #440000, processed 5492251 words, keeping 61318 word types\n",
      "2020-09-11 14:47:07,165 : INFO : PROGRESS: at sentence #450000, processed 5617307 words, keeping 61834 word types\n",
      "2020-09-11 14:47:07,185 : INFO : PROGRESS: at sentence #460000, processed 5741317 words, keeping 62423 word types\n",
      "2020-09-11 14:47:07,207 : INFO : PROGRESS: at sentence #470000, processed 5863805 words, keeping 63034 word types\n",
      "2020-09-11 14:47:07,230 : INFO : PROGRESS: at sentence #480000, processed 5987676 words, keeping 63860 word types\n",
      "2020-09-11 14:47:07,252 : INFO : PROGRESS: at sentence #490000, processed 6111213 words, keeping 64373 word types\n",
      "2020-09-11 14:47:07,274 : INFO : PROGRESS: at sentence #500000, processed 6236299 words, keeping 64903 word types\n",
      "2020-09-11 14:47:07,295 : INFO : PROGRESS: at sentence #510000, processed 6360005 words, keeping 65440 word types\n",
      "2020-09-11 14:47:07,318 : INFO : PROGRESS: at sentence #520000, processed 6485360 words, keeping 65930 word types\n",
      "2020-09-11 14:47:07,338 : INFO : PROGRESS: at sentence #530000, processed 6609447 words, keeping 66467 word types\n",
      "2020-09-11 14:47:07,360 : INFO : PROGRESS: at sentence #540000, processed 6729105 words, keeping 67133 word types\n",
      "2020-09-11 14:47:07,382 : INFO : PROGRESS: at sentence #550000, processed 6856005 words, keeping 67707 word types\n",
      "2020-09-11 14:47:07,402 : INFO : PROGRESS: at sentence #560000, processed 6978577 words, keeping 68231 word types\n",
      "2020-09-11 14:47:07,425 : INFO : PROGRESS: at sentence #570000, processed 7098030 words, keeping 68705 word types\n",
      "2020-09-11 14:47:07,447 : INFO : PROGRESS: at sentence #580000, processed 7223602 words, keeping 69251 word types\n",
      "2020-09-11 14:47:07,469 : INFO : PROGRESS: at sentence #590000, processed 7347636 words, keeping 69883 word types\n",
      "2020-09-11 14:47:07,491 : INFO : PROGRESS: at sentence #600000, processed 7476026 words, keeping 70428 word types\n",
      "2020-09-11 14:47:07,515 : INFO : PROGRESS: at sentence #610000, processed 7600483 words, keeping 70956 word types\n",
      "2020-09-11 14:47:07,537 : INFO : PROGRESS: at sentence #620000, processed 7720878 words, keeping 71479 word types\n",
      "2020-09-11 14:47:07,559 : INFO : PROGRESS: at sentence #630000, processed 7847440 words, keeping 72001 word types\n",
      "2020-09-11 14:47:07,582 : INFO : PROGRESS: at sentence #640000, processed 7972112 words, keeping 72555 word types\n",
      "2020-09-11 14:47:07,603 : INFO : PROGRESS: at sentence #650000, processed 8096053 words, keeping 73098 word types\n",
      "2020-09-11 14:47:07,628 : INFO : PROGRESS: at sentence #660000, processed 8220804 words, keeping 73614 word types\n",
      "2020-09-11 14:47:07,650 : INFO : PROGRESS: at sentence #670000, processed 8343766 words, keeping 74049 word types\n",
      "2020-09-11 14:47:07,671 : INFO : PROGRESS: at sentence #680000, processed 8462102 words, keeping 74568 word types\n",
      "2020-09-11 14:47:07,691 : INFO : PROGRESS: at sentence #690000, processed 8579464 words, keeping 75111 word types\n",
      "2020-09-11 14:47:07,714 : INFO : PROGRESS: at sentence #700000, processed 8703147 words, keeping 75584 word types\n",
      "2020-09-11 14:47:07,736 : INFO : PROGRESS: at sentence #710000, processed 8828357 words, keeping 76120 word types\n",
      "2020-09-11 14:47:07,757 : INFO : PROGRESS: at sentence #720000, processed 8951889 words, keeping 76638 word types\n",
      "2020-09-11 14:47:07,779 : INFO : PROGRESS: at sentence #730000, processed 9077227 words, keeping 77134 word types\n",
      "2020-09-11 14:47:07,800 : INFO : PROGRESS: at sentence #740000, processed 9199274 words, keeping 77572 word types\n",
      "2020-09-11 14:47:07,823 : INFO : PROGRESS: at sentence #750000, processed 9324672 words, keeping 77992 word types\n",
      "2020-09-11 14:47:07,844 : INFO : PROGRESS: at sentence #760000, processed 9450001 words, keeping 78482 word types\n",
      "2020-09-11 14:47:07,864 : INFO : PROGRESS: at sentence #770000, processed 9573216 words, keeping 79030 word types\n",
      "2020-09-11 14:47:07,887 : INFO : PROGRESS: at sentence #780000, processed 9696685 words, keeping 79473 word types\n",
      "2020-09-11 14:47:07,908 : INFO : PROGRESS: at sentence #790000, processed 9820234 words, keeping 79951 word types\n",
      "2020-09-11 14:47:07,933 : INFO : PROGRESS: at sentence #800000, processed 9948881 words, keeping 80514 word types\n",
      "2020-09-11 14:47:07,955 : INFO : PROGRESS: at sentence #810000, processed 10073796 words, keeping 81135 word types\n",
      "2020-09-11 14:47:07,977 : INFO : PROGRESS: at sentence #820000, processed 10203354 words, keeping 81616 word types\n",
      "2020-09-11 14:47:07,998 : INFO : PROGRESS: at sentence #830000, processed 10324562 words, keeping 82021 word types\n",
      "2020-09-11 14:47:08,021 : INFO : PROGRESS: at sentence #840000, processed 10453858 words, keeping 82498 word types\n",
      "2020-09-11 14:47:08,042 : INFO : PROGRESS: at sentence #850000, processed 10578614 words, keeping 82928 word types\n",
      "2020-09-11 14:47:08,064 : INFO : PROGRESS: at sentence #860000, processed 10703742 words, keeping 83456 word types\n",
      "2020-09-11 14:47:08,088 : INFO : PROGRESS: at sentence #870000, processed 10826414 words, keeping 83980 word types\n",
      "2020-09-11 14:47:08,109 : INFO : PROGRESS: at sentence #880000, processed 10951589 words, keeping 84406 word types\n",
      "2020-09-11 14:47:08,131 : INFO : PROGRESS: at sentence #890000, processed 11073231 words, keeping 84797 word types\n",
      "2020-09-11 14:47:08,153 : INFO : PROGRESS: at sentence #900000, processed 11196281 words, keeping 85236 word types\n",
      "2020-09-11 14:47:08,179 : INFO : PROGRESS: at sentence #910000, processed 11324812 words, keeping 85724 word types\n",
      "2020-09-11 14:47:08,200 : INFO : PROGRESS: at sentence #920000, processed 11449683 words, keeping 86154 word types\n",
      "2020-09-11 14:47:08,221 : INFO : PROGRESS: at sentence #930000, processed 11573186 words, keeping 86643 word types\n",
      "2020-09-11 14:47:08,243 : INFO : PROGRESS: at sentence #940000, processed 11697352 words, keeping 87064 word types\n",
      "2020-09-11 14:47:08,267 : INFO : PROGRESS: at sentence #950000, processed 11823200 words, keeping 87488 word types\n",
      "2020-09-11 14:47:08,292 : INFO : PROGRESS: at sentence #960000, processed 11947496 words, keeping 87972 word types\n",
      "2020-09-11 14:47:08,312 : INFO : PROGRESS: at sentence #970000, processed 12070155 words, keeping 88393 word types\n",
      "2020-09-11 14:47:08,334 : INFO : PROGRESS: at sentence #980000, processed 12193525 words, keeping 88851 word types\n",
      "2020-09-11 14:47:08,356 : INFO : PROGRESS: at sentence #990000, processed 12316235 words, keeping 89237 word types\n",
      "2020-09-11 14:47:08,378 : INFO : PROGRESS: at sentence #1000000, processed 12444245 words, keeping 89751 word types\n",
      "2020-09-11 14:47:08,402 : INFO : PROGRESS: at sentence #1010000, processed 12571409 words, keeping 90129 word types\n",
      "2020-09-11 14:47:08,424 : INFO : PROGRESS: at sentence #1020000, processed 12699370 words, keeping 90631 word types\n",
      "2020-09-11 14:47:08,447 : INFO : PROGRESS: at sentence #1030000, processed 12824368 words, keeping 90983 word types\n",
      "2020-09-11 14:47:08,470 : INFO : PROGRESS: at sentence #1040000, processed 12956018 words, keeping 91483 word types\n",
      "2020-09-11 14:47:08,494 : INFO : PROGRESS: at sentence #1050000, processed 13077010 words, keeping 91869 word types\n",
      "2020-09-11 14:47:08,516 : INFO : PROGRESS: at sentence #1060000, processed 13202714 words, keeping 92203 word types\n",
      "2020-09-11 14:47:08,537 : INFO : PROGRESS: at sentence #1070000, processed 13327225 words, keeping 92658 word types\n",
      "2020-09-11 14:47:08,560 : INFO : PROGRESS: at sentence #1080000, processed 13454514 words, keeping 93070 word types\n",
      "2020-09-11 14:47:08,582 : INFO : PROGRESS: at sentence #1090000, processed 13582856 words, keeping 93587 word types\n",
      "2020-09-11 14:47:08,605 : INFO : PROGRESS: at sentence #1100000, processed 13711710 words, keeping 94039 word types\n",
      "2020-09-11 14:47:08,627 : INFO : PROGRESS: at sentence #1110000, processed 13833577 words, keeping 94438 word types\n",
      "2020-09-11 14:47:08,650 : INFO : PROGRESS: at sentence #1120000, processed 13958744 words, keeping 94811 word types\n",
      "2020-09-11 14:47:08,671 : INFO : PROGRESS: at sentence #1130000, processed 14083394 words, keeping 95274 word types\n",
      "2020-09-11 14:47:08,692 : INFO : PROGRESS: at sentence #1140000, processed 14204409 words, keeping 95690 word types\n",
      "2020-09-11 14:47:08,714 : INFO : PROGRESS: at sentence #1150000, processed 14324973 words, keeping 96150 word types\n",
      "2020-09-11 14:47:08,736 : INFO : PROGRESS: at sentence #1160000, processed 14450590 words, keeping 96606 word types\n",
      "2020-09-11 14:47:08,759 : INFO : PROGRESS: at sentence #1170000, processed 14576319 words, keeping 97004 word types\n",
      "2020-09-11 14:47:08,782 : INFO : PROGRESS: at sentence #1180000, processed 14705040 words, keeping 97424 word types\n",
      "2020-09-11 14:47:08,805 : INFO : PROGRESS: at sentence #1190000, processed 14826437 words, keeping 97892 word types\n",
      "2020-09-11 14:47:08,828 : INFO : PROGRESS: at sentence #1200000, processed 14954851 words, keeping 98217 word types\n",
      "2020-09-11 14:47:08,848 : INFO : PROGRESS: at sentence #1210000, processed 15074983 words, keeping 98591 word types\n",
      "2020-09-11 14:47:08,871 : INFO : PROGRESS: at sentence #1220000, processed 15202194 words, keeping 98867 word types\n",
      "2020-09-11 14:47:08,893 : INFO : PROGRESS: at sentence #1230000, processed 15321097 words, keeping 99194 word types\n",
      "2020-09-11 14:47:08,915 : INFO : PROGRESS: at sentence #1240000, processed 15450543 words, keeping 99531 word types\n",
      "2020-09-11 14:47:08,937 : INFO : PROGRESS: at sentence #1250000, processed 15570243 words, keeping 99926 word types\n",
      "2020-09-11 14:47:08,959 : INFO : PROGRESS: at sentence #1260000, processed 15694977 words, keeping 100304 word types\n",
      "2020-09-11 14:47:08,981 : INFO : PROGRESS: at sentence #1270000, processed 15815956 words, keeping 100728 word types\n",
      "2020-09-11 14:47:09,001 : INFO : PROGRESS: at sentence #1280000, processed 15935442 words, keeping 101038 word types\n",
      "2020-09-11 14:47:09,024 : INFO : PROGRESS: at sentence #1290000, processed 16054741 words, keeping 101400 word types\n",
      "2020-09-11 14:47:09,048 : INFO : PROGRESS: at sentence #1300000, processed 16181137 words, keeping 101770 word types\n",
      "2020-09-11 14:47:09,071 : INFO : PROGRESS: at sentence #1310000, processed 16310020 words, keeping 102238 word types\n",
      "2020-09-11 14:47:09,094 : INFO : PROGRESS: at sentence #1320000, processed 16431841 words, keeping 102644 word types\n",
      "2020-09-11 14:47:09,115 : INFO : PROGRESS: at sentence #1330000, processed 16556077 words, keeping 102925 word types\n",
      "2020-09-11 14:47:09,139 : INFO : PROGRESS: at sentence #1340000, processed 16674193 words, keeping 103312 word types\n",
      "2020-09-11 14:47:09,160 : INFO : PROGRESS: at sentence #1350000, processed 16791942 words, keeping 103715 word types\n",
      "2020-09-11 14:47:09,184 : INFO : PROGRESS: at sentence #1360000, processed 16914066 words, keeping 104062 word types\n",
      "2020-09-11 14:47:09,206 : INFO : PROGRESS: at sentence #1370000, processed 17033893 words, keeping 104422 word types\n",
      "2020-09-11 14:47:09,228 : INFO : PROGRESS: at sentence #1380000, processed 17159611 words, keeping 104803 word types\n",
      "2020-09-11 14:47:09,252 : INFO : PROGRESS: at sentence #1390000, processed 17282147 words, keeping 105131 word types\n",
      "2020-09-11 14:47:09,274 : INFO : PROGRESS: at sentence #1400000, processed 17407741 words, keeping 105531 word types\n",
      "2020-09-11 14:47:09,297 : INFO : PROGRESS: at sentence #1410000, processed 17531946 words, keeping 105897 word types\n",
      "2020-09-11 14:47:09,319 : INFO : PROGRESS: at sentence #1420000, processed 17658654 words, keeping 106270 word types\n",
      "2020-09-11 14:47:09,345 : INFO : PROGRESS: at sentence #1430000, processed 17782611 words, keeping 106636 word types\n",
      "2020-09-11 14:47:09,366 : INFO : PROGRESS: at sentence #1440000, processed 17910039 words, keeping 107029 word types\n",
      "2020-09-11 14:47:09,389 : INFO : PROGRESS: at sentence #1450000, processed 18035439 words, keeping 107381 word types\n",
      "2020-09-11 14:47:09,411 : INFO : PROGRESS: at sentence #1460000, processed 18159240 words, keeping 107727 word types\n",
      "2020-09-11 14:47:09,430 : INFO : PROGRESS: at sentence #1470000, processed 18278234 words, keeping 108057 word types\n",
      "2020-09-11 14:47:09,453 : INFO : PROGRESS: at sentence #1480000, processed 18401995 words, keeping 108406 word types\n",
      "2020-09-11 14:47:09,472 : INFO : PROGRESS: at sentence #1490000, processed 18519944 words, keeping 108693 word types\n",
      "2020-09-11 14:47:09,495 : INFO : PROGRESS: at sentence #1500000, processed 18645383 words, keeping 109065 word types\n",
      "2020-09-11 14:47:09,518 : INFO : PROGRESS: at sentence #1510000, processed 18768897 words, keeping 109421 word types\n",
      "2020-09-11 14:47:09,541 : INFO : PROGRESS: at sentence #1520000, processed 18898766 words, keeping 109794 word types\n",
      "2020-09-11 14:47:09,564 : INFO : PROGRESS: at sentence #1530000, processed 19025737 words, keeping 110110 word types\n",
      "2020-09-11 14:47:09,586 : INFO : PROGRESS: at sentence #1540000, processed 19153453 words, keeping 110463 word types\n",
      "2020-09-11 14:47:09,610 : INFO : PROGRESS: at sentence #1550000, processed 19281043 words, keeping 110873 word types\n",
      "2020-09-11 14:47:09,633 : INFO : PROGRESS: at sentence #1560000, processed 19404680 words, keeping 111188 word types\n",
      "2020-09-11 14:47:09,657 : INFO : PROGRESS: at sentence #1570000, processed 19526697 words, keeping 111595 word types\n",
      "2020-09-11 14:47:09,679 : INFO : PROGRESS: at sentence #1580000, processed 19652916 words, keeping 111960 word types\n",
      "2020-09-11 14:47:09,702 : INFO : PROGRESS: at sentence #1590000, processed 19782398 words, keeping 112301 word types\n",
      "2020-09-11 14:47:09,726 : INFO : PROGRESS: at sentence #1600000, processed 19904256 words, keeping 112658 word types\n",
      "2020-09-11 14:47:09,747 : INFO : PROGRESS: at sentence #1610000, processed 20025740 words, keeping 113068 word types\n",
      "2020-09-11 14:47:09,772 : INFO : PROGRESS: at sentence #1620000, processed 20149848 words, keeping 113362 word types\n",
      "2020-09-11 14:47:09,793 : INFO : PROGRESS: at sentence #1630000, processed 20270646 words, keeping 113690 word types\n",
      "2020-09-11 14:47:09,815 : INFO : PROGRESS: at sentence #1640000, processed 20395961 words, keeping 114148 word types\n",
      "2020-09-11 14:47:09,837 : INFO : PROGRESS: at sentence #1650000, processed 20517434 words, keeping 114520 word types\n",
      "2020-09-11 14:47:09,858 : INFO : PROGRESS: at sentence #1660000, processed 20640027 words, keeping 114893 word types\n",
      "2020-09-11 14:47:09,882 : INFO : PROGRESS: at sentence #1670000, processed 20767478 words, keeping 115334 word types\n",
      "2020-09-11 14:47:09,904 : INFO : PROGRESS: at sentence #1680000, processed 20889462 words, keeping 115689 word types\n",
      "2020-09-11 14:47:09,927 : INFO : PROGRESS: at sentence #1690000, processed 21014923 words, keeping 116015 word types\n",
      "2020-09-11 14:47:09,948 : INFO : PROGRESS: at sentence #1700000, processed 21140631 words, keeping 116406 word types\n",
      "2020-09-11 14:47:09,971 : INFO : PROGRESS: at sentence #1710000, processed 21264022 words, keeping 116704 word types\n",
      "2020-09-11 14:47:09,993 : INFO : PROGRESS: at sentence #1720000, processed 21389263 words, keeping 117056 word types\n",
      "2020-09-11 14:47:10,014 : INFO : PROGRESS: at sentence #1730000, processed 21517373 words, keeping 117403 word types\n",
      "2020-09-11 14:47:10,037 : INFO : PROGRESS: at sentence #1740000, processed 21639315 words, keeping 117708 word types\n",
      "2020-09-11 14:47:10,058 : INFO : PROGRESS: at sentence #1750000, processed 21764053 words, keeping 118043 word types\n",
      "2020-09-11 14:47:10,081 : INFO : PROGRESS: at sentence #1760000, processed 21888396 words, keeping 118419 word types\n",
      "2020-09-11 14:47:10,104 : INFO : PROGRESS: at sentence #1770000, processed 22013953 words, keeping 118784 word types\n",
      "2020-09-11 14:47:10,125 : INFO : PROGRESS: at sentence #1780000, processed 22141475 words, keeping 119129 word types\n",
      "2020-09-11 14:47:10,151 : INFO : PROGRESS: at sentence #1790000, processed 22267807 words, keeping 119545 word types\n",
      "2020-09-11 14:47:10,171 : INFO : PROGRESS: at sentence #1800000, processed 22390669 words, keeping 119843 word types\n",
      "2020-09-11 14:47:10,195 : INFO : PROGRESS: at sentence #1810000, processed 22512126 words, keeping 120136 word types\n",
      "2020-09-11 14:47:10,218 : INFO : PROGRESS: at sentence #1820000, processed 22641073 words, keeping 120429 word types\n",
      "2020-09-11 14:47:10,241 : INFO : PROGRESS: at sentence #1830000, processed 22767539 words, keeping 120744 word types\n",
      "2020-09-11 14:47:10,263 : INFO : PROGRESS: at sentence #1840000, processed 22891258 words, keeping 121076 word types\n",
      "2020-09-11 14:47:10,284 : INFO : PROGRESS: at sentence #1850000, processed 23013506 words, keeping 121356 word types\n",
      "2020-09-11 14:47:10,307 : INFO : PROGRESS: at sentence #1860000, processed 23140698 words, keeping 121661 word types\n",
      "2020-09-11 14:47:10,329 : INFO : PROGRESS: at sentence #1870000, processed 23268227 words, keeping 121977 word types\n",
      "2020-09-11 14:47:10,353 : INFO : PROGRESS: at sentence #1880000, processed 23395440 words, keeping 122309 word types\n",
      "2020-09-11 14:47:10,374 : INFO : PROGRESS: at sentence #1890000, processed 23514747 words, keeping 122642 word types\n",
      "2020-09-11 14:47:10,397 : INFO : PROGRESS: at sentence #1900000, processed 23642878 words, keeping 123013 word types\n",
      "2020-09-11 14:47:10,420 : INFO : PROGRESS: at sentence #1910000, processed 23766903 words, keeping 123421 word types\n",
      "2020-09-11 14:47:10,443 : INFO : PROGRESS: at sentence #1920000, processed 23894312 words, keeping 123745 word types\n",
      "2020-09-11 14:47:10,469 : INFO : PROGRESS: at sentence #1930000, processed 24018251 words, keeping 124093 word types\n",
      "2020-09-11 14:47:10,489 : INFO : PROGRESS: at sentence #1940000, processed 24138617 words, keeping 124354 word types\n",
      "2020-09-11 14:47:10,513 : INFO : PROGRESS: at sentence #1950000, processed 24263778 words, keeping 124731 word types\n",
      "2020-09-11 14:47:10,536 : INFO : PROGRESS: at sentence #1960000, processed 24391674 words, keeping 125065 word types\n",
      "2020-09-11 14:47:10,559 : INFO : PROGRESS: at sentence #1970000, processed 24520798 words, keeping 125351 word types\n",
      "2020-09-11 14:47:10,580 : INFO : PROGRESS: at sentence #1980000, processed 24640295 words, keeping 125613 word types\n",
      "2020-09-11 14:47:10,602 : INFO : PROGRESS: at sentence #1990000, processed 24765594 words, keeping 125915 word types\n",
      "2020-09-11 14:47:10,626 : INFO : PROGRESS: at sentence #2000000, processed 24892615 words, keeping 126206 word types\n",
      "2020-09-11 14:47:10,647 : INFO : PROGRESS: at sentence #2010000, processed 25013516 words, keeping 126534 word types\n",
      "2020-09-11 14:47:10,669 : INFO : PROGRESS: at sentence #2020000, processed 25135222 words, keeping 126776 word types\n",
      "2020-09-11 14:47:10,691 : INFO : PROGRESS: at sentence #2030000, processed 25256745 words, keeping 127216 word types\n",
      "2020-09-11 14:47:10,710 : INFO : PROGRESS: at sentence #2040000, processed 25366774 words, keeping 127480 word types\n",
      "2020-09-11 14:47:10,733 : INFO : PROGRESS: at sentence #2050000, processed 25489525 words, keeping 127768 word types\n",
      "2020-09-11 14:47:10,754 : INFO : PROGRESS: at sentence #2060000, processed 25611203 words, keeping 128133 word types\n",
      "2020-09-11 14:47:10,776 : INFO : PROGRESS: at sentence #2070000, processed 25731786 words, keeping 128436 word types\n",
      "2020-09-11 14:47:10,801 : INFO : PROGRESS: at sentence #2080000, processed 25854518 words, keeping 128747 word types\n",
      "2020-09-11 14:47:10,823 : INFO : PROGRESS: at sentence #2090000, processed 25980499 words, keeping 129013 word types\n",
      "2020-09-11 14:47:10,847 : INFO : PROGRESS: at sentence #2100000, processed 26102698 words, keeping 129357 word types\n",
      "2020-09-11 14:47:10,868 : INFO : PROGRESS: at sentence #2110000, processed 26227154 words, keeping 129655 word types\n",
      "2020-09-11 14:47:10,893 : INFO : PROGRESS: at sentence #2120000, processed 26354109 words, keeping 129990 word types\n",
      "2020-09-11 14:47:10,913 : INFO : PROGRESS: at sentence #2130000, processed 26472959 words, keeping 130277 word types\n",
      "2020-09-11 14:47:10,936 : INFO : PROGRESS: at sentence #2140000, processed 26598038 words, keeping 130621 word types\n",
      "2020-09-11 14:47:10,959 : INFO : PROGRESS: at sentence #2150000, processed 26720283 words, keeping 130892 word types\n",
      "2020-09-11 14:47:10,981 : INFO : PROGRESS: at sentence #2160000, processed 26850441 words, keeping 131242 word types\n",
      "2020-09-11 14:47:11,004 : INFO : PROGRESS: at sentence #2170000, processed 26976127 words, keeping 131621 word types\n",
      "2020-09-11 14:47:11,026 : INFO : PROGRESS: at sentence #2180000, processed 27100836 words, keeping 131912 word types\n",
      "2020-09-11 14:47:11,048 : INFO : PROGRESS: at sentence #2190000, processed 27225131 words, keeping 132236 word types\n",
      "2020-09-11 14:47:11,072 : INFO : PROGRESS: at sentence #2200000, processed 27351710 words, keeping 132559 word types\n",
      "2020-09-11 14:47:11,094 : INFO : PROGRESS: at sentence #2210000, processed 27475194 words, keeping 132835 word types\n",
      "2020-09-11 14:47:11,118 : INFO : PROGRESS: at sentence #2220000, processed 27600388 words, keeping 133087 word types\n",
      "2020-09-11 14:47:11,139 : INFO : PROGRESS: at sentence #2230000, processed 27723605 words, keeping 133387 word types\n",
      "2020-09-11 14:47:11,160 : INFO : PROGRESS: at sentence #2240000, processed 27850143 words, keeping 133650 word types\n",
      "2020-09-11 14:47:11,183 : INFO : PROGRESS: at sentence #2250000, processed 27974659 words, keeping 134040 word types\n",
      "2020-09-11 14:47:11,217 : INFO : PROGRESS: at sentence #2260000, processed 28098547 words, keeping 134356 word types\n",
      "2020-09-11 14:47:11,240 : INFO : PROGRESS: at sentence #2270000, processed 28224267 words, keeping 134650 word types\n",
      "2020-09-11 14:47:11,261 : INFO : PROGRESS: at sentence #2280000, processed 28348642 words, keeping 134914 word types\n",
      "2020-09-11 14:47:11,285 : INFO : PROGRESS: at sentence #2290000, processed 28472566 words, keeping 135242 word types\n",
      "2020-09-11 14:47:11,306 : INFO : PROGRESS: at sentence #2300000, processed 28596609 words, keeping 135566 word types\n",
      "2020-09-11 14:47:11,331 : INFO : PROGRESS: at sentence #2310000, processed 28716844 words, keeping 135900 word types\n",
      "2020-09-11 14:47:11,354 : INFO : PROGRESS: at sentence #2320000, processed 28845003 words, keeping 136113 word types\n",
      "2020-09-11 14:47:11,374 : INFO : PROGRESS: at sentence #2330000, processed 28964727 words, keeping 136461 word types\n",
      "2020-09-11 14:47:11,397 : INFO : PROGRESS: at sentence #2340000, processed 29083719 words, keeping 136777 word types\n",
      "2020-09-11 14:47:11,419 : INFO : PROGRESS: at sentence #2350000, processed 29211511 words, keeping 137093 word types\n",
      "2020-09-11 14:47:11,441 : INFO : PROGRESS: at sentence #2360000, processed 29334633 words, keeping 137365 word types\n",
      "2020-09-11 14:47:11,463 : INFO : PROGRESS: at sentence #2370000, processed 29457962 words, keeping 137650 word types\n",
      "2020-09-11 14:47:11,485 : INFO : PROGRESS: at sentence #2380000, processed 29586476 words, keeping 137965 word types\n",
      "2020-09-11 14:47:11,510 : INFO : PROGRESS: at sentence #2390000, processed 29711346 words, keeping 138319 word types\n",
      "2020-09-11 14:47:11,531 : INFO : PROGRESS: at sentence #2400000, processed 29834214 words, keeping 138652 word types\n",
      "2020-09-11 14:47:11,554 : INFO : PROGRESS: at sentence #2410000, processed 29956266 words, keeping 138970 word types\n",
      "2020-09-11 14:47:11,577 : INFO : PROGRESS: at sentence #2420000, processed 30086784 words, keeping 139253 word types\n",
      "2020-09-11 14:47:11,602 : INFO : PROGRESS: at sentence #2430000, processed 30212436 words, keeping 139538 word types\n",
      "2020-09-11 14:47:11,623 : INFO : PROGRESS: at sentence #2440000, processed 30330467 words, keeping 139806 word types\n",
      "2020-09-11 14:47:11,644 : INFO : PROGRESS: at sentence #2450000, processed 30455352 words, keeping 140111 word types\n",
      "2020-09-11 14:47:11,667 : INFO : PROGRESS: at sentence #2460000, processed 30580811 words, keeping 140368 word types\n",
      "2020-09-11 14:47:11,688 : INFO : PROGRESS: at sentence #2470000, processed 30706202 words, keeping 140663 word types\n",
      "2020-09-11 14:47:11,712 : INFO : PROGRESS: at sentence #2480000, processed 30830680 words, keeping 140964 word types\n",
      "2020-09-11 14:47:11,736 : INFO : PROGRESS: at sentence #2490000, processed 30958617 words, keeping 141200 word types\n",
      "2020-09-11 14:47:11,758 : INFO : PROGRESS: at sentence #2500000, processed 31086979 words, keeping 141503 word types\n",
      "2020-09-11 14:47:11,781 : INFO : PROGRESS: at sentence #2510000, processed 31206853 words, keeping 141834 word types\n",
      "2020-09-11 14:47:11,802 : INFO : PROGRESS: at sentence #2520000, processed 31331585 words, keeping 142133 word types\n",
      "2020-09-11 14:47:11,825 : INFO : PROGRESS: at sentence #2530000, processed 31452329 words, keeping 142471 word types\n",
      "2020-09-11 14:47:11,846 : INFO : PROGRESS: at sentence #2540000, processed 31573736 words, keeping 142723 word types\n",
      "2020-09-11 14:47:11,868 : INFO : PROGRESS: at sentence #2550000, processed 31699574 words, keeping 143017 word types\n",
      "2020-09-11 14:47:11,892 : INFO : PROGRESS: at sentence #2560000, processed 31822464 words, keeping 143376 word types\n",
      "2020-09-11 14:47:11,913 : INFO : PROGRESS: at sentence #2570000, processed 31945161 words, keeping 143620 word types\n",
      "2020-09-11 14:47:11,935 : INFO : PROGRESS: at sentence #2580000, processed 32069702 words, keeping 143912 word types\n",
      "2020-09-11 14:47:11,958 : INFO : PROGRESS: at sentence #2590000, processed 32192085 words, keeping 144162 word types\n",
      "2020-09-11 14:47:11,979 : INFO : PROGRESS: at sentence #2600000, processed 32312922 words, keeping 144436 word types\n",
      "2020-09-11 14:47:12,003 : INFO : PROGRESS: at sentence #2610000, processed 32430840 words, keeping 144808 word types\n",
      "2020-09-11 14:47:12,026 : INFO : PROGRESS: at sentence #2620000, processed 32554634 words, keeping 145050 word types\n",
      "2020-09-11 14:47:12,046 : INFO : PROGRESS: at sentence #2630000, processed 32677588 words, keeping 145280 word types\n",
      "2020-09-11 14:47:12,072 : INFO : PROGRESS: at sentence #2640000, processed 32804580 words, keeping 145592 word types\n",
      "2020-09-11 14:47:12,093 : INFO : PROGRESS: at sentence #2650000, processed 32927467 words, keeping 145958 word types\n",
      "2020-09-11 14:47:12,118 : INFO : PROGRESS: at sentence #2660000, processed 33052114 words, keeping 146231 word types\n",
      "2020-09-11 14:47:12,142 : INFO : PROGRESS: at sentence #2670000, processed 33178335 words, keeping 146509 word types\n",
      "2020-09-11 14:47:12,163 : INFO : PROGRESS: at sentence #2680000, processed 33300423 words, keeping 146819 word types\n",
      "2020-09-11 14:47:12,185 : INFO : PROGRESS: at sentence #2690000, processed 33421371 words, keeping 147073 word types\n",
      "2020-09-11 14:47:12,206 : INFO : PROGRESS: at sentence #2700000, processed 33541597 words, keeping 147362 word types\n",
      "2020-09-11 14:47:12,229 : INFO : PROGRESS: at sentence #2710000, processed 33664468 words, keeping 147711 word types\n",
      "2020-09-11 14:47:12,250 : INFO : PROGRESS: at sentence #2720000, processed 33785587 words, keeping 147949 word types\n",
      "2020-09-11 14:47:12,268 : INFO : PROGRESS: at sentence #2730000, processed 33882740 words, keeping 148444 word types\n",
      "2020-09-11 14:47:12,290 : INFO : PROGRESS: at sentence #2740000, processed 34008434 words, keeping 148725 word types\n",
      "2020-09-11 14:47:12,312 : INFO : PROGRESS: at sentence #2750000, processed 34134548 words, keeping 148978 word types\n",
      "2020-09-11 14:47:12,334 : INFO : PROGRESS: at sentence #2760000, processed 34259624 words, keeping 149279 word types\n",
      "2020-09-11 14:47:12,358 : INFO : PROGRESS: at sentence #2770000, processed 34381656 words, keeping 149563 word types\n",
      "2020-09-11 14:47:12,380 : INFO : PROGRESS: at sentence #2780000, processed 34507211 words, keeping 149855 word types\n",
      "2020-09-11 14:47:12,404 : INFO : PROGRESS: at sentence #2790000, processed 34628481 words, keeping 150071 word types\n",
      "2020-09-11 14:47:12,425 : INFO : PROGRESS: at sentence #2800000, processed 34754794 words, keeping 150401 word types\n",
      "2020-09-11 14:47:12,447 : INFO : PROGRESS: at sentence #2810000, processed 34880862 words, keeping 150685 word types\n",
      "2020-09-11 14:47:12,469 : INFO : PROGRESS: at sentence #2820000, processed 35004965 words, keeping 150890 word types\n",
      "2020-09-11 14:47:12,491 : INFO : PROGRESS: at sentence #2830000, processed 35129347 words, keeping 151146 word types\n",
      "2020-09-11 14:47:12,514 : INFO : PROGRESS: at sentence #2840000, processed 35253492 words, keeping 151433 word types\n",
      "2020-09-11 14:47:12,535 : INFO : PROGRESS: at sentence #2850000, processed 35375130 words, keeping 151739 word types\n",
      "2020-09-11 14:47:12,556 : INFO : PROGRESS: at sentence #2860000, processed 35497797 words, keeping 152050 word types\n",
      "2020-09-11 14:47:12,581 : INFO : PROGRESS: at sentence #2870000, processed 35626649 words, keeping 152294 word types\n",
      "2020-09-11 14:47:12,602 : INFO : PROGRESS: at sentence #2880000, processed 35747724 words, keeping 152562 word types\n",
      "2020-09-11 14:47:12,626 : INFO : PROGRESS: at sentence #2890000, processed 35876107 words, keeping 152827 word types\n",
      "2020-09-11 14:47:12,646 : INFO : PROGRESS: at sentence #2900000, processed 35995576 words, keeping 153058 word types\n",
      "2020-09-11 14:47:12,668 : INFO : PROGRESS: at sentence #2910000, processed 36119307 words, keeping 153301 word types\n",
      "2020-09-11 14:47:12,691 : INFO : PROGRESS: at sentence #2920000, processed 36247362 words, keeping 153545 word types\n",
      "2020-09-11 14:47:12,712 : INFO : PROGRESS: at sentence #2930000, processed 36367971 words, keeping 153858 word types\n",
      "2020-09-11 14:47:12,734 : INFO : PROGRESS: at sentence #2940000, processed 36488496 words, keeping 154092 word types\n",
      "2020-09-11 14:47:12,757 : INFO : PROGRESS: at sentence #2950000, processed 36614339 words, keeping 154308 word types\n",
      "2020-09-11 14:47:12,780 : INFO : PROGRESS: at sentence #2960000, processed 36742899 words, keeping 154579 word types\n",
      "2020-09-11 14:47:12,803 : INFO : PROGRESS: at sentence #2970000, processed 36870513 words, keeping 154822 word types\n",
      "2020-09-11 14:47:12,835 : INFO : PROGRESS: at sentence #2980000, processed 36992150 words, keeping 154999 word types\n",
      "2020-09-11 14:47:12,858 : INFO : PROGRESS: at sentence #2990000, processed 37118082 words, keeping 155279 word types\n",
      "2020-09-11 14:47:12,880 : INFO : PROGRESS: at sentence #3000000, processed 37246563 words, keeping 155579 word types\n",
      "2020-09-11 14:47:12,903 : INFO : PROGRESS: at sentence #3010000, processed 37372285 words, keeping 155844 word types\n",
      "2020-09-11 14:47:12,922 : INFO : PROGRESS: at sentence #3020000, processed 37495001 words, keeping 156055 word types\n",
      "2020-09-11 14:47:12,945 : INFO : PROGRESS: at sentence #3030000, processed 37621724 words, keeping 156362 word types\n",
      "2020-09-11 14:47:12,967 : INFO : PROGRESS: at sentence #3040000, processed 37747773 words, keeping 156610 word types\n",
      "2020-09-11 14:47:12,991 : INFO : PROGRESS: at sentence #3050000, processed 37870480 words, keeping 156908 word types\n",
      "2020-09-11 14:47:13,012 : INFO : PROGRESS: at sentence #3060000, processed 37991942 words, keeping 157092 word types\n",
      "2020-09-11 14:47:13,035 : INFO : PROGRESS: at sentence #3070000, processed 38119967 words, keeping 157357 word types\n",
      "2020-09-11 14:47:13,057 : INFO : PROGRESS: at sentence #3080000, processed 38245046 words, keeping 157584 word types\n",
      "2020-09-11 14:47:13,079 : INFO : PROGRESS: at sentence #3090000, processed 38369634 words, keeping 157947 word types\n",
      "2020-09-11 14:47:13,101 : INFO : PROGRESS: at sentence #3100000, processed 38492054 words, keeping 158243 word types\n",
      "2020-09-11 14:47:13,122 : INFO : PROGRESS: at sentence #3110000, processed 38616375 words, keeping 158481 word types\n",
      "2020-09-11 14:47:13,144 : INFO : PROGRESS: at sentence #3120000, processed 38739055 words, keeping 158837 word types\n",
      "2020-09-11 14:47:13,166 : INFO : PROGRESS: at sentence #3130000, processed 38862390 words, keeping 159163 word types\n",
      "2020-09-11 14:47:13,186 : INFO : PROGRESS: at sentence #3140000, processed 38982085 words, keeping 159392 word types\n",
      "2020-09-11 14:47:13,211 : INFO : PROGRESS: at sentence #3150000, processed 39104196 words, keeping 159670 word types\n",
      "2020-09-11 14:47:13,233 : INFO : PROGRESS: at sentence #3160000, processed 39231144 words, keeping 159884 word types\n",
      "2020-09-11 14:47:13,254 : INFO : PROGRESS: at sentence #3170000, processed 39353440 words, keeping 160165 word types\n",
      "2020-09-11 14:47:13,276 : INFO : PROGRESS: at sentence #3180000, processed 39477547 words, keeping 160376 word types\n",
      "2020-09-11 14:47:13,298 : INFO : PROGRESS: at sentence #3190000, processed 39605079 words, keeping 160708 word types\n",
      "2020-09-11 14:47:13,322 : INFO : PROGRESS: at sentence #3200000, processed 39730748 words, keeping 160943 word types\n",
      "2020-09-11 14:47:13,345 : INFO : PROGRESS: at sentence #3210000, processed 39856208 words, keeping 161211 word types\n",
      "2020-09-11 14:47:13,367 : INFO : PROGRESS: at sentence #3220000, processed 39981155 words, keeping 161460 word types\n",
      "2020-09-11 14:47:13,390 : INFO : PROGRESS: at sentence #3230000, processed 40106297 words, keeping 161781 word types\n",
      "2020-09-11 14:47:13,413 : INFO : PROGRESS: at sentence #3240000, processed 40230493 words, keeping 162029 word types\n",
      "2020-09-11 14:47:13,436 : INFO : PROGRESS: at sentence #3250000, processed 40348051 words, keeping 162276 word types\n",
      "2020-09-11 14:47:13,459 : INFO : PROGRESS: at sentence #3260000, processed 40476121 words, keeping 162564 word types\n",
      "2020-09-11 14:47:13,481 : INFO : PROGRESS: at sentence #3270000, processed 40599877 words, keeping 162829 word types\n",
      "2020-09-11 14:47:13,504 : INFO : PROGRESS: at sentence #3280000, processed 40726962 words, keeping 163090 word types\n",
      "2020-09-11 14:47:13,525 : INFO : PROGRESS: at sentence #3290000, processed 40848450 words, keeping 163280 word types\n",
      "2020-09-11 14:47:13,547 : INFO : PROGRESS: at sentence #3300000, processed 40975061 words, keeping 163557 word types\n",
      "2020-09-11 14:47:13,569 : INFO : PROGRESS: at sentence #3310000, processed 41095748 words, keeping 163820 word types\n",
      "2020-09-11 14:47:13,586 : INFO : PROGRESS: at sentence #3320000, processed 41187926 words, keeping 164035 word types\n",
      "2020-09-11 14:47:13,608 : INFO : PROGRESS: at sentence #3330000, processed 41311500 words, keeping 164318 word types\n",
      "2020-09-11 14:47:13,631 : INFO : PROGRESS: at sentence #3340000, processed 41436603 words, keeping 164573 word types\n",
      "2020-09-11 14:47:13,653 : INFO : PROGRESS: at sentence #3350000, processed 41558519 words, keeping 164803 word types\n",
      "2020-09-11 14:47:13,676 : INFO : PROGRESS: at sentence #3360000, processed 41684731 words, keeping 165091 word types\n",
      "2020-09-11 14:47:13,698 : INFO : PROGRESS: at sentence #3370000, processed 41807608 words, keeping 165259 word types\n",
      "2020-09-11 14:47:13,721 : INFO : PROGRESS: at sentence #3380000, processed 41934356 words, keeping 165602 word types\n",
      "2020-09-11 14:47:13,746 : INFO : PROGRESS: at sentence #3390000, processed 42060180 words, keeping 165844 word types\n",
      "2020-09-11 14:47:13,768 : INFO : PROGRESS: at sentence #3400000, processed 42183659 words, keeping 166194 word types\n",
      "2020-09-11 14:47:13,792 : INFO : PROGRESS: at sentence #3410000, processed 42311834 words, keeping 166540 word types\n",
      "2020-09-11 14:47:13,814 : INFO : PROGRESS: at sentence #3420000, processed 42436712 words, keeping 166801 word types\n",
      "2020-09-11 14:47:13,835 : INFO : PROGRESS: at sentence #3430000, processed 42558836 words, keeping 167059 word types\n",
      "2020-09-11 14:47:13,857 : INFO : PROGRESS: at sentence #3440000, processed 42683357 words, keeping 167302 word types\n",
      "2020-09-11 14:47:13,882 : INFO : PROGRESS: at sentence #3450000, processed 42808946 words, keeping 167605 word types\n",
      "2020-09-11 14:47:13,904 : INFO : PROGRESS: at sentence #3460000, processed 42928227 words, keeping 167822 word types\n",
      "2020-09-11 14:47:13,928 : INFO : PROGRESS: at sentence #3470000, processed 43051400 words, keeping 168062 word types\n",
      "2020-09-11 14:47:13,950 : INFO : PROGRESS: at sentence #3480000, processed 43177308 words, keeping 168361 word types\n",
      "2020-09-11 14:47:13,973 : INFO : PROGRESS: at sentence #3490000, processed 43304168 words, keeping 168623 word types\n",
      "2020-09-11 14:47:13,996 : INFO : PROGRESS: at sentence #3500000, processed 43429749 words, keeping 168891 word types\n",
      "2020-09-11 14:47:14,020 : INFO : PROGRESS: at sentence #3510000, processed 43551694 words, keeping 169161 word types\n",
      "2020-09-11 14:47:14,043 : INFO : PROGRESS: at sentence #3520000, processed 43677665 words, keeping 169455 word types\n",
      "2020-09-11 14:47:14,065 : INFO : PROGRESS: at sentence #3530000, processed 43801452 words, keeping 169696 word types\n",
      "2020-09-11 14:47:14,087 : INFO : PROGRESS: at sentence #3540000, processed 43922223 words, keeping 169943 word types\n",
      "2020-09-11 14:47:14,110 : INFO : PROGRESS: at sentence #3550000, processed 44047785 words, keeping 170205 word types\n",
      "2020-09-11 14:47:14,133 : INFO : PROGRESS: at sentence #3560000, processed 44170121 words, keeping 170443 word types\n",
      "2020-09-11 14:47:14,156 : INFO : PROGRESS: at sentence #3570000, processed 44297490 words, keeping 170719 word types\n",
      "2020-09-11 14:47:14,179 : INFO : PROGRESS: at sentence #3580000, processed 44418998 words, keeping 170965 word types\n",
      "2020-09-11 14:47:14,205 : INFO : PROGRESS: at sentence #3590000, processed 44542188 words, keeping 171224 word types\n",
      "2020-09-11 14:47:14,229 : INFO : PROGRESS: at sentence #3600000, processed 44671989 words, keeping 171498 word types\n",
      "2020-09-11 14:47:14,251 : INFO : PROGRESS: at sentence #3610000, processed 44794921 words, keeping 171778 word types\n",
      "2020-09-11 14:47:14,273 : INFO : PROGRESS: at sentence #3620000, processed 44915252 words, keeping 172032 word types\n",
      "2020-09-11 14:47:14,295 : INFO : PROGRESS: at sentence #3630000, processed 45042775 words, keeping 172291 word types\n",
      "2020-09-11 14:47:14,316 : INFO : PROGRESS: at sentence #3640000, processed 45166974 words, keeping 172540 word types\n",
      "2020-09-11 14:47:14,340 : INFO : PROGRESS: at sentence #3650000, processed 45291559 words, keeping 172806 word types\n",
      "2020-09-11 14:47:14,360 : INFO : PROGRESS: at sentence #3660000, processed 45412646 words, keeping 173051 word types\n",
      "2020-09-11 14:47:14,381 : INFO : PROGRESS: at sentence #3670000, processed 45533647 words, keeping 173305 word types\n",
      "2020-09-11 14:47:14,404 : INFO : PROGRESS: at sentence #3680000, processed 45659128 words, keeping 173488 word types\n",
      "2020-09-11 14:47:14,427 : INFO : PROGRESS: at sentence #3690000, processed 45786352 words, keeping 173797 word types\n",
      "2020-09-11 14:47:14,449 : INFO : PROGRESS: at sentence #3700000, processed 45908474 words, keeping 174052 word types\n",
      "2020-09-11 14:47:14,472 : INFO : PROGRESS: at sentence #3710000, processed 46033673 words, keeping 174297 word types\n",
      "2020-09-11 14:47:14,493 : INFO : PROGRESS: at sentence #3720000, processed 46157620 words, keeping 174576 word types\n",
      "2020-09-11 14:47:14,526 : INFO : PROGRESS: at sentence #3730000, processed 46279826 words, keeping 174819 word types\n",
      "2020-09-11 14:47:14,549 : INFO : PROGRESS: at sentence #3740000, processed 46408044 words, keeping 175131 word types\n",
      "2020-09-11 14:47:14,572 : INFO : PROGRESS: at sentence #3750000, processed 46535222 words, keeping 175330 word types\n",
      "2020-09-11 14:47:14,595 : INFO : PROGRESS: at sentence #3760000, processed 46659229 words, keeping 175561 word types\n",
      "2020-09-11 14:47:14,616 : INFO : PROGRESS: at sentence #3770000, processed 46784423 words, keeping 175745 word types\n",
      "2020-09-11 14:47:14,640 : INFO : PROGRESS: at sentence #3780000, processed 46906954 words, keeping 176013 word types\n",
      "2020-09-11 14:47:14,660 : INFO : PROGRESS: at sentence #3790000, processed 47027480 words, keeping 176241 word types\n",
      "2020-09-11 14:47:14,682 : INFO : PROGRESS: at sentence #3800000, processed 47152199 words, keeping 176497 word types\n",
      "2020-09-11 14:47:14,705 : INFO : PROGRESS: at sentence #3810000, processed 47271360 words, keeping 176723 word types\n",
      "2020-09-11 14:47:14,728 : INFO : PROGRESS: at sentence #3820000, processed 47397519 words, keeping 177004 word types\n",
      "2020-09-11 14:47:14,750 : INFO : PROGRESS: at sentence #3830000, processed 47522808 words, keeping 177246 word types\n",
      "2020-09-11 14:47:14,772 : INFO : PROGRESS: at sentence #3840000, processed 47642965 words, keeping 177494 word types\n",
      "2020-09-11 14:47:14,793 : INFO : PROGRESS: at sentence #3850000, processed 47762774 words, keeping 177731 word types\n",
      "2020-09-11 14:47:14,816 : INFO : PROGRESS: at sentence #3860000, processed 47887811 words, keeping 178006 word types\n",
      "2020-09-11 14:47:14,843 : INFO : PROGRESS: at sentence #3870000, processed 48014599 words, keeping 178233 word types\n",
      "2020-09-11 14:47:14,865 : INFO : PROGRESS: at sentence #3880000, processed 48138790 words, keeping 178459 word types\n",
      "2020-09-11 14:47:14,888 : INFO : PROGRESS: at sentence #3890000, processed 48260752 words, keeping 178690 word types\n",
      "2020-09-11 14:47:14,910 : INFO : PROGRESS: at sentence #3900000, processed 48382060 words, keeping 178928 word types\n",
      "2020-09-11 14:47:14,932 : INFO : PROGRESS: at sentence #3910000, processed 48508538 words, keeping 179208 word types\n",
      "2020-09-11 14:47:14,957 : INFO : PROGRESS: at sentence #3920000, processed 48638028 words, keeping 179443 word types\n",
      "2020-09-11 14:47:14,979 : INFO : PROGRESS: at sentence #3930000, processed 48764493 words, keeping 179689 word types\n",
      "2020-09-11 14:47:15,001 : INFO : PROGRESS: at sentence #3940000, processed 48891041 words, keeping 179922 word types\n",
      "2020-09-11 14:47:15,024 : INFO : PROGRESS: at sentence #3950000, processed 49014788 words, keeping 180194 word types\n",
      "2020-09-11 14:47:15,046 : INFO : PROGRESS: at sentence #3960000, processed 49142631 words, keeping 180452 word types\n",
      "2020-09-11 14:47:15,070 : INFO : PROGRESS: at sentence #3970000, processed 49266244 words, keeping 180725 word types\n",
      "2020-09-11 14:47:15,092 : INFO : PROGRESS: at sentence #3980000, processed 49389596 words, keeping 180983 word types\n",
      "2020-09-11 14:47:15,113 : INFO : PROGRESS: at sentence #3990000, processed 49510190 words, keeping 181236 word types\n",
      "2020-09-11 14:47:15,137 : INFO : PROGRESS: at sentence #4000000, processed 49637020 words, keeping 181511 word types\n",
      "2020-09-11 14:47:15,153 : INFO : collected 181678 word types from a corpus of 49713429 raw words and 4006269 sentences\n",
      "2020-09-11 14:47:15,154 : INFO : Loading a fresh vocabulary\n",
      "2020-09-11 14:47:15,244 : INFO : min_count=10 retains 40597 unique words (22% of original 181678, drops 141081)\n",
      "2020-09-11 14:47:15,244 : INFO : min_count=10 leaves 49428961 word corpus (99% of original 49713429, drops 284468)\n",
      "2020-09-11 14:47:15,345 : INFO : deleting the raw counts dictionary of 181678 items\n",
      "2020-09-11 14:47:15,349 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2020-09-11 14:47:15,350 : INFO : downsampling leaves estimated 43694806 word corpus (88.4% of prior 49428961)\n",
      "2020-09-11 14:47:15,453 : INFO : estimated required memory for 40597 words and 300 dimensions: 117731300 bytes\n",
      "2020-09-11 14:47:15,454 : INFO : resetting layer weights\n",
      "2020-09-11 14:47:15,916 : INFO : training model with 4 workers on 40597 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2020-09-11 14:47:16,926 : INFO : EPOCH 1 - PROGRESS: at 3.47% examples, 1522035 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:17,934 : INFO : EPOCH 1 - PROGRESS: at 6.99% examples, 1528714 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:18,936 : INFO : EPOCH 1 - PROGRESS: at 10.27% examples, 1498531 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:19,939 : INFO : EPOCH 1 - PROGRESS: at 12.98% examples, 1419760 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:20,951 : INFO : EPOCH 1 - PROGRESS: at 15.82% examples, 1380449 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:21,952 : INFO : EPOCH 1 - PROGRESS: at 18.76% examples, 1362346 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:22,952 : INFO : EPOCH 1 - PROGRESS: at 21.66% examples, 1350984 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:23,965 : INFO : EPOCH 1 - PROGRESS: at 24.60% examples, 1340249 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:24,971 : INFO : EPOCH 1 - PROGRESS: at 27.64% examples, 1340545 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:25,973 : INFO : EPOCH 1 - PROGRESS: at 30.99% examples, 1352824 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:26,975 : INFO : EPOCH 1 - PROGRESS: at 34.60% examples, 1370902 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:27,984 : INFO : EPOCH 1 - PROGRESS: at 38.19% examples, 1386470 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:28,985 : INFO : EPOCH 1 - PROGRESS: at 41.74% examples, 1399007 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:29,988 : INFO : EPOCH 1 - PROGRESS: at 45.25% examples, 1409192 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:30,992 : INFO : EPOCH 1 - PROGRESS: at 48.72% examples, 1416717 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:31,997 : INFO : EPOCH 1 - PROGRESS: at 52.32% examples, 1424844 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:33,001 : INFO : EPOCH 1 - PROGRESS: at 55.86% examples, 1432108 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:34,010 : INFO : EPOCH 1 - PROGRESS: at 59.44% examples, 1438664 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:35,012 : INFO : EPOCH 1 - PROGRESS: at 63.01% examples, 1445104 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:36,015 : INFO : EPOCH 1 - PROGRESS: at 66.63% examples, 1451169 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:37,019 : INFO : EPOCH 1 - PROGRESS: at 70.30% examples, 1456581 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:38,025 : INFO : EPOCH 1 - PROGRESS: at 73.88% examples, 1461006 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:39,027 : INFO : EPOCH 1 - PROGRESS: at 77.39% examples, 1464531 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:40,028 : INFO : EPOCH 1 - PROGRESS: at 80.99% examples, 1468929 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:41,031 : INFO : EPOCH 1 - PROGRESS: at 84.63% examples, 1472616 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:42,034 : INFO : EPOCH 1 - PROGRESS: at 88.20% examples, 1475820 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:43,040 : INFO : EPOCH 1 - PROGRESS: at 91.82% examples, 1479425 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:44,042 : INFO : EPOCH 1 - PROGRESS: at 95.42% examples, 1482556 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:45,043 : INFO : EPOCH 1 - PROGRESS: at 99.00% examples, 1485522 words/s, in_qsize 6, out_qsize 1\n",
      "2020-09-11 14:47:45,315 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-11 14:47:45,316 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-11 14:47:45,318 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-11 14:47:45,321 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-11 14:47:45,322 : INFO : EPOCH - 1 : training on 49713429 raw words (43693526 effective words) took 29.4s, 1486234 effective words/s\n",
      "2020-09-11 14:47:46,332 : INFO : EPOCH 2 - PROGRESS: at 3.47% examples, 1522744 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:47,332 : INFO : EPOCH 2 - PROGRESS: at 6.99% examples, 1533905 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-11 14:47:48,334 : INFO : EPOCH 2 - PROGRESS: at 10.45% examples, 1528059 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:49,336 : INFO : EPOCH 2 - PROGRESS: at 13.97% examples, 1530149 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:50,336 : INFO : EPOCH 2 - PROGRESS: at 17.13% examples, 1498000 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:51,339 : INFO : EPOCH 2 - PROGRESS: at 20.64% examples, 1505418 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:52,343 : INFO : EPOCH 2 - PROGRESS: at 23.79% examples, 1486519 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:53,343 : INFO : EPOCH 2 - PROGRESS: at 27.22% examples, 1490510 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:54,352 : INFO : EPOCH 2 - PROGRESS: at 30.38% examples, 1477656 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:55,355 : INFO : EPOCH 2 - PROGRESS: at 33.99% examples, 1484898 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:56,357 : INFO : EPOCH 2 - PROGRESS: at 37.54% examples, 1489953 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:57,357 : INFO : EPOCH 2 - PROGRESS: at 40.95% examples, 1490829 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:58,360 : INFO : EPOCH 2 - PROGRESS: at 44.08% examples, 1481293 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:47:59,366 : INFO : EPOCH 2 - PROGRESS: at 47.33% examples, 1477674 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:00,366 : INFO : EPOCH 2 - PROGRESS: at 50.94% examples, 1483379 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:01,367 : INFO : EPOCH 2 - PROGRESS: at 54.47% examples, 1486762 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:02,367 : INFO : EPOCH 2 - PROGRESS: at 58.01% examples, 1490704 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:03,367 : INFO : EPOCH 2 - PROGRESS: at 61.59% examples, 1494621 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:04,368 : INFO : EPOCH 2 - PROGRESS: at 65.23% examples, 1499147 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:05,372 : INFO : EPOCH 2 - PROGRESS: at 68.79% examples, 1500299 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:06,375 : INFO : EPOCH 2 - PROGRESS: at 72.37% examples, 1502982 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:07,379 : INFO : EPOCH 2 - PROGRESS: at 75.96% examples, 1506233 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:08,380 : INFO : EPOCH 2 - PROGRESS: at 79.55% examples, 1508696 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:09,382 : INFO : EPOCH 2 - PROGRESS: at 83.15% examples, 1510152 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:10,393 : INFO : EPOCH 2 - PROGRESS: at 86.82% examples, 1513313 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:11,398 : INFO : EPOCH 2 - PROGRESS: at 90.43% examples, 1515667 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:12,399 : INFO : EPOCH 2 - PROGRESS: at 94.01% examples, 1517628 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:13,402 : INFO : EPOCH 2 - PROGRESS: at 97.62% examples, 1519058 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:14,052 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-11 14:48:14,057 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-11 14:48:14,060 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-11 14:48:14,066 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-11 14:48:14,066 : INFO : EPOCH - 2 : training on 49713429 raw words (43694459 effective words) took 28.7s, 1520489 effective words/s\n",
      "2020-09-11 14:48:15,073 : INFO : EPOCH 3 - PROGRESS: at 3.53% examples, 1551987 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:16,074 : INFO : EPOCH 3 - PROGRESS: at 7.11% examples, 1561807 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:17,076 : INFO : EPOCH 3 - PROGRESS: at 10.65% examples, 1558028 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:18,079 : INFO : EPOCH 3 - PROGRESS: at 14.28% examples, 1563148 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:19,079 : INFO : EPOCH 3 - PROGRESS: at 17.57% examples, 1536800 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:20,079 : INFO : EPOCH 3 - PROGRESS: at 20.67% examples, 1507612 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:21,086 : INFO : EPOCH 3 - PROGRESS: at 24.21% examples, 1512785 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:22,087 : INFO : EPOCH 3 - PROGRESS: at 27.76% examples, 1519940 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:23,088 : INFO : EPOCH 3 - PROGRESS: at 31.40% examples, 1527533 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:24,090 : INFO : EPOCH 3 - PROGRESS: at 35.08% examples, 1533356 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:25,093 : INFO : EPOCH 3 - PROGRESS: at 38.63% examples, 1535522 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:26,098 : INFO : EPOCH 3 - PROGRESS: at 42.22% examples, 1537141 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-11 14:48:27,106 : INFO : EPOCH 3 - PROGRESS: at 45.76% examples, 1538293 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:28,108 : INFO : EPOCH 3 - PROGRESS: at 49.32% examples, 1539797 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:29,116 : INFO : EPOCH 3 - PROGRESS: at 52.95% examples, 1540492 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:30,120 : INFO : EPOCH 3 - PROGRESS: at 56.55% examples, 1542750 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-11 14:48:31,121 : INFO : EPOCH 3 - PROGRESS: at 60.11% examples, 1543216 words/s, in_qsize 8, out_qsize 1\n",
      "2020-09-11 14:48:32,131 : INFO : EPOCH 3 - PROGRESS: at 63.74% examples, 1545028 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:33,136 : INFO : EPOCH 3 - PROGRESS: at 67.41% examples, 1547044 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:34,136 : INFO : EPOCH 3 - PROGRESS: at 71.09% examples, 1548643 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:35,139 : INFO : EPOCH 3 - PROGRESS: at 74.71% examples, 1550335 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:36,143 : INFO : EPOCH 3 - PROGRESS: at 78.27% examples, 1550227 words/s, in_qsize 8, out_qsize 1\n",
      "2020-09-11 14:48:37,146 : INFO : EPOCH 3 - PROGRESS: at 81.83% examples, 1550586 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:38,147 : INFO : EPOCH 3 - PROGRESS: at 85.50% examples, 1551807 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:39,150 : INFO : EPOCH 3 - PROGRESS: at 89.08% examples, 1551985 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:40,152 : INFO : EPOCH 3 - PROGRESS: at 92.70% examples, 1553035 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:41,152 : INFO : EPOCH 3 - PROGRESS: at 96.31% examples, 1553673 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:42,147 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-11 14:48:42,151 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-11 14:48:42,155 : INFO : EPOCH 3 - PROGRESS: at 99.98% examples, 1555522 words/s, in_qsize 1, out_qsize 1\n",
      "2020-09-11 14:48:42,156 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-11 14:48:42,161 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-11 14:48:42,162 : INFO : EPOCH - 3 : training on 49713429 raw words (43692953 effective words) took 28.1s, 1555457 effective words/s\n",
      "2020-09-11 14:48:43,172 : INFO : EPOCH 4 - PROGRESS: at 3.57% examples, 1570794 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:44,178 : INFO : EPOCH 4 - PROGRESS: at 7.21% examples, 1580366 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:45,181 : INFO : EPOCH 4 - PROGRESS: at 10.79% examples, 1576051 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:46,186 : INFO : EPOCH 4 - PROGRESS: at 13.97% examples, 1528093 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:47,197 : INFO : EPOCH 4 - PROGRESS: at 17.15% examples, 1494898 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:48,197 : INFO : EPOCH 4 - PROGRESS: at 20.17% examples, 1466691 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-11 14:48:49,199 : INFO : EPOCH 4 - PROGRESS: at 23.75% examples, 1481325 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:50,200 : INFO : EPOCH 4 - PROGRESS: at 27.16% examples, 1484551 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:51,207 : INFO : EPOCH 4 - PROGRESS: at 30.40% examples, 1476752 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:52,209 : INFO : EPOCH 4 - PROGRESS: at 34.07% examples, 1486556 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:53,218 : INFO : EPOCH 4 - PROGRESS: at 37.66% examples, 1492294 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:54,223 : INFO : EPOCH 4 - PROGRESS: at 41.20% examples, 1496670 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:55,227 : INFO : EPOCH 4 - PROGRESS: at 44.77% examples, 1501975 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:56,232 : INFO : EPOCH 4 - PROGRESS: at 48.24% examples, 1503364 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:57,237 : INFO : EPOCH 4 - PROGRESS: at 51.92% examples, 1508513 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:58,244 : INFO : EPOCH 4 - PROGRESS: at 55.46% examples, 1510723 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:48:59,248 : INFO : EPOCH 4 - PROGRESS: at 59.07% examples, 1514053 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:00,248 : INFO : EPOCH 4 - PROGRESS: at 62.61% examples, 1516283 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:01,254 : INFO : EPOCH 4 - PROGRESS: at 66.32% examples, 1520641 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:02,255 : INFO : EPOCH 4 - PROGRESS: at 69.96% examples, 1522608 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:03,258 : INFO : EPOCH 4 - PROGRESS: at 73.59% examples, 1525115 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:04,261 : INFO : EPOCH 4 - PROGRESS: at 77.14% examples, 1527004 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:05,266 : INFO : EPOCH 4 - PROGRESS: at 80.77% examples, 1529036 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:06,267 : INFO : EPOCH 4 - PROGRESS: at 84.39% examples, 1530082 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:07,268 : INFO : EPOCH 4 - PROGRESS: at 87.95% examples, 1531229 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:08,271 : INFO : EPOCH 4 - PROGRESS: at 91.58% examples, 1533026 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:09,276 : INFO : EPOCH 4 - PROGRESS: at 95.16% examples, 1533812 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:10,285 : INFO : EPOCH 4 - PROGRESS: at 98.78% examples, 1535261 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:10,605 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-11 14:49:10,609 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-11 14:49:10,613 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-11 14:49:10,616 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-11 14:49:10,617 : INFO : EPOCH - 4 : training on 49713429 raw words (43693901 effective words) took 28.4s, 1536047 effective words/s\n",
      "2020-09-11 14:49:11,625 : INFO : EPOCH 5 - PROGRESS: at 3.41% examples, 1497500 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:12,627 : INFO : EPOCH 5 - PROGRESS: at 7.01% examples, 1537573 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:13,631 : INFO : EPOCH 5 - PROGRESS: at 10.61% examples, 1550238 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:14,637 : INFO : EPOCH 5 - PROGRESS: at 14.26% examples, 1558167 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:15,644 : INFO : EPOCH 5 - PROGRESS: at 17.87% examples, 1559263 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:16,657 : INFO : EPOCH 5 - PROGRESS: at 21.38% examples, 1553598 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:17,658 : INFO : EPOCH 5 - PROGRESS: at 24.92% examples, 1552160 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:18,661 : INFO : EPOCH 5 - PROGRESS: at 28.42% examples, 1552018 words/s, in_qsize 8, out_qsize 0\n",
      "2020-09-11 14:49:19,667 : INFO : EPOCH 5 - PROGRESS: at 32.04% examples, 1552936 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:20,670 : INFO : EPOCH 5 - PROGRESS: at 35.52% examples, 1548105 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:21,676 : INFO : EPOCH 5 - PROGRESS: at 39.08% examples, 1548633 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:22,680 : INFO : EPOCH 5 - PROGRESS: at 42.66% examples, 1549265 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:23,683 : INFO : EPOCH 5 - PROGRESS: at 46.25% examples, 1551360 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:24,685 : INFO : EPOCH 5 - PROGRESS: at 49.78% examples, 1551350 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:25,686 : INFO : EPOCH 5 - PROGRESS: at 53.40% examples, 1551436 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:26,691 : INFO : EPOCH 5 - PROGRESS: at 56.89% examples, 1550065 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:27,694 : INFO : EPOCH 5 - PROGRESS: at 60.46% examples, 1550564 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:28,697 : INFO : EPOCH 5 - PROGRESS: at 64.05% examples, 1551117 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:29,701 : INFO : EPOCH 5 - PROGRESS: at 67.68% examples, 1551818 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:30,702 : INFO : EPOCH 5 - PROGRESS: at 71.25% examples, 1550884 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:31,711 : INFO : EPOCH 5 - PROGRESS: at 74.81% examples, 1550847 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:32,712 : INFO : EPOCH 5 - PROGRESS: at 78.33% examples, 1550117 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:33,718 : INFO : EPOCH 5 - PROGRESS: at 81.85% examples, 1549507 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:34,723 : INFO : EPOCH 5 - PROGRESS: at 85.48% examples, 1549908 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:35,726 : INFO : EPOCH 5 - PROGRESS: at 89.04% examples, 1549757 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:36,727 : INFO : EPOCH 5 - PROGRESS: at 92.62% examples, 1550238 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:37,735 : INFO : EPOCH 5 - PROGRESS: at 96.21% examples, 1550276 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:38,736 : INFO : EPOCH 5 - PROGRESS: at 99.75% examples, 1550279 words/s, in_qsize 7, out_qsize 0\n",
      "2020-09-11 14:49:38,788 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-11 14:49:38,795 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-11 14:49:38,799 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-11 14:49:38,810 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-11 14:49:38,811 : INFO : EPOCH - 5 : training on 49713429 raw words (43693754 effective words) took 28.2s, 1550126 effective words/s\n",
      "2020-09-11 14:49:38,811 : INFO : training on a 248567145 raw words (218468593 effective words) took 142.9s, 1528886 effective words/s\n",
      "2020-09-11 14:49:38,812 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-09-11 14:49:39,094 : INFO : saving Word2Vec object under hansardQuestions, separately None\n",
      "2020-09-11 14:49:39,095 : INFO : storing np array 'vectors' to hansardQuestions.wv.vectors.npy\n",
      "2020-09-11 14:49:39,185 : INFO : not storing attribute vectors_norm\n",
      "2020-09-11 14:49:39,187 : INFO : storing np array 'syn1neg' to hansardQuestions.trainables.syn1neg.npy\n",
      "2020-09-11 14:49:39,274 : INFO : not storing attribute cum_table\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2020-09-11 14:49:39,376 : INFO : saved hansardQuestions\n",
      "2020-09-11 14:49:39,377 : INFO : loading Word2Vec object from hansardQuestions\n",
      "2020-09-11 14:49:39,440 : INFO : loading wv recursively from hansardQuestions.wv.* with mmap=None\n",
      "2020-09-11 14:49:39,441 : INFO : loading vectors from hansardQuestions.wv.vectors.npy with mmap=None\n",
      "2020-09-11 14:49:39,464 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-09-11 14:49:39,465 : INFO : loading vocabulary recursively from hansardQuestions.vocabulary.* with mmap=None\n",
      "2020-09-11 14:49:39,465 : INFO : loading trainables recursively from hansardQuestions.trainables.* with mmap=None\n",
      "2020-09-11 14:49:39,467 : INFO : loading syn1neg from hansardQuestions.trainables.syn1neg.npy with mmap=None\n",
      "2020-09-11 14:49:39,488 : INFO : setting ignored attribute cum_table to None\n",
      "2020-09-11 14:49:39,488 : INFO : loaded hansardQuestions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process complete--the first 25 words in the vocabulary are:\n",
      "['madam', 'speaker', 'have', 'honour', 'present', 'both', 'official', 'languages', 'second', 'report', 'standing', 'committee', 'justice', 'human', 'rights', 'relation', 'bill', 'c10', 'act', 'enact', 'victims', 'terrorism', 'to', 'amend', 'state']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Fri Jan  4 05:38:46 2019\n",
    "@author: chriscochrane\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Phrases\n",
    "import logging\n",
    "\n",
    "\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "hansardSpeeches = pd.read_csv('hansardExtractedSpeechesFull.csv', sep=\"\\t\", encoding=\"utf-8\", header=0) \n",
    "\n",
    "\n",
    "\n",
    "print(hansardSpeeches['mentionedEntityName'][1])\n",
    "\n",
    "def sentence_to_wordlist(sentence, remove_stopwords=False):\n",
    "    sentence_text = re.sub(r'[^\\w\\s]','', sentence)\n",
    "    words = sentence_text.lower().split()\n",
    "\n",
    "    for word in words: #Remove Stopwords (Cochrane)\n",
    "        if word in stopwords:\n",
    "            words.remove(word)\n",
    "\n",
    "    return words\n",
    "\n",
    "def hansard_to_sentences(hansard, tokenizer, remove_stopwords=False ):\n",
    "    #print(\"currently processing: word tokenizer\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # 1. Use the NLTK tokenizer to split the text into sentences\n",
    "        raw_sentences = tokenizer.tokenize(hansard.strip())\n",
    "        # 2. Loop over each sentence\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            # If a sentence is empty, skip it\n",
    "            if len(raw_sentence) > 0:\n",
    "                # Otherwise, call sentence_to_wordlist to get a list of words\n",
    "                sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "        # 3. Return the list of sentences (each sentence is a list of words, so this returns a list of lists)\n",
    "        len(sentences)\n",
    "        return sentences\n",
    "    except:\n",
    "        print('nope')\n",
    "\n",
    "    end_time = time.time()-start_time\n",
    "\n",
    "questions = hansardSpeeches['speech']\n",
    "\n",
    "questions = pd.Series.tolist(questions)\n",
    "sentences = []\n",
    "\n",
    "for i in range(0,len(questions)):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Need to first change \"./.\" to \".\" so that sentences parse correctly\n",
    "        hansard = questions[i].replace(\"/.\", '')\n",
    "        # Now apply functions\n",
    "        sentences += hansard_to_sentences(hansard, tokenizer)\n",
    "    except:\n",
    "        print('no!')\n",
    "\n",
    "print(\"There are \" + str(len(sentences)) + \" sentences in our corpus of questions.\")\n",
    "\n",
    "print(\"currently processing: training model\")\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 10   # Minimum word count \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 6           # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "iterations = 5        # Epochs, 5 is default\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, iter=iterations)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "model_name = 'hansardQuestions'\n",
    "model.save(model_name)\n",
    "new_model = gensim.models.Word2Vec.load('hansardQuestions')\n",
    "\n",
    "vocab = list(model.wv.vocab.keys())\n",
    "\n",
    "\n",
    "print(\"Process complete--the first 25 words in the vocabulary are:\")\n",
    "\n",
    "print(vocab[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Positive: [('excellent', 0.2646590058375939), ('outstanding', 0.2212536033509594), ('mentorship', 0.22074900753260668), ('invaluable', 0.21737490034471763), ('highquality', 0.21481580187637048), ('innovative', 0.21233921025935604), ('cooperative', 0.19852254101654596), ('tirelessly', 0.19441230001342802), ('welcome', 0.19195264057452213), ('develop', 0.18963906731900787), ('talents', 0.18880922264327804), ('collaboratively', 0.18879452521764434), ('midwives', 0.18672595078660845), ('excellence', 0.18635928122468895), ('collaboration', 0.18456441907168294), ('thoughtful', 0.18274887654800306), ('dedicated', 0.18254853884006073), ('diligent', 0.18120867219162248), ('diligently', 0.18069801331470703), ('productive', 0.18068655745303022), ('constructive', 0.18039648958676818), ('secure', 0.17893176213810552), ('best', 0.17813326050871692), ('cooperatively', 0.17805248108901445), ('happy', 0.1763290887509615), ('highimpact', 0.17622271639581888), ('strengthen', 0.1761013943410477), ('flexible', 0.17596027544991097), ('fortunate', 0.17593999768257354), ('collaborative', 0.1747343649007415), ('mentoring', 0.17447793078799054), ('nonpartisan', 0.17396414210128502), ('continue', 0.17107951328695892), ('fostering', 0.1710263716754077), ('ideally', 0.17097180016296193), ('cooperatives', 0.17066031112239255), ('thank', 0.170555844803443), ('mutual', 0.1704582570191826), ('promote', 0.17040351818619584), ('fantastic', 0.16996221541235496), ('healthy', 0.16962206374798824), ('inclusive', 0.1694885790377839), ('ethic', 0.16922962141108344), ('harmony', 0.16910104367726286), ('enable', 0.16896183913317414), ('vibrant', 0.1688496032034801), ('constructively', 0.16857995324130645), ('worldclass', 0.1679501779484365), ('efficient', 0.16778399221062976), ('chars', 0.16777072688198622), ('build', 0.16769024621702705), ('collaborate', 0.16638755904595928), ('volunteerism', 0.16616151119474448), ('strive', 0.16592098988968462), ('hospice', 0.165777340537423), ('oakville', 0.1651445074489637), ('interpreters', 0.16505845935433663), ('wonderful', 0.1649224504436503), ('teamwork', 0.16436321876479093), ('solid', 0.16411579014082), ('expertise', 0.163859648430564), ('worldleading', 0.16357492328046963), ('enables', 0.16338100492135169), ('explore', 0.16261342776048865), ('knowhow', 0.16254040490985405), ('nurture', 0.1621838230741176), ('enhance', 0.1617774030201688), ('communitybased', 0.16156639137504192), ('entrepreneurial', 0.16090765459311546), ('rotarians', 0.15999159522025275), ('strives', 0.15988861135510077), ('apprentices', 0.15962950530907286), ('robust', 0.1596140899754686), ('venus', 0.15948036767541174), ('pleased', 0.15917817103228304), ('valuable', 0.15836930638541483), ('ncc', 0.1576596497705552), ('coops', 0.1576192802242618), ('volunteer', 0.15742946063772859), ('abilities', 0.15717849000676964), ('salute', 0.15665930586152918), ('apprenticeship', 0.1565275431127677), ('fabulous', 0.15651800335232857), ('handson', 0.15632384262217935), ('congratulate', 0.15618974602246471), ('honoured', 0.1561252862401194), ('encourage', 0.15596249439809895), ('prosperous', 0.15577335310764484), ('locally', 0.15559727295713657), ('workable', 0.15555245456475658), ('volunteers', 0.15550382726980627), ('outreach', 0.1550601741912682), ('enabling', 0.15469925098798556), ('sustainable', 0.15378230631045026), ('better', 0.15371402558604483), ('showcase', 0.15350439753337414), ('proud', 0.15341181092503578), ('foster', 0.15331024133052992), ('partnerships', 0.15311750165588608), ('terrific', 0.1530611924384327)]\n",
      "Top Negative: [('indifference', -0.22716033033922672), ('worst', -0.22717426420446368), ('doubletalk', -0.22754935637750545), ('disgusting', -0.22761743962649172), ('e160coli', -0.22769929467495634), ('grossly', -0.22785831854151734), ('illogical', -0.2281290558296606), ('marred', -0.2281992300955076), ('dumbeddown', -0.2282385868116093), ('intolerable', -0.22845042298837812), ('oblivious', -0.22849857399972048), ('ridicule', -0.22877398714392908), ('brutal', -0.22877954499817402), ('negligence', -0.22927009744459076), ('perverse', -0.23025812239394472), ('needlessly', -0.23037855970353727), ('unfathomable', -0.2306167265014201), ('terrifying', -0.2307540578748638), ('irresponsible', -0.2309675122936606), ('inevitable', -0.23174386460867263), ('demise', -0.23193390687321283), ('smacks', -0.23200445487154592), ('escalate', -0.2322154069854102), ('antidemocratic', -0.23264314544514636), ('callous', -0.23268310521345018), ('shocking', -0.2329578099630187), ('mayhem', -0.23324351516236874), ('reprehensible', -0.2334312856844986), ('wanton', -0.23443458017526847), ('irrational', -0.23491049665205344), ('shameful', -0.23498251638912937), ('indecision', -0.2349901419913959), ('complicity', -0.23499198162342103), ('outrageously', -0.2352093310195675), ('absurd', -0.23523961515875824), ('shortsightedness', -0.23528887187666897), ('cowardly', -0.23580311608147014), ('immoral', -0.2358432852048262), ('humiliation', -0.23585338452853732), ('unbelievable', -0.23632553178688298), ('inhumane', -0.23753454237494184), ('inhuman', -0.2375443346530437), ('footdragging', -0.23776339880487798), ('shock', -0.23799041398552634), ('devastation', -0.23811936339726397), ('insensitive', -0.2383565763531823), ('uncaring', -0.23845892888307216), ('deadly', -0.23846056009037098), ('horror', -0.2385586339508705), ('meanspirited', -0.23881170844052813), ('exploded', -0.23895692744471647), ('starvation', -0.23912477969566606), ('unimaginable', -0.24036240367943149), ('grotesque', -0.24073951909891664), ('uncontrollable', -0.24083551970675698), ('catastrophic', -0.24114439251464523), ('bungling', -0.2417142907663628), ('ugly', -0.24187435567718923), ('unthinkable', -0.24197284226824153), ('chaos', -0.2425906705372431), ('mishandling', -0.24278646318061356), ('despicable', -0.243145153141638), ('unjustifiable', -0.24323958740254617), ('brutality', -0.24379531799993198), ('hysteria', -0.2441462492292218), ('unwarranted', -0.24436359916030628), ('selfinflicted', -0.24470577499629692), ('cruel', -0.2450874727415439), ('terribly', -0.24558765901411828), ('appalling', -0.2458619000854968), ('doublespeak', -0.24660977761034986), ('devastating', -0.24773454267013106), ('fallout', -0.2477442165094127), ('bloody', -0.2483958411751362), ('disgraceful', -0.24877158130927063), ('hoax', -0.24916918317885034), ('unfortunate', -0.25001077444541076), ('scandalous', -0.2512835859691177), ('downright', -0.2513086637000355), ('unjustified', -0.25208601989470086), ('unspeakable', -0.25217923081782273), ('perpetuated', -0.25266149657059983), ('inexcusable', -0.2531725624639908), ('horribly', -0.2536099962350867), ('horrific', -0.25461091576680656), ('vindictive', -0.25550515377199123), ('rash', -0.2556416437068501), ('deception', -0.2559147698650043), ('senseless', -0.25611408916766715), ('avoidable', -0.25621858900780964), ('heartless', -0.2564725941468949), ('indiscriminate', -0.26259439197912615), ('outrageous', -0.2635261886906141), ('disastrous', -0.2669385147106239), ('horrendous', -0.26798061298557957), ('unforgivable', -0.2710790379997461), ('panic', -0.274180638499337), ('pernicious', -0.28253853560819336), ('terrible', -0.29075825254365395), ('horrible', -0.2943638412836602)]\n",
      "Total Vocabulary Size: 40597\n",
      "good 59784\n",
      "excellent 7512\n",
      "correct 6258\n",
      "best 29659\n",
      "happy 9667\n",
      "positive 9799\n",
      "fortunate 2029\n",
      "bad 10374\n",
      "terrible 3645\n",
      "wrong 12455\n",
      "worst 3807\n",
      "disappointed 3380\n",
      "negative 4361\n",
      "unfortunate 4461\n",
      "Warning! Word:  urination  from speech:  9  not in w2v model!\n",
      "Warning! Word:  battlefords  from speech:  14  not in w2v model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:244: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Word:  petri  from speech:  107  not in w2v model!\n",
      "Warning! Word:  diversecity  from speech:  133  not in w2v model!\n",
      "Warning! Word:  dina  from speech:  134  not in w2v model!\n",
      "Warning! Word:  westholm  from speech:  164  not in w2v model!\n",
      "Warning! Word:  eastman  from speech:  235  not in w2v model!\n",
      "Warning! Word:  shangri  from speech:  266  not in w2v model!\n",
      "Warning! Word:  stephane  from speech:  273  not in w2v model!\n",
      "Warning! Word:  lamely  from speech:  288  not in w2v model!\n",
      "Warning! Word:  bas  from speech:  296  not in w2v model!\n",
      "Warning! Word:  gaspesie  from speech:  296  not in w2v model!\n",
      "Warning! Word:  îles  from speech:  296  not in w2v model!\n",
      "Warning! Word:  fated  from speech:  326  not in w2v model!\n",
      "Warning! Word:  agri  from speech:  355  not in w2v model!\n",
      "Warning! Word:  weeknd  from speech:  369  not in w2v model!\n",
      "Warning! Word:  mydemocracy  from speech:  383  not in w2v model!\n",
      "Warning! Word:  ca  from speech:  383  not in w2v model!\n",
      "Warning! Word:  megantic  from speech:  419  not in w2v model!\n",
      "Warning! Word:  na  from speech:  424  not in w2v model!\n",
      "Warning! Word:  labellethat  from speech:  426  not in w2v model!\n",
      "Warning! Word:  tunney  from speech:  449  not in w2v model!\n",
      "Warning! Word:  trademia  from speech:  450  not in w2v model!\n",
      "Warning! Word:  astonishes  from speech:  453  not in w2v model!\n",
      "Warning! Word:  unprecedentedly  from speech:  509  not in w2v model!\n",
      "Warning! Word:  valls  from speech:  512  not in w2v model!\n",
      "Warning! Word:  joyceville  from speech:  604  not in w2v model!\n",
      "Warning! Word:  ll  from speech:  609  not in w2v model!\n",
      "Warning! Word:  socio  from speech:  612  not in w2v model!\n",
      "Warning! Word:  agri  from speech:  619  not in w2v model!\n",
      "Warning! Word:  aquadvantage  from speech:  619  not in w2v model!\n",
      "Warning! Word:  haney  from speech:  675  not in w2v model!\n",
      "Warning! Word:  arcade  from speech:  682  not in w2v model!\n",
      "Warning! Word:  hyeon  from speech:  698  not in w2v model!\n",
      "Warning! Word:  gaspe  from speech:  819  not in w2v model!\n",
      "Warning! Word:  issuances  from speech:  828  not in w2v model!\n",
      "Warning! Word:  ostracizing  from speech:  837  not in w2v model!\n",
      "Warning! Word:  toope  from speech:  897  not in w2v model!\n",
      "Warning! Word:  audcent  from speech:  911  not in w2v model!\n",
      "Warning! Word:  bnn  from speech:  925  not in w2v model!\n",
      "Warning! Word:  odawa  from speech:  1007  not in w2v model!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jan  4 14:54:42 2019\n",
    "@author: chris cochrane\n",
    "NOTES: Requires Windows\n",
    "\"\"\"\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Initialization\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import pylab as pl\n",
    "import scipy.stats as stats\n",
    "import nltk\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Loading stored w2v Model\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "model = gensim.models.Word2Vec.load('hansardQuestions')\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Seed Words (Adapted from Turney and Littman)\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "good = model.wv['good'].astype('float64')\n",
    "excellent = model.wv['excellent'].astype('float64')\n",
    "correct = model.wv['correct'].astype('float64')\n",
    "best = model.wv['best'].astype('float64')\n",
    "happy = model.wv['happy'].astype('float64')\n",
    "positive = model.wv['positive'].astype('float64')\n",
    "fortunate = model.wv['fortunate'].astype('float64')\n",
    "\n",
    "\n",
    "bad = model.wv['bad'].astype('float64')\n",
    "terrible = model.wv['terrible'].astype('float64')\n",
    "wrong = model.wv['wrong'].astype('float64')\n",
    "worst = model.wv['worst'].astype('float64')\n",
    "disappointed = model.wv['disappointed'].astype('float64')\n",
    "negative = model.wv['negative'].astype('float64')\n",
    "unfortunate = model.wv['unfortunate'].astype('float64')\n",
    "\n",
    "vocab = list(model.wv.vocab.keys()) #the full vocabulary of Hansard\n",
    "\n",
    "\n",
    "# an empty list for storing wights and an empty dictionary for linking\n",
    "# weights and words\n",
    "\n",
    "runningTally=[]\n",
    "dictOfWeights = {}\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Model\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "'''for every word in the hansard, calculate its cosine similarity to the \n",
    "lists of positive words and negative words, then substract the sum of that\n",
    "word's cosine simlarity to negative seed words from its cosine similarity to the\n",
    "postive seed words.''' \n",
    "\n",
    "for word in vocab:\n",
    "\n",
    "    word_model = model.wv[word].astype('float64')\n",
    "\n",
    "    pos1 = (np.dot(word_model, good) / (np.linalg.norm(word_model) * np.linalg.norm(good))).astype('float64')\n",
    "    pos2 = np.dot(word_model, excellent) / (np.linalg.norm(word_model) * np.linalg.norm(excellent))\n",
    "    pos3 = np.dot(word_model, correct) / (np.linalg.norm(word_model) * np.linalg.norm(correct))\n",
    "    pos4 = np.dot(word_model, best) / (np.linalg.norm(word_model) * np.linalg.norm(best))\n",
    "    pos5 = np.dot(word_model, happy) / (np.linalg.norm(word_model) * np.linalg.norm(happy))\n",
    "    pos6 = np.dot(word_model, positive) / (np.linalg.norm(word_model) * np.linalg.norm(positive))\n",
    "    pos7 = np.dot(word_model, fortunate) / (np.linalg.norm(word_model) * np.linalg.norm(fortunate))\n",
    "\n",
    "    neg1 = np.dot(word_model, bad) / (np.linalg.norm(word_model) * np.linalg.norm(bad))\n",
    "    neg2 = np.dot(word_model, terrible) / (np.linalg.norm(word_model) * np.linalg.norm(terrible))\n",
    "    neg3 = np.dot(word_model, wrong) / (np.linalg.norm(word_model) * np.linalg.norm(wrong))\n",
    "    neg4 = np.dot(word_model, worst) / (np.linalg.norm(word_model) * np.linalg.norm(worst))\n",
    "    neg5 = np.dot(word_model, disappointed) / (np.linalg.norm(word_model) * np.linalg.norm(disappointed))\n",
    "    neg6 = np.dot(word_model, negative) / (np.linalg.norm(word_model) * np.linalg.norm(negative))\n",
    "    neg7 = np.dot(word_model, unfortunate) / (np.linalg.norm(word_model) * np.linalg.norm(unfortunate))\n",
    "\n",
    "    pos = sum([pos1, pos2, pos3, pos4,  pos5, pos6, pos7])/7\n",
    "    neg = sum([neg1, neg2, neg3, neg4, neg5, neg6, neg7])/7\n",
    "    posneg = pos-neg\n",
    "    result = (word, posneg)\n",
    "    runningTally.append(result)\n",
    "    dictOfWeights[word] = result\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Results\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "'''The 100 most positive signed and most negative signed words'''\n",
    "\n",
    "runningTally = sorted(runningTally, key=itemgetter(1), reverse=True)\n",
    "print(\"Top Positive:\", runningTally[:100])\n",
    "print(\"Top Negative:\", runningTally[len(runningTally)-100:])\n",
    "print(\"Total Vocabulary Size:\", len(vocab))\n",
    "\n",
    "\n",
    "'''word counts for the seed words'''\n",
    "vocab_obj_good = model.wv.vocab[\"good\"]\n",
    "print(\"good\", vocab_obj_good.count)\n",
    "\n",
    "vocab_obj_good = model.wv.vocab[\"excellent\"]\n",
    "print(\"excellent\", vocab_obj_good.count)\n",
    "\n",
    "vocab_obj_good = model.wv.vocab[\"correct\"]\n",
    "print(\"correct\", vocab_obj_good.count)\n",
    "\n",
    "vocab_obj_good = model.wv.vocab[\"best\"]\n",
    "print(\"best\", vocab_obj_good.count)\n",
    "\n",
    "vocab_obj_good = model.wv.vocab[\"happy\"]\n",
    "print(\"happy\", vocab_obj_good.count)\n",
    "\n",
    "\n",
    "vocab_obj_good = model.wv.vocab[\"positive\"]\n",
    "print(\"positive\", vocab_obj_good.count)\n",
    "\n",
    "vocab_obj_good = model.wv.vocab[\"fortunate\"]\n",
    "print(\"fortunate\", vocab_obj_good.count)\n",
    "\n",
    "vocab_obj_good = model.wv.vocab[\"bad\"]\n",
    "print(\"bad\", vocab_obj_good.count)\n",
    "\n",
    "vocab_obj_good = model.wv.vocab[\"terrible\"]\n",
    "print(\"terrible\", vocab_obj_good.count)\n",
    "\n",
    "vocab_obj_good = model.wv.vocab[\"wrong\"]\n",
    "print(\"wrong\", vocab_obj_good.count)\n",
    "\n",
    "vocab_obj_good = model.wv.vocab[\"worst\"]\n",
    "print(\"worst\", vocab_obj_good.count)\n",
    "\n",
    "vocab_obj_good = model.wv.vocab[\"disappointed\"]\n",
    "print(\"disappointed\", vocab_obj_good.count)\n",
    "\n",
    "vocab_obj_good = model.wv.vocab[\"negative\"]\n",
    "print(\"negative\", vocab_obj_good.count)\n",
    "\n",
    "vocab_obj_good = model.wv.vocab[\"unfortunate\"]\n",
    "print(\"unfortunate\", vocab_obj_good.count)\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Apply Lexicon\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "'''Apply the Lexicon to score the transcripts of the video clips.'''\n",
    "\n",
    "#remove stopwords.  They are not relevant to sentiment scoring.\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def sentence_to_wordlist(sentence, remove_stopwords=False):\n",
    "    sentence_text = re.sub(r'[^\\w\\s]',' ', sentence)\n",
    "    words = sentence_text.lower().split()\n",
    "\n",
    "    for word in words: #Remove Stopwords (Cochrane)\n",
    "        if word in stopwords:\n",
    "            words.remove(word)\n",
    "\n",
    "    return words\n",
    "\n",
    "#Import Data re: transcripts of video snippets\n",
    "\n",
    "gitHub = 'hansardExtractedVideoTranscripts.csv'\n",
    "hansardVideos = pd.read_csv(gitHub, encoding='utf-8')\n",
    "\n",
    "df = pd.DataFrame(columns=['Label', 'Date' 'ID_main', 'youTube',\n",
    "                  'timeStamp', 'Speaker', 'French', 'Party',\n",
    "                  'Seconds', 'English', 'Floor', \n",
    "                  'Sentiment', 'sentencePolarity',\n",
    "                  'wordPolaritySummed', 'sentencePolaritySTD',\n",
    "                  'countedWords'])\n",
    "\n",
    "\n",
    "labelList = []\n",
    "dateList = []\n",
    "IDmainList = []\n",
    "youTubeList = []\n",
    "timeStampList = []\n",
    "speakerList = []\n",
    "frenchList = []\n",
    "partyList = []\n",
    "secondsList = []\n",
    "englishList = []\n",
    "floorList = []\n",
    "sentenceList = []\n",
    "sentimentList = []\n",
    "wordPolaritySummedList = []\n",
    "sentencePolarityList = []\n",
    "countedWordsList = []\n",
    "    \n",
    "\n",
    "'''A loop for cycling through the list of rows in the hansardVideo\n",
    "Transcripts.'''\n",
    "    \n",
    "i = 0\n",
    "for x in range(0, len(hansardVideos)):\n",
    "    labelList.append(hansardVideos[\"Label\"][i])\n",
    "    dateList.append(hansardVideos[\"Date\"][i])\n",
    "    IDmainList.append(hansardVideos[\"ID_main\"][i])\n",
    "    youTubeList.append(hansardVideos[\"youTube\"][i])\n",
    "    timeStampList.append(hansardVideos[\"timeStamp\"][i])\n",
    "    speakerList.append(hansardVideos[\"Speaker\"][i])\n",
    "    frenchList.append(hansardVideos[\"French\"][i])\n",
    "    partyList.append(hansardVideos[\"party\"][i])\n",
    "    secondsList.append(hansardVideos[\"seconds\"][i])\n",
    "    englishList.append(hansardVideos[\"english\"][i])\n",
    "    floorList.append(hansardVideos[\"floor\"][i])\n",
    "\n",
    "    sentenceList.append(hansardVideos[\"english\"][i])\n",
    "    sentence = hansardVideos[\"english\"][i]\n",
    "    '''break the sentence into words'''\n",
    "    sentence_words = sentence_to_wordlist(sentence)\n",
    "    '''initialize variables'''\n",
    "    sentiment = 0 #positivity minus negativity\n",
    "    sentencePolarity = 0 #absolute values of positivity minus negativity\n",
    "    wordPolaritySummed = 0 #sum of absolute value of word polarities\n",
    "    countedWords = 0\n",
    "    \n",
    "    \n",
    "    '''for every word in the sentence, subtract the sum of its cosine \n",
    "    similarity to the negative seed words from the sum of its cosine\n",
    "    similarity to the positive seed words, and then sum the difference\n",
    "    across all words in the sentence.'''\n",
    "    \n",
    "    for word in sentence_words:\n",
    "\n",
    "        try:\n",
    "\n",
    "            word_model = model[word].astype('float64')\n",
    "\n",
    "            pos1 = np.dot(word_model, good).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(good))\n",
    "            pos2 = np.dot(word_model, excellent).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(excellent).astype('float64'))\n",
    "            pos3 = np.dot(word_model, correct).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(correct).astype('float64'))\n",
    "            pos4 = np.dot(word_model, best).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(best).astype('float64'))\n",
    "            pos5 = np.dot(word_model, happy).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(happy).astype('float64'))\n",
    "            pos6 = np.dot(word_model, positive).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(positive).astype('float64'))\n",
    "            pos7 = np.dot(word_model, fortunate).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(fortunate).astype('float64'))\n",
    "\n",
    "            neg1 = np.dot(word_model, bad).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(bad).astype('float64'))\n",
    "            neg2 = np.dot(word_model, terrible).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(terrible).astype('float64'))\n",
    "            neg3 = np.dot(word_model, wrong).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(wrong).astype('float64'))\n",
    "            neg4 = np.dot(word_model, worst).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(worst).astype('float64'))\n",
    "            neg5 = np.dot(word_model, disappointed).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(disappointed).astype('float64'))\n",
    "            neg6 = np.dot(word_model, negative).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(negative).astype('float64'))\n",
    "            neg7 = np.dot(word_model, unfortunate).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(unfortunate).astype('float64'))\n",
    "\n",
    "            pos = sum([pos1, pos2, pos3, pos4, pos5, pos6, pos7]) / 7\n",
    "            neg = sum([neg1, neg2, neg3, neg4, neg5, neg6, neg7]) / 7\n",
    "            posneg = pos - neg\n",
    "            sentiment += posneg\n",
    "            wordPolaritySummed += abs(posneg)\n",
    "            countedWords +=1\n",
    "            \n",
    "\n",
    "        except:\n",
    "            \n",
    "            #handful of garbage words -- did not meet minimum word threschold\n",
    "            print(\"Warning! Word: \", word, \" from speech: \", i, \" not in w2v model!\")\n",
    "\n",
    "            continue\n",
    "\n",
    "    sentimentList.append(sentiment)\n",
    "    wordPolaritySummedList.append(wordPolaritySummed)   \n",
    "    sentencePolarityList.append(abs(sentiment))\n",
    "    countedWordsList.append(countedWords)\n",
    "\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "\n",
    "'''output to csv vile'''\n",
    "    \n",
    "w2vScores = pd.DataFrame({'label': labelList, \n",
    "                               'date': dateList, \n",
    "                               'IDMain': IDmainList, \n",
    "                               'youTube': youTubeList, \n",
    "                               'timeStamp': timeStampList, \n",
    "                               'speaker': speakerList,\n",
    "                               'french': frenchList, \n",
    "                               'party': partyList,\n",
    "                               'seconds': secondsList,\n",
    "                               'english': englishList,\n",
    "                               'floor': floorList,\n",
    "                               'sentiment': sentimentList,\n",
    "                               'sentencePolarity': sentencePolarityList,\n",
    "                               'wordPolaritySummed': wordPolaritySummedList,\n",
    "                               'countedWords': countedWordsList})\n",
    "                               \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "w2vScores.to_csv(\"w2vScores.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IX. Integrate Other Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX. A. Apply Leading Sentiment Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use applySentimentR_ccEd.R in R-Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX. B. Apply Additional Sentiment Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script trains Support Vector Machines and FastText on a training subset of the IMDB movie review (https://www.kaggle.com/utathya/imdb-review-dataset) and Stanford handcoded tweet (https://snap.stanford.edu/data/twitter7.html) databases, and applies these models to a testing subset of each corpora. The script applies the models to classify the sentiment of the Hansard transcripts of the video snippets (from Step 2). The script also scores the sentiment of these snippets using the Valence Aware Dictionary and Sentiment Reasoner (VADER - https://github.com/cjhutto/vaderSentiment) and LIWC (https://liwc.wpengine.com/) sentiment dictionaries. The training data and models are available at https://www.dropbox.com/sh/u91njzwcuvdu8oa/AABkl2vUJRUNEq4WEGSCBql_a?dl=0 (~1.5GB combined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jan 10 06:06:37 2019\n",
    "@author: ludovic rheault with cochrane \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import fastText \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn import metrics\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# Library used to preprocess the texts (already done)\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def process(text):\n",
    "    doc = nlp(text)\n",
    "    text = ' '.join([w.lemma_ for w in doc if not w.is_punct and not w.is_stop])\n",
    "    text = text.replace('-PRON-', '') #remocing Spacy lemma convention for pronouns\n",
    "    return text\n",
    "\n",
    "\n",
    "#=======================================================#\n",
    "# Fitting the SVM model\n",
    "# Imdb dataset\n",
    "#=======================================================#\n",
    "\n",
    "# To build tfidf matrix\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
    "\n",
    "# Fitting model on the training data\n",
    "with open('training_imdb.csv') as f:\n",
    "    training = f.read().splitlines()\n",
    "training=[(0, t.replace('__label__Negative ','')) if t.startswith('__label__Negative') else (1, t.replace('__label__Positive ','')) for t in training]\n",
    "y = [l for l,_ in training]\n",
    "X = [t for _,t in training]\n",
    "X = vectorizer.fit_transform(X)\n",
    "kbest = SelectKBest(chi2, k=2000)\n",
    "X = kbest.fit_transform(X, y)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Evaluating the model\n",
    "with open('testing_imdb.csv') as f:\n",
    "    testing = f.read().splitlines()\n",
    "testing=[(0, t.replace('__label__Negative ','')) if t.startswith('__label__Negative') else (1, t.replace('__label__Positive ','')) for t in testing]\n",
    "y_test = [l for l,_ in testing]\n",
    "X_test = [t for _,t in testing]\n",
    "X_test = vectorizer.transform(X_test)\n",
    "X_test = kbest.transform(X_test)\n",
    "\n",
    "yhat = clf.predict(X_test)\n",
    "acc = metrics.accuracy_score(y_test, yhat)\n",
    "f1 = metrics.f1_score(y_test, yhat)\n",
    "precision = metrics.precision_score(y_test, yhat)\n",
    "recall = metrics.recall_score(y_test, yhat)\n",
    "print(acc)\n",
    "print(f1)\n",
    "print(precision)\n",
    "print(recall)\n",
    "\n",
    "# Saving model\n",
    "with open('svm_imdb.pkl', 'wb') as fout:\n",
    "    pickle.dump((vectorizer, kbest, clf), fout)\n",
    "\n",
    "#------------------------------------------\n",
    "# Predicting sentiment of Hansard sentences\n",
    "#------------------------------------------\n",
    "    \n",
    "# Read Data    \n",
    "\n",
    "gitHub = 'hansardExtractedVideoTranscripts.csv'\n",
    "df = pd.read_table(gitHub, sep=',', header=0)\n",
    "\n",
    "\n",
    "# Applying Processing\n",
    "preProcessed = []\n",
    "for item in df['english']:\n",
    "    preProcessed.append(process(item))\n",
    "\n",
    "df['preprocessed'] = preProcessed\n",
    "\n",
    "\n",
    "\n",
    "texts = df.preprocessed.tolist()\n",
    "texts = vectorizer.transform(texts)\n",
    "texts = kbest.transform(texts)\n",
    "df['svm_imdb'] = clf.predict(texts)\n",
    "df['svm_imdb'] = df.svm_imdb.replace({0:'Negative', 1:'Positive'})\n",
    "\n",
    "#=======================================================#\n",
    "# Predictions from fastText model\n",
    "#=======================================================#\n",
    "\n",
    "# Loading model\n",
    "fasttext_model = fastText.load_model('fasttext_imdb.bin') \n",
    "\n",
    "\n",
    "# Adding predictions from fastText\n",
    "texts = df.preprocessed.tolist() #de-vectorizing texts (see above)\n",
    "\n",
    "fasttext_imdb = []\n",
    "fasttext_imdbWeights = []\n",
    "\n",
    "for p in texts:\n",
    "    prediction = fasttext_model.predict(p)\n",
    "    weight= prediction[1]\n",
    "    verdict=str(prediction[0])   \n",
    "    if '__label__Negative' in verdict:\n",
    "        verdict = 'Negative'\n",
    "        weight = weight*-1\n",
    "    elif 'Positive' in verdict:\n",
    "        verdict = 'Positive'\n",
    "    else:\n",
    "        print(\"Missing Estimate!!: \", p)\n",
    "    fasttext_imdb.append(verdict)\n",
    "    weight = str(weight)\n",
    "    weight = weight.replace(\"[\",\"\")\n",
    "    weight = weight.replace(\"]\",\"\")\n",
    "    fasttext_imdbWeights.append(weight)\n",
    "    \n",
    "\n",
    "df['fasttext_imbd'] = list(item for item in fasttext_imdb)\n",
    "df['fasttext_imbdWeight'] = list(item for item in fasttext_imdbWeights)\n",
    "  \n",
    "# Saving Hansard file with predictions\n",
    "df.to_csv('hansardExtractedVideoTranscripts_SVMFastText.csv', index=False, sep=',')\n",
    "\n",
    "\n",
    "# Evaluating fastText model accuracy\n",
    "\n",
    "with open('testing_imdb.csv') as f:\n",
    "    IMDBclassified = f.read().splitlines() #read classified imdb reviews\n",
    "    \n",
    "\n",
    "IMDBunclassified = []\n",
    "IMDBclassifiedScores = [] #list holders\n",
    "\n",
    "\n",
    "for n, i in enumerate(IMDBclassified): #remove classification labels form imdb \n",
    "    if i.startswith(\"__label__Negative \"):\n",
    "        #remove known classification for unclassified\n",
    "        IMDBunclassified.append(IMDBclassified[n].replace('__label__Negative ', ''))\n",
    "        IMDBclassifiedScores.append(0)\n",
    "    elif i.startswith(\"__label__Positive \"):\n",
    "        IMDBunclassified.append(IMDBclassified[n].replace('__label__Positive ', ''))\n",
    "        IMDBclassifiedScores.append(1)\n",
    "    else:\n",
    "        print(\"Flag! Line: \", n, \" with text \", i, \" Unclassified in IMDB Corpus\")\n",
    "\n",
    "\n",
    "IMDBFastTextScore = []\n",
    "for review in IMDBunclassified:\n",
    "    check = list(fasttext_model.predict(review)[0])   \n",
    "    if check[0] == '__label__Positive':\n",
    "        IMDBFastTextScore.append(1)\n",
    "    elif check[0] == '__label__Negative':\n",
    "        IMDBFastTextScore.append(0)\n",
    "    else:\n",
    "        print(\"Warning! No FastText Generated for: \", review)\n",
    "    \n",
    "\n",
    "acc = metrics.accuracy_score(IMDBclassifiedScores, IMDBFastTextScore)\n",
    "f1 = metrics.f1_score(IMDBclassifiedScores, IMDBFastTextScore)\n",
    "precision = metrics.precision_score(IMDBclassifiedScores, IMDBFastTextScore)\n",
    "recall = metrics.recall_score(IMDBclassifiedScores, IMDBFastTextScore)\n",
    "\n",
    "print(\"Fast Text IMDB Accuracy\")\n",
    "print(acc)\n",
    "print(f1)\n",
    "print(precision)\n",
    "print(recall)\n",
    "\n",
    "#=======================================================#\n",
    "# Fitting the SVM model\n",
    "# Stanford dataset\n",
    "#=======================================================#\n",
    "\n",
    "# To build tfidf matrix\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
    "\n",
    "# Fitting model on the training data\n",
    "with open('training_stanford.csv') as f:\n",
    "    training = f.read().splitlines()\n",
    "training=[(0, t.replace('__label__Negative ','')) if t.startswith('__label__Negative') else (1, t.replace('__label__Positive ','')) for t in training]\n",
    "y = [l for l,_ in training]\n",
    "X = [t for _,t in training]\n",
    "X = vectorizer.fit_transform(X)\n",
    "kbest = SelectKBest(chi2, k=2000)\n",
    "X = kbest.fit_transform(X, y)\n",
    "\n",
    "clf = LinearSVC(C=100)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Evaluating the model\n",
    "with open('testing_stanford.csv') as f:\n",
    "    testing = f.read().splitlines()\n",
    "testing=[(0, t.replace('__label__Negative ','')) if t.startswith('__label__Negative') else (1, t.replace('__label__Positive ','')) for t in testing]\n",
    "y_test = [l for l,_ in testing]\n",
    "X_test = [t for _,t in testing]\n",
    "X_test = vectorizer.transform(X_test)\n",
    "X_test = kbest.transform(X_test)\n",
    "\n",
    "yhat = clf.predict(X_test)\n",
    "acc = metrics.accuracy_score(y_test, yhat)\n",
    "f1 = metrics.f1_score(y_test, yhat)\n",
    "precision = metrics.precision_score(y_test, yhat)\n",
    "recall = metrics.recall_score(y_test, yhat)\n",
    "print(\"IMDB Estimates Model Accuracy\")\n",
    "print(acc)\n",
    "print(f1)\n",
    "print(precision)\n",
    "print(recall)\n",
    "\n",
    "# Saving model\n",
    "with open('svm_stanford.pkl', 'wb') as fout:\n",
    "    pickle.dump((vectorizer, kbest, clf), fout)\n",
    "\n",
    "# Predicting sentiment of Hansard sentences\n",
    "texts = df.preprocessed.tolist()\n",
    "texts = vectorizer.transform(texts)\n",
    "texts = kbest.transform(texts)\n",
    "df['svm_stanford'] = clf.predict(texts)\n",
    "df['svm_stanford'] = df.svm_imdb.replace({0:'Negative', 1:'Positive'})\n",
    "\n",
    "#=======================================================#\n",
    "# Predictions from fastText model\n",
    "#=======================================================#\n",
    "\n",
    "# Loading model\n",
    "fasttext_model = fastText.load_model('fasttext_stanford.bin') #Cochrane edited\n",
    "\n",
    "# Adding predictions from fastText\n",
    "texts = df.preprocessed.tolist() #de-vectorizing texts (see above)\n",
    "\n",
    "fasttext_stanford = []\n",
    "fasttext_stanfordWeights = []\n",
    "for p in texts:\n",
    "    prediction = fasttext_model.predict(p)\n",
    "    weight = prediction[1]\n",
    "    verdict=str(prediction[0]) \n",
    "    if '__label__Negative' in verdict:\n",
    "        verdict = 'Negative' \n",
    "        weight = weight*-1\n",
    "    elif '__label__Positive' in verdict:\n",
    "        verdict = 'Positive'\n",
    "    else:\n",
    "        print('Missing Estimate!!: ', p)\n",
    "    weight = str(weight)\n",
    "    weight = weight.replace(\"[\",\"\")\n",
    "    weight = weight.replace(\"]\",\"\")\n",
    "    fasttext_stanford.append(verdict)\n",
    "    fasttext_stanfordWeights.append(weight)\n",
    "    \n",
    "\n",
    "df['fasttext_stanford'] = list(item for item in fasttext_stanford)\n",
    "df['fasttext_stanfordWeights'] = list(item for item in fasttext_stanfordWeights)\n",
    "  \n",
    "\n",
    "\n",
    "# Evaluating fastText model accuracy\n",
    "\n",
    "with open('testing_stanford.csv') as f:\n",
    "    StanfordClassified = f.read().splitlines() #read classified Stanford reviews\n",
    "    \n",
    "\n",
    "StanfordUnclassified = []\n",
    "StanfordClassifiedScores = [] \n",
    "newStanfordClassified = [] # will exclude handful of unclassified tweets in Stanford corpus\n",
    "\n",
    "for n, i in enumerate(StanfordClassified): #remove classification labels form imdb \n",
    "    if i.startswith(\"__label__Negative \"):\n",
    "        #remove known classification from unclassified\n",
    "        StanfordUnclassified.append(StanfordClassified[n].replace('__label__Negative ', ''))\n",
    "        StanfordClassifiedScores.append(0)\n",
    "        newStanfordClassified.append(StanfordClassified[n])\n",
    "    elif i.startswith(\"__label__Positive \"):\n",
    "        StanfordUnclassified.append(StanfordClassified[n].replace('__label__Positive ', ''))\n",
    "        StanfordClassifiedScores.append(1)\n",
    "        newStanfordClassified.append(StanfordClassified[n])\n",
    "    else:\n",
    "        print(\"Flag! Line: \", n, \" with text \", i, \" Unclassified in Stanford Corpus. Dropping\")\n",
    "\n",
    "\n",
    "StanfordFastTextScore = []\n",
    "for review in StanfordUnclassified:\n",
    "    check = list(fasttext_model.predict(review)[0])   \n",
    "    if check[0] == '__label__Positive':\n",
    "        StanfordFastTextScore.append(1)\n",
    "    elif check[0] == '__label__Negative':\n",
    "        StanfordFastTextScore.append(0)\n",
    "    else:\n",
    "        print(\"Warning! No FastText Generated for: \", review)\n",
    "    \n",
    "\n",
    "acc = metrics.accuracy_score(StanfordClassifiedScores, StanfordFastTextScore)\n",
    "f1 = metrics.f1_score(StanfordClassifiedScores, StanfordFastTextScore)\n",
    "precision = metrics.precision_score(StanfordClassifiedScores, StanfordFastTextScore)\n",
    "recall = metrics.recall_score(StanfordClassifiedScores, StanfordFastTextScore)\n",
    "print(\"Stanford Estimates Model Accuracy\")\n",
    "print(acc)\n",
    "print(f1)\n",
    "print(precision)\n",
    "print(recall)\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "#Apply Vader and LIWC Dictionaries\n",
    "#######################################################\n",
    "\n",
    "\n",
    "#The popular VADER library for Python, which performs valence shifting for negation words.\n",
    "# More adapted to social media.\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Negation words that could be used to account for valence shifting.\n",
    "negation = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"no\", \"nothing\", \"nowhere\",\n",
    "     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    "     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
    "\n",
    "# To compute scores from dictionaries with wild card symbols for inflexions. \n",
    "def sentiment_scores(text, lexicon):\n",
    "    text = text.lower()\n",
    "    count = 0\n",
    "    for word in lexicon:\n",
    "        if word.endswith('*'):\n",
    "            count += len([t for t in text if t.startswith(word[:-1])])\n",
    "        else:\n",
    "            count += text.count(word) \n",
    "    score = (count)/len(text)\n",
    "    return score\n",
    "\n",
    "# Rescaling variable.\n",
    "def rescale(x, newmin, newmax, oldmin=None, oldmax=None):\n",
    "    if not oldmin:\n",
    "        oldmin = min(x)\n",
    "    if not oldmax:\n",
    "        oldmax = max(x)\n",
    "    return (((x - oldmin) * (newmax - newmin)) / (oldmax - oldmin)) + newmin\n",
    "\n",
    "\n",
    "# The LIWC positive and negative emotion dictionaries.\n",
    "# Note: these are a part of proprietary software that we don't have permission\n",
    "# to share. To reproduce our analysis here, visit http://liwc.wpengine.com/\n",
    "with open('liwc2015_positive.txt') as f:\n",
    "    liwc_pos = f.read().splitlines()\n",
    "with open('liwc2015_negative.txt') as f:\n",
    "    liwc_neg = f.read().splitlines()\n",
    "\n",
    "# Adding a LIWC sentiment score, and the VADER compound score.\n",
    "df['liwc'] = df.preprocessed.apply(lambda x: sentiment_scores(x, liwc_pos) - sentiment_scores(x, liwc_neg))\n",
    "df['vader'] = df.preprocessed.apply(lambda x: vader.polarity_scores(x.lower())['compound'])\n",
    "\n",
    "# SVM model fitted with Platt probabilities on the IMDb dataset.\n",
    "with open('svm_imdb_probs.pkl', 'rb') as f:\n",
    "    vectorizer, kbest, svm = pickle.load(f)\n",
    "\n",
    "# Predicting Platt probabilities on Hansard texts.\n",
    "texts = df.preprocessed.tolist()\n",
    "texts = vectorizer.transform(texts)\n",
    "texts = kbest.transform(texts)\n",
    "df['svm_imdb_probability'] = svm.predict_proba(texts)[:,1]\n",
    "\n",
    "# Rescaling lexicons between 0 and 10.\n",
    "df['liwc'] = rescale(df.liwc, 0, 10)\n",
    "# Vader was already rescaled between -1 and 1.\n",
    "df['vader'] = rescale(df.vader, newmin=0, newmax=10, oldmin=-1, oldmax=1)\n",
    "\n",
    "\n",
    "# Saving Hansard file with predictions\n",
    "\n",
    "#removing '-' in preprocessed, which excel reads as minus when first character\n",
    "\n",
    "\n",
    "df.to_csv('hansardExtractedVideoTranscripts_SVMFastTextVader.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X. Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding = pd.read_csv('fullCodingDataVideoOnePer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding = coding.rename(columns={'Video': 'ID_main'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding['sent_textCoders'] = coding[['t1SentAvg', 't2SentAvg', 't3SentAvg']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Date</th>\n",
       "      <th>French</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>ID_main</th>\n",
       "      <th>english</th>\n",
       "      <th>floor</th>\n",
       "      <th>party</th>\n",
       "      <th>seconds</th>\n",
       "      <th>timeStamp</th>\n",
       "      <th>...</th>\n",
       "      <th>v2Act2</th>\n",
       "      <th>v2SentAvg</th>\n",
       "      <th>v2ActAvg</th>\n",
       "      <th>v3Sent1</th>\n",
       "      <th>v3Act1</th>\n",
       "      <th>v3Sent2</th>\n",
       "      <th>v3Act2</th>\n",
       "      <th>v3SentAvg</th>\n",
       "      <th>v3ActAvg</th>\n",
       "      <th>sent_textCoders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>1</td>\n",
       "      <td>Alexandre Boulerice</td>\n",
       "      <td>2017 12 13 0</td>\n",
       "      <td>I thought we usually hired an investigator to ...</td>\n",
       "      <td>Moi, je pensais qu'on embauchait d'habitude un...</td>\n",
       "      <td>NDP</td>\n",
       "      <td>6.45</td>\n",
       "      <td>9M 12S</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label        Date  French              Speaker       ID_main  \\\n",
       "0      1  2017-12-13       1  Alexandre Boulerice  2017 12 13 0   \n",
       "\n",
       "                                             english  \\\n",
       "0  I thought we usually hired an investigator to ...   \n",
       "\n",
       "                                               floor party seconds timeStamp  \\\n",
       "0  Moi, je pensais qu'on embauchait d'habitude un...   NDP    6.45    9M 12S   \n",
       "\n",
       "        ...        v2Act2  v2SentAvg  v2ActAvg  v3Sent1  v3Act1  v3Sent2  \\\n",
       "0       ...           8.0        2.0       8.0      3.0     7.0      2.0   \n",
       "\n",
       "   v3Act2  v3SentAvg  v3ActAvg  sent_textCoders  \n",
       "0     8.0        2.5       7.5         2.333333  \n",
       "\n",
       "[1 rows x 48 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coding.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "rSentiment = pd.read_csv('hansardVideoSentiment_lsdweightCCed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "rSentiment = rSentiment.rename(columns={'IDMain': 'ID_main',\n",
    "                                     'LexiTextScore': 'sent_lexicoder',\n",
    "                                     'LexiTextContextScore': 'sent_lexicoderContext',\n",
    "                                     'sentiment_jockers_rinker': 'sent_jockersRinker',\n",
    "                                     'sentiment_huliu': 'sent_huLiu',\n",
    "                                     'sentiment_sentiwordnet': 'sent_sentiwordnet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Label</th>\n",
       "      <th>Date_text</th>\n",
       "      <th>Date</th>\n",
       "      <th>num</th>\n",
       "      <th>ID_main</th>\n",
       "      <th>youTube</th>\n",
       "      <th>timeStamp</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>French</th>\n",
       "      <th>...</th>\n",
       "      <th>X.6</th>\n",
       "      <th>X.7</th>\n",
       "      <th>lexiText</th>\n",
       "      <th>lexiTextContext</th>\n",
       "      <th>sent_lexicoder</th>\n",
       "      <th>sent_lexicoderContext</th>\n",
       "      <th>LexiTextScoreRaw</th>\n",
       "      <th>sent_jockersRinker</th>\n",
       "      <th>sent_sentiwordnet</th>\n",
       "      <th>sent_huLiu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017 12 13</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>0</td>\n",
       "      <td>2017 12 13 0</td>\n",
       "      <td>https://youtu.be/6p2IWa2rfO4</td>\n",
       "      <td>9M 12S</td>\n",
       "      <td>Alexandre Boulerice</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I thought we usually hired an investigator to ...</td>\n",
       "      <td>Mr Speaker the new Conflict of Interest and Et...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.136865</td>\n",
       "      <td>-0.105193</td>\n",
       "      <td>-0.218384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Label   Date_text        Date  num       ID_main  \\\n",
       "0           1      1  2017 12 13  2017-12-13    0  2017 12 13 0   \n",
       "\n",
       "                        youTube timeStamp              Speaker  French  \\\n",
       "0  https://youtu.be/6p2IWa2rfO4    9M 12S  Alexandre Boulerice       1   \n",
       "\n",
       "      ...     X.6 X.7                                           lexiText  \\\n",
       "0     ...     NaN NaN  I thought we usually hired an investigator to ...   \n",
       "\n",
       "                                     lexiTextContext sent_lexicoder  \\\n",
       "0  Mr Speaker the new Conflict of Interest and Et...           -1.0   \n",
       "\n",
       "  sent_lexicoderContext LexiTextScoreRaw sent_jockersRinker sent_sentiwordnet  \\\n",
       "0             -0.272727             -2.0          -0.136865         -0.105193   \n",
       "\n",
       "   sent_huLiu  \n",
       "0   -0.218384  \n",
       "\n",
       "[1 rows x 35 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rSentiment.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vScores = pd.read_csv('w2vScores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vScores = w2vScores.rename(columns={'IDMain': 'ID_main',\n",
    "                                     'sentiment': 'sent_w2v'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID_main</th>\n",
       "      <th>countedWords</th>\n",
       "      <th>date</th>\n",
       "      <th>english</th>\n",
       "      <th>floor</th>\n",
       "      <th>french</th>\n",
       "      <th>label</th>\n",
       "      <th>party</th>\n",
       "      <th>seconds</th>\n",
       "      <th>sentencePolarity</th>\n",
       "      <th>sent_w2v</th>\n",
       "      <th>speaker</th>\n",
       "      <th>timeStamp</th>\n",
       "      <th>wordPolaritySummed</th>\n",
       "      <th>youTube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2017 12 13 0</td>\n",
       "      <td>13</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>I thought we usually hired an investigator to ...</td>\n",
       "      <td>Moi, je pensais qu'on embauchait d'habitude un...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NDP</td>\n",
       "      <td>6.45</td>\n",
       "      <td>0.417042</td>\n",
       "      <td>-0.417042</td>\n",
       "      <td>Alexandre Boulerice</td>\n",
       "      <td>9M 12S</td>\n",
       "      <td>0.653231</td>\n",
       "      <td>https://youtu.be/6p2IWa2rfO4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       ID_main  countedWords        date  \\\n",
       "0           0  2017 12 13 0            13  2017-12-13   \n",
       "\n",
       "                                             english  \\\n",
       "0  I thought we usually hired an investigator to ...   \n",
       "\n",
       "                                               floor  french  label party  \\\n",
       "0  Moi, je pensais qu'on embauchait d'habitude un...       1      1   NDP   \n",
       "\n",
       "  seconds  sentencePolarity  sent_w2v              speaker timeStamp  \\\n",
       "0    6.45          0.417042 -0.417042  Alexandre Boulerice    9M 12S   \n",
       "\n",
       "   wordPolaritySummed                       youTube  \n",
       "0            0.653231  https://youtu.be/6p2IWa2rfO4  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vScores.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMetc = pd.read_csv('hansardExtractedVideoTranscripts_SVMFastTextVader.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMetc = SVMetc.rename(columns={'vader': 'sent_vader',\n",
    "                               'svm_imdb_probability': 'sent_svmIMDBPlattProbs',\n",
    "                               'fasttext_stanford': 'sent_fastTextStanfordClass',\n",
    "                               'svm_imdb': 'sent_svmIMDBClass',\n",
    "                               'svm_stanford': 'sent_svmStanfordClass'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMetc['sent_svmStanfordClass'] = SVMetc['sent_svmStanfordClass'].map({'Negative': -1, 'Positive':1})\n",
    "SVMetc['sent_svmIMDBClass'] = SVMetc['sent_svmIMDBClass'].map({'Negative': -1, 'Positive':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Date</th>\n",
       "      <th>ID_main</th>\n",
       "      <th>youTube</th>\n",
       "      <th>timeStamp</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>French</th>\n",
       "      <th>party</th>\n",
       "      <th>seconds</th>\n",
       "      <th>english</th>\n",
       "      <th>...</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>sent_svmIMDBClass</th>\n",
       "      <th>fasttext_imbd</th>\n",
       "      <th>fasttext_imbdWeight</th>\n",
       "      <th>sent_svmStanfordClass</th>\n",
       "      <th>sent_fastTextStanfordClass</th>\n",
       "      <th>fasttext_stanfordWeights</th>\n",
       "      <th>liwc</th>\n",
       "      <th>sent_vader</th>\n",
       "      <th>sent_svmIMDBPlattProbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>2017 12 13 0</td>\n",
       "      <td>https://youtu.be/6p2IWa2rfO4</td>\n",
       "      <td>9M 12S</td>\n",
       "      <td>Alexandre Boulerice</td>\n",
       "      <td>1</td>\n",
       "      <td>NDP</td>\n",
       "      <td>6.45</td>\n",
       "      <td>I thought we usually hired an investigator to ...</td>\n",
       "      <td>...</td>\n",
       "      <td>think usually hire investigator crime cover o...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-0.503811</td>\n",
       "      <td>-1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.714237</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>1.408</td>\n",
       "      <td>0.473646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label        Date       ID_main                       youTube timeStamp  \\\n",
       "0      1  2017-12-13  2017 12 13 0  https://youtu.be/6p2IWa2rfO4    9M 12S   \n",
       "\n",
       "               Speaker  French party seconds  \\\n",
       "0  Alexandre Boulerice       1   NDP    6.45   \n",
       "\n",
       "                                             english           ...            \\\n",
       "0  I thought we usually hired an investigator to ...           ...             \n",
       "\n",
       "                                        preprocessed sent_svmIMDBClass  \\\n",
       "0   think usually hire investigator crime cover o...                -1   \n",
       "\n",
       "   fasttext_imbd fasttext_imbdWeight  sent_svmStanfordClass  \\\n",
       "0       Negative           -0.503811                     -1   \n",
       "\n",
       "   sent_fastTextStanfordClass fasttext_stanfordWeights      liwc  sent_vader  \\\n",
       "0                    Positive                 0.714237  3.181818       1.408   \n",
       "\n",
       "   sent_svmIMDBPlattProbs  \n",
       "0                0.473646  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVMetc.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = pd.merge(coding, rSentiment,\n",
    "                               how = 'left',                         \n",
    "                               left_on='ID_main',\n",
    "                               right_on='ID_main')\n",
    "\n",
    "temp2 = pd.merge(temp1, w2vScores, how='left', on='ID_main')\n",
    "\n",
    "sentimentScores = pd.merge(temp2, SVMetc, how='left', on='ID_main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(sentimentScores.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentScores = sentimentScores.rename(columns={\n",
    "                               'French_x': 'French',\n",
    "                               'english_x': 'english',\n",
    "                               'party_x': 'party',\n",
    "                                'seconds_x': 'seconds',\n",
    "                                'youTube_x': 'youTube',\n",
    "                                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentScores = sentimentScores.rename(columns={\n",
    "                                't1Act1': 'act_txt1_1',\n",
    "                                't1Act2': 'act_txt1_2',\n",
    "                                't1ActAvg': 'act_txt1_avg',\n",
    "                                't2Act1': 'act_txt2_1',\n",
    "                                't2Act2': 'act_txt2_2',\n",
    "                                't2ActAvg': 'act_txt2_avg',\n",
    "                                't3Act1': 'act_txt3_1',\n",
    "                                't3Act2': 'act_txt3_2',\n",
    "                                't3ActAvg': 'act_txt3_avg',\n",
    "                                'v1Act1': 'act_vid1_1',\n",
    "                                'v1Act2': 'act_vid1_2',\n",
    "                                'v1ActAvg': 'act_vid1_avg',\n",
    "                                'v2Act1': 'act_vid2_1',\n",
    "                                'v2Act2': 'act_vid2_2',\n",
    "                                'v2ActAvg': 'act_vid2_avg',\n",
    "                                'v3Act1': 'act_vid3_1',\n",
    "                                'v3Act2': 'act_vid3_2',\n",
    "                                'v3ActAvg': 'act_vid3_avg',\n",
    "                                't1Sent1': 'sent_txt1_1',\n",
    "                                't1Sent2': 'sent_txt1_2',\n",
    "                                't1SentAvg': 'sent_txt1_avg',\n",
    "                                't2Sent1': 'sent_txt2_1',\n",
    "                                't2Sent2': 'sent_txt2_2',\n",
    "                                't2SentAvg': 'sent_txt2_avg',\n",
    "                                't3Sent1': 'sent_txt3_1',\n",
    "                                't3Sent2': 'sent_txt3_2',\n",
    "                                't3SentAvg': 'sent_txt3_avg',\n",
    "                                'v1Sent1': 'sent_vid1_1',\n",
    "                                'v1Sent2': 'sent_vid1_2',\n",
    "                                'v1SentAvg': 'sent_vid1_avg',\n",
    "                                'v2Sent1': 'sent_vid2_1',\n",
    "                                'v2Sent2': 'sent_vid2_2',\n",
    "                                'v2SentAvg': 'sent_vid2_avg',\n",
    "                                'v3Sent1': 'sent_vid3_1',\n",
    "                                'v3Sent2': 'sent_vid3_2',\n",
    "                                'v3SentAvg': 'sent_vid3_avg',\n",
    "                                'textSent': 'sent_textCoders',\n",
    "                                'videoSent': 'sent_videoCoders',\n",
    "                                'textAct': 'act_textCoders',\n",
    "                                'videoAct': 'act_videoCoders',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentScores = sentimentScores.drop(['Unnamed: 0_x',\n",
    "                                        'Unnamed: 0.1',\n",
    "                                        'Label_x',\n",
    "                                        'Date_x',\n",
    "                                        'youTube_x',\n",
    "                                        'timeStamp_x',\n",
    "                                        'Speaker_x',\n",
    "                                        'French_x',\n",
    "                                        'party_x',\n",
    "                                        'seconds_x',\n",
    "                                        'english_x',\n",
    "                                        'floor_x',\n",
    "                                        'Label_y',\n",
    "                                        'Date_y',\n",
    "                                        'youTube_y',\n",
    "                                        'timeStamp_y',\n",
    "                                        'Speaker_y',\n",
    "                                        'French_y',\n",
    "                                        'party_y',\n",
    "                                        'seconds_y',\n",
    "                                        'english_y',\n",
    "                                        'floor_y',\n",
    "                                        'Unnamed: 0_y',\n",
    "                                        'X.1',\n",
    "                                        'X.2',\n",
    "                                        'X.3',\n",
    "                                        'X.4',\n",
    "                                        'X.5',\n",
    "                                        'X.6',\n",
    "                                        'X.7',\n",
    "                                        'X',],                                        \n",
    "                                        axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['French',\n",
       " 'ID_main',\n",
       " 'english',\n",
       " 'party',\n",
       " 'seconds',\n",
       " 'youTube',\n",
       " 'sent_txt1_1',\n",
       " 'act_txt1_1',\n",
       " 'sent_txt1_2',\n",
       " 'act_txt1_2',\n",
       " 'sent_txt1_avg',\n",
       " 'act_txt1_avg',\n",
       " 'sent_txt2_1',\n",
       " 'act_txt2_1',\n",
       " 'sent_txt2_2',\n",
       " 'act_txt2_2',\n",
       " 'sent_txt2_avg',\n",
       " 'act_txt2_avg',\n",
       " 'sent_txt3_1',\n",
       " 'act_txt3_1',\n",
       " 'sent_txt3_2',\n",
       " 'act_txt3_2',\n",
       " 'sent_txt3_avg',\n",
       " 'act_txt3_avg',\n",
       " 'sent_vid1_1',\n",
       " 'act_vid1_1',\n",
       " 'sent_vid1_2',\n",
       " 'act_vid1_2',\n",
       " 'sent_vid1_avg',\n",
       " 'act_vid1_avg',\n",
       " 'sent_vid2_1',\n",
       " 'act_vid2_1',\n",
       " 'sent_vid2_2',\n",
       " 'act_vid2_2',\n",
       " 'sent_vid2_avg',\n",
       " 'act_vid2_avg',\n",
       " 'sent_vid3_1',\n",
       " 'act_vid3_1',\n",
       " 'sent_vid3_2',\n",
       " 'act_vid3_2',\n",
       " 'sent_vid3_avg',\n",
       " 'act_vid3_avg',\n",
       " 'sent_textCoders',\n",
       " 'Date_text',\n",
       " 'num',\n",
       " 'FloorGoogleTrans.French.',\n",
       " 'FloorGoogleTrans.English.',\n",
       " 'French.Hansard.of.English.Floor',\n",
       " 'EnglishContext',\n",
       " 'FrenchContext',\n",
       " 'lexiText',\n",
       " 'lexiTextContext',\n",
       " 'sent_lexicoder',\n",
       " 'sent_lexicoderContext',\n",
       " 'LexiTextScoreRaw',\n",
       " 'sent_jockersRinker',\n",
       " 'sent_sentiwordnet',\n",
       " 'sent_huLiu',\n",
       " 'countedWords',\n",
       " 'date',\n",
       " 'english',\n",
       " 'french',\n",
       " 'label',\n",
       " 'party',\n",
       " 'seconds',\n",
       " 'sentencePolarity',\n",
       " 'sent_w2v',\n",
       " 'speaker',\n",
       " 'wordPolaritySummed',\n",
       " 'youTube',\n",
       " 'Label',\n",
       " 'Date',\n",
       " 'Speaker',\n",
       " 'French',\n",
       " 'preprocessed',\n",
       " 'sent_svmIMDBClass',\n",
       " 'fasttext_imbd',\n",
       " 'fasttext_imbdWeight',\n",
       " 'sent_svmStanfordClass',\n",
       " 'sent_fastTextStanfordClass',\n",
       " 'fasttext_stanfordWeights',\n",
       " 'liwc',\n",
       " 'sent_vader',\n",
       " 'sent_svmIMDBPlattProbs']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sentimentScores.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>French</th>\n",
       "      <th>ID_main</th>\n",
       "      <th>english</th>\n",
       "      <th>party</th>\n",
       "      <th>seconds</th>\n",
       "      <th>youTube</th>\n",
       "      <th>sent_txt1_1</th>\n",
       "      <th>act_txt1_1</th>\n",
       "      <th>sent_txt1_2</th>\n",
       "      <th>act_txt1_2</th>\n",
       "      <th>...</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>sent_svmIMDBClass</th>\n",
       "      <th>fasttext_imbd</th>\n",
       "      <th>fasttext_imbdWeight</th>\n",
       "      <th>sent_svmStanfordClass</th>\n",
       "      <th>sent_fastTextStanfordClass</th>\n",
       "      <th>fasttext_stanfordWeights</th>\n",
       "      <th>liwc</th>\n",
       "      <th>sent_vader</th>\n",
       "      <th>sent_svmIMDBPlattProbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017 12 13 0</td>\n",
       "      <td>I thought we usually hired an investigator to ...</td>\n",
       "      <td>NDP</td>\n",
       "      <td>6.45</td>\n",
       "      <td>https://youtu.be/6p2IWa2rfO4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>think usually hire investigator crime cover o...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-0.503811</td>\n",
       "      <td>-1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.714237</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>1.408</td>\n",
       "      <td>0.473646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   French       ID_main                                            english  \\\n",
       "0       1  2017 12 13 0  I thought we usually hired an investigator to ...   \n",
       "\n",
       "  party seconds                       youTube  sent_txt1_1  act_txt1_1  \\\n",
       "0   NDP    6.45  https://youtu.be/6p2IWa2rfO4          2.0         7.0   \n",
       "\n",
       "   sent_txt1_2  act_txt1_2           ...            \\\n",
       "0          3.0         6.0           ...             \n",
       "\n",
       "                                        preprocessed  sent_svmIMDBClass  \\\n",
       "0   think usually hire investigator crime cover o...                 -1   \n",
       "\n",
       "   fasttext_imbd  fasttext_imbdWeight  sent_svmStanfordClass  \\\n",
       "0       Negative            -0.503811                     -1   \n",
       "\n",
       "   sent_fastTextStanfordClass  fasttext_stanfordWeights      liwc  sent_vader  \\\n",
       "0                    Positive                  0.714237  3.181818       1.408   \n",
       "\n",
       "   sent_svmIMDBPlattProbs  \n",
       "0                0.473646  \n",
       "\n",
       "[1 rows x 84 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentScores.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentScores.to_csv('sentimentScores.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sentimentScores.rename(columns={'preprocessed': 'spacyPreProcessed',\n",
    "#                                 'lexiProcessed': 'lexiPreProcessed',\n",
    "#                                 'svm_imdb': 'sent_svmIMDB',\n",
    "#                                 'svm_stanford': 'sent_svmStanford',\n",
    "#                                 'fasttext_imbd': 'sent_fastTextIMDB',\n",
    "#                                 'fasttext_imbdWeight': 'sent_fastTextIMDBweight',\n",
    "#                                 'fasttext_stanford': 'sent_fastTextStanford',\n",
    "#                                 'fasttext_stanfordWeights': 'sent_fastTextStanfordWeights',\n",
    "#                                 'svm_imdb_probability': 'sent_svmIMDBPlattProbs',\n",
    "#                                 'sentiment_lsd': 'sent_lexicoder',\n",
    "#                                 'sentiment_jockers_rinker': 'sent_jockersRinker',\n",
    "#                                 'sentiment_sentiwordnet': 'sent_sentiwordnet',\n",
    "#                                 'sentiment_huliu': 'sent_huLiu',\n",
    "#                                 'vader': 'sent_vader',\n",
    "#                                 'liwc': 'sent_liwc',\n",
    "#                                 'sentiment': 'sent_w2v',\n",
    "#                                 'wordPolaritySummed': 'act_wordPolarity_w2v',\n",
    "#                                 'sentencePolarity': 'act_sentencePolarity_w2v',\n",
    "#                                 't1Act1': 'act_txt1_1',\n",
    "#                                 't1Act2': 'act_txt1_2',\n",
    "#                                 't1ActAvg': 'act_txt1_avg',\n",
    "#                                 't2Act1': 'act_txt2_1',\n",
    "#                                 't2Act2': 'act_txt2_2',\n",
    "#                                 't2ActAvg': 'act_txt2_avg',\n",
    "#                                 't3Act1': 'act_txt3_1',\n",
    "#                                 't3Act2': 'act_txt3_2',\n",
    "#                                 't3ActAvg': 'act_txt3_avg',\n",
    "#                                 'v1Act1': 'act_vid1_1',\n",
    "#                                 'v1Act2': 'act_vid1_2',\n",
    "#                                 'v1ActAvg': 'act_vid1_avg',\n",
    "#                                 'v2Act1': 'act_vid2_1',\n",
    "#                                 'v2Act2': 'act_vid2_2',\n",
    "#                                 'v2ActAvg': 'act_vid2_avg',\n",
    "#                                 'v3Act1': 'act_vid3_1',\n",
    "#                                 'v3Act2': 'act_vid3_2',\n",
    "#                                 'v3ActAvg': 'act_vid3_avg',\n",
    "#                                 't1Sent1': 'sent_txt1_1',\n",
    "#                                 't1Sent2': 'sent_txt1_2',\n",
    "#                                 't1SentAvg': 'sent_txt1_avg',\n",
    "#                                 't2Sent1': 'sent_txt2_1',\n",
    "#                                 't2Sent2': 'sent_txt2_2',\n",
    "#                                 't2SentAvg': 'sent_txt2_avg',\n",
    "#                                 't3Sent1': 'sent_txt3_1',\n",
    "#                                 't3Sent2': 'sent_txt3_2',\n",
    "#                                 't3SentAvg': 'sent_txt3_avg',\n",
    "#                                 'v1Sent1': 'sent_vid1_1',\n",
    "#                                 'v1Sent2': 'sent_vid1_2',\n",
    "#                                 'v1SentAvg': 'sent_vid1_avg',\n",
    "#                                 'v2Sent1': 'sent_vid2_1',\n",
    "#                                 'v2Sent2': 'sent_vid2_2',\n",
    "#                                 'v2SentAvg': 'sent_vid2_avg',\n",
    "#                                 'v3Sent1': 'sent_vid3_1',\n",
    "#                                 'v3Sent2': 'sent_vid3_2',\n",
    "#                                 'v3SentAvg': 'sent_vid3_avg',\n",
    "#                                 'textSent': 'sent_textCoders',\n",
    "#                                 'videoSent': 'sent_videoCoders',\n",
    "#                                 'textAct': 'act_textCoders',\n",
    "#                                 'videoAct': 'act_videoCoders',\n",
    "# })\n",
    "\n",
    "\n",
    "# sentimentScores = sentimentScores.drop(['Unnamed: 0_x',\n",
    "#                                         'Unnamed: 0.1',\n",
    "#                                         'Label_x',\n",
    "#                                         'Date_x',\n",
    "#                                         'youTube_x',\n",
    "#                                         'timeStamp_x',\n",
    "#                                         'Speaker_x',\n",
    "#                                         'French_x',\n",
    "#                                         'party_x',\n",
    "#                                         'seconds_x',\n",
    "#                                         'english_x',\n",
    "#                                         'floor_x',\n",
    "#                                         'Label_y',\n",
    "#                                         'Date_y',\n",
    "#                                         'youTube_y',\n",
    "#                                         'timeStamp_y',\n",
    "#                                         'Speaker_y',\n",
    "#                                         'French_y',\n",
    "#                                         'party_y',\n",
    "#                                         'seconds_y',\n",
    "#                                         'english_y',\n",
    "#                                         'floor_y',\n",
    "#                                         'Unnamed: 0_y'],                                        \n",
    "#                                       axis=1)\n",
    "\n",
    "\n",
    "# sentimentScores = sentimentScores.reindex(sorted(sentimentScores.columns), axis=1)\n",
    "\n",
    "# sentimentScores.to_csv('sentimentScores.csv', sep=',', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XI. Comparison of Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use predictTextCoderScores.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XI. Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples -.01n% of original corpus, re-trains model, tests model, stores model fit, and repeats for n = 1...99.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jan  4 05:38:46 2019/modified on Wed May 22 2019\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import random\n",
    "\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Phrases\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "stopwords_ = stopwords.words('english')\n",
    "\n",
    "\n",
    "hansardSpeeches = pd.read_csv('hansardExtractedSpeechesFull.csv', sep=\"\\t\", encoding=\"utf-8\", header=0) \n",
    "\n",
    "\n",
    "\n",
    "print(hansardSpeeches['mentionedEntityName'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(sentence, remove_stopwords=False):\n",
    "    sentence_text = re.sub(r'[^\\w\\s]','', sentence)\n",
    "    words = sentence_text.lower().split()\n",
    "\n",
    "    for word in words: #Remove Stopwords (Cochrane)\n",
    "        if word in stopwords_:\n",
    "            words.remove(word)\n",
    "\n",
    "    return words\n",
    "\n",
    "def hansard_to_sentences(hansard, tokenizer, remove_stopwords=False ):\n",
    "    #print(\"currently processing: word tokenizer\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # 1. Use the NLTK tokenizer to split the text into sentences\n",
    "        raw_sentences = tokenizer.tokenize(hansard.strip())\n",
    "        #raw_sentences = [sentence_to_wordlist(raw_sentence) for raw_sentence in raw_sentences]\n",
    "        #sentences = [sentence for sublist in raw_sentences for sentence in sublist]\n",
    "        # 2. Loop over each sentence\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            # If a sentence is empty, skip it\n",
    "            if len(raw_sentence) > 0:\n",
    "                # Otherwise, call sentence_to_wordlist to get a list of words\n",
    "                sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "        # 3. Return the list of sentences (each sentence is a list of words, so this returns a list of lists)\n",
    "        #print(len(sentences))\n",
    "        return sentences\n",
    "    except:\n",
    "        print('nope')\n",
    "\n",
    "    end_time = time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing ...\n",
      "no!\n",
      "no!\n",
      "no!\n",
      "Tokenization Complete\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "\n",
    "def speech_tokenizer(hansard):\n",
    "    sentences = []\n",
    "    try:\n",
    "        # Need to first change \"./.\" to \".\" so that sentences parse correctly\n",
    "        hansard = hansard.replace(\"/.\",\"\")\n",
    "        sentences += hansard_to_sentences(hansard, tokenizer)\n",
    "    except:\n",
    "        print(\"no!\")\n",
    "    return sentences\n",
    "\n",
    "print(\"Tokenizing ...\")\n",
    "hansardSpeeches[\"sentences_tokenized\"] = hansardSpeeches[\"speech\"].apply(speech_tokenizer)\n",
    "print(\"Tokenization Complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_removal(hansardSpeeches,days_removed,Seed):\n",
    "    # Set random seed for day removal\n",
    "    random.seed(Seed)\n",
    "    \n",
    "    unique_days_left = hansardSpeeches[\"date\"].unique().tolist()\n",
    "    to_be_removed = random.sample(unique_days_left,days_removed)\n",
    "    #print(to_be_removed)\n",
    "    \n",
    "    ## Keep rows if date value is not in the to_be_removed list\n",
    "    hansardSpeeches = hansardSpeeches[~hansardSpeeches[\"date\"].isin(to_be_removed)]\n",
    "    print(\"Number of days in corpus:\",hansardSpeeches[\"date\"].nunique())\n",
    "    \n",
    "    return hansardSpeeches\n",
    "\n",
    "\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 10   # Minimum word count \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 6           # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "iterations = 5\n",
    "\n",
    "fraction_removed = 0.05 # (Wong) Fraction of days that are removed at each training instance\n",
    "\n",
    "total_days = hansardSpeeches[\"date\"].nunique()\n",
    "print(\"There are\",total_days,\"days in corpus\")\n",
    "days_removed = int((total_days)*(fraction_removed))\n",
    "print(\"Amount of days to be discarded at each model iteration: \",days_removed)\n",
    "print(\"\")\n",
    "\n",
    "## Iterate until there are insufficient days left\n",
    "i = 0\n",
    "while fraction_removed*i < 1:\n",
    "    if i == 0:  \n",
    "        \n",
    "        ## sentences is now a list of sentences formatted correctly for word2vec\n",
    "        sentences = [sentence for sublist in hansardSpeeches[\"sentences_tokenized\"].tolist() for sentence in sublist]\n",
    "        print(\"Current population size =\",len(sentences))\n",
    "        \n",
    "        print(\"currently processing: training model\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "            level=logging.INFO)\n",
    "\n",
    "        model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                    size=num_features, min_count = min_word_count, \\\n",
    "                    window = context, sample = downsampling, iter = iteration)\n",
    "\n",
    "        model.init_sims(replace=True)\n",
    "\n",
    "        model_name = 'hansardQuestions_removed_0.00.model'\n",
    "        model.save(model_name)\n",
    "        model = gensim.models.Word2Vec.load(model_name)\n",
    "\n",
    "        vocab = list(model.wv.vocab.keys())\n",
    "\n",
    "\n",
    "        print(\"Process complete--the first 25 words in the vocabulary are:\")\n",
    "\n",
    "        print(vocab[:25])\n",
    "        print(\"\")\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    else:\n",
    "        \n",
    "        hansardSpeeches = day_removal(hansardSpeeches, days_removed, 42)\n",
    "        \n",
    "        ## sentences is now a list of sentences formatted correctly for word2vec\n",
    "        sentences = [sentence for sublist in hansardSpeeches[\"sentences_tokenized\"].tolist() for sentence in sublist]\n",
    "        print(\"Current population size =\",len(sentences))\n",
    "        \n",
    "        print(\"currently processing: training model, removing\",\n",
    "              \"{0:.2f}\".format(fraction_removed*i),\n",
    "              \"of samples\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "            level=logging.INFO)\n",
    "\n",
    "        model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                    size=num_features, min_count = min_word_count, \\\n",
    "                    window = context, sample = downsampling)\n",
    "\n",
    "        model.init_sims(replace=True)\n",
    "\n",
    "        model_name = 'hansardQuestions_removed_'+str(\"{0:.2f}\".format(fraction_removed*i)+'.model')\n",
    "        model.save(model_name)\n",
    "        model = gensim.models.Word2Vec.load(model_name)\n",
    "\n",
    "        vocab = list(model.wv.vocab.keys())\n",
    "\n",
    "\n",
    "        print(\"Process complete--the first 25 words in the vocabulary are:\")\n",
    "\n",
    "        print(vocab[:25])\n",
    "        print(\"\")\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jan  4 14:54:42 2019/modified on Wed May 23 2019\n",
    "@author: chriscochrane/michaelwcwong\n",
    "\"\"\"\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Initialization\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import pylab as pl\n",
    "import scipy.stats as stats\n",
    "import nltk\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "# Loading stored w2v Model\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "model_dict = {}\n",
    "for i in range(0,100,5):\n",
    "    try:\n",
    "        model_dict[\"w2v_model_removed_0.\"+str(\"{:02d}\".format(i))] = gensim.models.Word2Vec.load(\"hansardQuestions_removed_0.\"+str(\"{:02d}\".format(i))+\".model\")\n",
    "    except:\n",
    "        pass\n",
    "print(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords.  They are not relevant to sentiment scoring.\n",
    "\n",
    "def sentence_to_wordlist(sentence, remove_stopwords=False):\n",
    "    sentence_text = re.sub(r'[^\\w\\s]',' ', sentence)\n",
    "    words = sentence_text.lower().split()\n",
    "\n",
    "    for word in words: #Remove Stopwords (Cochrane)\n",
    "        if word in stopwords_:\n",
    "            words.remove(word)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = stopwords.words('english')\n",
    "\n",
    "## Initialize lists\n",
    "labelList = []\n",
    "dateList = []\n",
    "IDmainList = []\n",
    "youTubeList = []\n",
    "timeStampList = []\n",
    "speakerList = []\n",
    "frenchList = []\n",
    "partyList = []\n",
    "secondsList = []\n",
    "englishList = []\n",
    "floorList = []\n",
    "sentenceList = []\n",
    "sentiment_columnList = []\n",
    "wordPolaritySummed_columnList = []\n",
    "sentencePolarity_columnList = []\n",
    "countedWords_columnList = []\n",
    "\n",
    "#Import Data re: transcripts of video snippets\n",
    "\n",
    "gitHub = 'hansardExtractedVideoTranscripts.csv'\n",
    "hansardVideos = pd.read_csv(gitHub, encoding='utf-8')\n",
    "\n",
    "'''A loop for cycling through the list of rows in the hansardVideo\n",
    "Transcripts.'''\n",
    "\n",
    "## Iterate through all word2vec models for each sentence\n",
    "for model_name, model in model_dict.items():\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"/nCurrently processing \"+model_name)\n",
    "\n",
    "    #-----------------------------------------------------------------------------\n",
    "    # Seed Words (Adapted from Turney and Littman)\n",
    "    #-----------------------------------------------------------------------------\n",
    "\n",
    "    good = model['good'].astype('float64')\n",
    "    excellent = model['excellent'].astype('float64')\n",
    "    correct = model['correct'].astype('float64')\n",
    "    best = model['best'].astype('float64')\n",
    "    happy = model['happy'].astype('float64')\n",
    "    positive = model['positive'].astype('float64')\n",
    "    fortunate = model['fortunate'].astype('float64')\n",
    "\n",
    "\n",
    "    bad = model['bad'].astype('float64')\n",
    "    terrible = model['terrible'].astype('float64')\n",
    "    wrong = model['wrong'].astype('float64')\n",
    "    worst = model['worst'].astype('float64')\n",
    "    disappointed = model['disappointed'].astype('float64')\n",
    "    negative = model['negative'].astype('float64')\n",
    "    unfortunate = model['unfortunate'].astype('float64')\n",
    "\n",
    "    vocab = list(model.wv.vocab.keys()) #the full vocabulary of Hansard\n",
    "\n",
    "\n",
    "    # an empty list for storing wights and an empty dictionary for linking\n",
    "    # weights and words\n",
    "\n",
    "    runningTally=[]\n",
    "    dictOfWeights = {}\n",
    "\n",
    "    #-----------------------------------------------------------------------------\n",
    "    # Model\n",
    "    #-----------------------------------------------------------------------------\n",
    "\n",
    "    '''for every word in the hansard, calculate its cosine similarity to the \n",
    "    lists of positive words and negative words, then substract the sum of that\n",
    "    word's cosine simlarity to negative seed words from its cosine similarity to the\n",
    "    postive seed words.''' \n",
    "\n",
    "    for word in vocab:\n",
    "\n",
    "        word_model = model[word].astype('float64')\n",
    "\n",
    "        pos1 = np.dot(word_model, good).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(good).astype('float64'))\n",
    "        pos2 = np.dot(word_model, excellent).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(excellent).astype('float64'))\n",
    "        pos3 = np.dot(word_model, correct).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(correct).astype('float64'))\n",
    "        pos4 = np.dot(word_model, best).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(best).astype('float64'))\n",
    "        pos5 = np.dot(word_model, happy).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(happy).astype('float64'))\n",
    "        pos6 = np.dot(word_model, positive).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(positive).astype('float64'))\n",
    "        pos7 = np.dot(word_model, fortunate).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(fortunate).astype('float64'))\n",
    "\n",
    "        neg1 = np.dot(word_model, bad).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(bad))\n",
    "        neg2 = np.dot(word_model, terrible).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(terrible))\n",
    "        neg3 = np.dot(word_model, wrong).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(wrong))\n",
    "        neg4 = np.dot(word_model, worst).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(worst))\n",
    "        neg5 = np.dot(word_model, disappointed).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(disappointed))\n",
    "        neg6 = np.dot(word_model, negative).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(negative))\n",
    "        neg7 = np.dot(word_model, unfortunate).astype('float64') / (np.linalg.norm(word_model) * np.linalg.norm(unfortunate))\n",
    "\n",
    "        pos = sum([pos1, pos2, pos3, pos4,  pos5, pos6, pos7])/7\n",
    "        neg = sum([neg1, neg2, neg3, neg4, neg5, neg6, neg7])/7\n",
    "        posneg = pos-neg\n",
    "        result = (word, posneg)\n",
    "        runningTally.append(result)\n",
    "        dictOfWeights[word] = result\n",
    "\n",
    "\n",
    "        #-----------------------------------------------------------------------------\n",
    "        # Results\n",
    "        #-----------------------------------------------------------------------------\n",
    "\n",
    "        '''The 100 most positive signed and most negative signed words'''\n",
    "\n",
    "        runningTally = sorted(runningTally, key=itemgetter(1), reverse=True)\n",
    "        print(\"Top Positive:\", runningTally[:100])\n",
    "        print(\"Top Negative:\", runningTally[len(runningTally)-100:])\n",
    "        print(\"Total Vocabulary Size:\", len(vocab))\n",
    "        print(\"\")\n",
    "\n",
    "        '''word counts for the seed words'''\n",
    "        vocab_obj_good = model.wv.vocab[\"good\"]\n",
    "        print(\"good\", vocab_obj_good.count)\n",
    "\n",
    "        vocab_obj_good = model.wv.vocab[\"excellent\"]\n",
    "        print(\"excellent\", vocab_obj_good.count)\n",
    "\n",
    "        vocab_obj_good = model.wv.vocab[\"correct\"]\n",
    "        print(\"correct\", vocab_obj_good.count)\n",
    "\n",
    "        vocab_obj_good = model.wv.vocab[\"best\"]\n",
    "        print(\"best\", vocab_obj_good.count)\n",
    "\n",
    "        vocab_obj_good = model.wv.vocab[\"happy\"]\n",
    "        print(\"happy\", vocab_obj_good.count)\n",
    "\n",
    "\n",
    "        vocab_obj_good = model.wv.vocab[\"positive\"]\n",
    "        print(\"positive\", vocab_obj_good.count)\n",
    "\n",
    "        vocab_obj_good = model.wv.vocab[\"fortunate\"]\n",
    "        print(\"fortunate\", vocab_obj_good.count)\n",
    "\n",
    "        vocab_obj_good = model.wv.vocab[\"bad\"]\n",
    "        print(\"bad\", vocab_obj_good.count)\n",
    "\n",
    "        vocab_obj_good = model.wv.vocab[\"terrible\"]\n",
    "        print(\"terrible\", vocab_obj_good.count)\n",
    "\n",
    "        vocab_obj_good = model.wv.vocab[\"wrong\"]\n",
    "        print(\"wrong\", vocab_obj_good.count)\n",
    "\n",
    "        vocab_obj_good = model.wv.vocab[\"worst\"]\n",
    "        print(\"worst\", vocab_obj_good.count)\n",
    "\n",
    "        vocab_obj_good = model.wv.vocab[\"disappointed\"]\n",
    "        print(\"disappointed\", vocab_obj_good.count)\n",
    "\n",
    "        vocab_obj_good = model.wv.vocab[\"negative\"]\n",
    "        print(\"negative\", vocab_obj_good.count)\n",
    "\n",
    "        vocab_obj_good = model.wv.vocab[\"unfortunate\"]\n",
    "        print(\"unfortunate\", vocab_obj_good.count)\n",
    "  \n",
    "    sentimentList = []\n",
    "    wordPolaritySummedList = [] \n",
    "    sentencePolarityList = []\n",
    "    countedWordsList = []\n",
    "    \n",
    "    for i, sentence in enumerate(hansardVideos[\"english\"]):\n",
    "        labelList.append(hansardVideos[\"Label\"][i])\n",
    "        dateList.append(hansardVideos[\"Date\"][i])\n",
    "        IDmainList.append(hansardVideos[\"ID_main\"][i])\n",
    "        youTubeList.append(hansardVideos[\"youTube\"][i])\n",
    "        timeStampList.append(hansardVideos[\"timeStamp\"][i])\n",
    "        speakerList.append(hansardVideos[\"Speaker\"][i])\n",
    "        frenchList.append(hansardVideos[\"French\"][i])\n",
    "        partyList.append(hansardVideos[\"party\"][i])\n",
    "        secondsList.append(hansardVideos[\"seconds\"][i])\n",
    "        englishList.append(hansardVideos[\"english\"][i])\n",
    "        floorList.append(hansardVideos[\"floor\"][i])\n",
    "\n",
    "        sentenceList.append(sentence)\n",
    "        '''break the sentence into words'''\n",
    "        sentence_words = sentence_to_wordlist(sentence)\n",
    "\n",
    "        '''initialize variables'''\n",
    "        sentiment = 0 #positivity minus negativity\n",
    "        sentencePolarity = 0 #absolute values of positivity minus negativity\n",
    "        wordPolaritySummed = 0 #sum of absolute value of word polarities\n",
    "        countedWords = 0\n",
    "\n",
    "        '''for every word in the sentence, subtract the sum of its cosine \n",
    "        similarity to the negative seed words from the sum of its cosine\n",
    "        similarity to the positive seed words, and then sum the difference\n",
    "        across all words in the sentence.'''\n",
    "\n",
    "        #-----------------------------------------------------------------------------\n",
    "        # Apply Lexicon\n",
    "        #-----------------------------------------------------------------------------\n",
    "\n",
    "        '''Apply the Lexicon to score the transcripts of the video clips.'''\n",
    "\n",
    "        for word in sentence_words: \n",
    "\n",
    "            try:\n",
    "\n",
    "                word_model = model[word]\n",
    "\n",
    "                pos1 = np.dot(word_model, good) / (np.linalg.norm(word_model) * np.linalg.norm(good))\n",
    "                pos2 = np.dot(word_model, excellent) / (np.linalg.norm(word_model) * np.linalg.norm(excellent))\n",
    "                pos3 = np.dot(word_model, correct) / (np.linalg.norm(word_model) * np.linalg.norm(correct))\n",
    "                pos4 = np.dot(word_model, best) / (np.linalg.norm(word_model) * np.linalg.norm(best))\n",
    "                pos5 = np.dot(word_model, happy) / (np.linalg.norm(word_model) * np.linalg.norm(happy))\n",
    "                pos6 = np.dot(word_model, positive) / (np.linalg.norm(word_model) * np.linalg.norm(positive))\n",
    "                pos7 = np.dot(word_model, fortunate) / (np.linalg.norm(word_model) * np.linalg.norm(fortunate))\n",
    "\n",
    "                neg1 = np.dot(word_model, bad) / (np.linalg.norm(word_model) * np.linalg.norm(bad))\n",
    "                neg2 = np.dot(word_model, terrible) / (np.linalg.norm(word_model) * np.linalg.norm(terrible))\n",
    "                neg3 = np.dot(word_model, wrong) / (np.linalg.norm(word_model) * np.linalg.norm(wrong))\n",
    "                neg4 = np.dot(word_model, worst) / (np.linalg.norm(word_model) * np.linalg.norm(worst))\n",
    "                neg5 = np.dot(word_model, disappointed) / (np.linalg.norm(word_model) * np.linalg.norm(disappointed))\n",
    "                neg6 = np.dot(word_model, negative) / (np.linalg.norm(word_model) * np.linalg.norm(negative))\n",
    "                neg7 = np.dot(word_model, unfortunate) / (np.linalg.norm(word_model) * np.linalg.norm(unfortunate))\n",
    "\n",
    "                pos = sum([pos1, pos2, pos3, pos4, pos5, pos6, pos7]) / 7\n",
    "                neg = sum([neg1, neg2, neg3, neg4, neg5, neg6, neg7]) / 7\n",
    "                posneg = pos - neg\n",
    "                sentiment += posneg\n",
    "                wordPolaritySummed += abs(posneg)\n",
    "                countedWords +=1\n",
    "\n",
    "\n",
    "            except:\n",
    "\n",
    "                #handful of garbage words -- did not meet minimum word threschold\n",
    "                print(\"Warning! Word: \", word, \" from speech: \", i, \" not in w2v model!\")\n",
    "\n",
    "                continue\n",
    "\n",
    "        ## Each value of these lists represent a row for the dataframe\n",
    "        ## The length of these lists should be equal to the number of total sentences\n",
    "        sentimentList.append(sentiment)\n",
    "        wordPolaritySummedList.append(wordPolaritySummed)   \n",
    "        sentencePolarityList.append(abs(sentiment))\n",
    "        countedWordsList.append(countedWords)\n",
    "        \n",
    "    ## Each value (list) of these lists represents a single column\n",
    "    sentiment_columnList.append(sentimentList)\n",
    "    wordPolaritySummed_columnList.append(wordPolaritySummedList)\n",
    "    sentencePolarity_columnList.append(sentencePolarityList)\n",
    "    countedWords_columnList.append(countedWordsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''output to csv file'''\n",
    "\n",
    "## Create separate dataframes and then merge\n",
    "baseline_df = pd.DataFrame({'label': labelList, \n",
    "                               'date': dateList, \n",
    "                               'IDMain': IDmainList, \n",
    "                               'youTube': youTubeList, \n",
    "                               'timeStamp': timeStampList, \n",
    "                               'speaker': speakerList,\n",
    "                               'french': frenchList, \n",
    "                               'party': partyList,\n",
    "                               'seconds': secondsList,\n",
    "                               'english': englishList,\n",
    "                               'floor': floorList})\n",
    "\n",
    "\n",
    "def df_creator(list_of_lists,column_name,df_list)\n",
    "    df = pd.Dataframe(list_of_lists,columns=[column_name+str(\"{:02d}\".format(i)) for i in range(len(model_dict))])\n",
    "    df_list.append(df)\n",
    "    return df_list\n",
    "\n",
    "df_list = []\n",
    "df_list = df_creator(sentiment_columnList,\"sentiment_removed_0.\",df_list)\n",
    "df_list = df_creator(sentencePolarity_columnList,\"sentencePolarity_removed_0.\",df_list)\n",
    "df_list = df_creator(wordPolaritySummed_columnList,\"wordPolaritySummed_removed_0.\",df_list)\n",
    "df_list = df_creator(countedWords_columnList,\"countedWords_removed_0.\",df_list)\n",
    "\n",
    "w2vScores = pd.concat(df_list.insert(0,baseline_df),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(w2vScores.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
